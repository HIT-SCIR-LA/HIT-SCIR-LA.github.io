@inproceedings{wang-etal-2024-enhancing-numerical,
 abstract = {Numerical reasoning is an essential ability for NLP systems to handle numeric information. Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are {``}unreliable{''} since such processes could contain information unrelated to the answer. To address this limitation, we introduce enhancing numerical reasoning with reliable processes (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula. To overcome this difficulty, we present a series of pre-training tasks to help models learn the reasoning process generation with synthesized data. The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8{\%}, proving the effectiveness of our method.},
 address = {Bangkok, Thailand},
 author = {Wang, Dingzirui  and
Dou, Longxu  and
Zhang, Xuanliang  and
Zhu, Qingfu  and
Che, Wanxiang},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.acl-long.582},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {10812--10828},
 publisher = {Association for Computational Linguistics},
 title = {Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes},
 url = {https://aclanthology.org/2024.acl-long.582},
 year = {2024}
}
