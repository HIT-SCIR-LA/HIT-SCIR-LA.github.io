@article{cui2022multilingual,
 abstract = {Achieving human-level performance on some of the machine reading comprehension (MRC) datasets is no longer challenging with the help of powerful pre-trained language models (PLMs). However, the internal mechanism of these artifacts remains unclear, placing an obstacle to further understand these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final MRC system performance, revealing the potential explainability in PLM-based MRC},
 author = {Cui, Yiming and Zhang, Wei-Nan and Che, Wanxiang and Liu, Ting and Chen, Zhigang and Wang, Shijin},
 journal = {Iscience},
 number = {5},
 pages = {104176},
 pub_year = {2022},
 publisher = {Elsevier},
 title = {Multilingual multi-aspect explainability analyses on machine reading comprehension models},
 url = {https://www.sciencedirect.com/science/article/pii/S2589004222004461},
 venue = {Iscience},
 volume = {25}
}

