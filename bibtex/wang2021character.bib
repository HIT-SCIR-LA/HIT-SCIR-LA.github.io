@article{wang2021character,
 abstract = {Semantic role labeling (SRL) aims at identifying the predicate-argument structure of a sentence. Recent work has significantly improved SRL performance by incorporating syntactic information and exploiting pre-trained models like BERT. Most of them use pre-trained models as isolated encoders to obtain word embeddings and enhance them with word-level syntax. Unlike many other languages, Chinese pre-trained models normally use Chinese characters instead of subwords as the basic input units, making the many-units-in},
 author = {Wang, Yuxuan and Lei, Zhilin and Che, Wanxiang},
 journal = {International Journal of Machine Learning and Cybernetics},
 number = {12},
 pages = {3503--3515},
 pub_year = {2021},
 publisher = {Springer},
 title = {Character-Level Syntax Infusion in Pre-Trained Models for Chinese Semantic Role Labeling},
 url = {https://link.springer.com/article/10.1007/s13042-021-01397-3},
 venue = {International Journal of Machine Learning and â€¦},
 volume = {12}
}

