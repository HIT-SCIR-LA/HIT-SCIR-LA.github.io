@inproceedings{wang-etal-2024-improving-demonstration,
 abstract = {In-context learning with large language models (LLMs) is the current mainstream method for text-to-SQL. Previous studies have explored selecting relevant demonstrations from a human-labeled demonstration pool, but these methods lack diversity and incur high labeling costs. In this work, we address measuring and enhancing the diversity of the text-to-SQL demonstration pool. First, we introduce a diversity metric and present that the diversity of the existing labeling data can be further enhanced. Motivated by these findings, we propose Fused that iteratively fuses demonstrations to create a diverse demonstration pool based on human labeling or even from scratch with LLMs, reducing labeling costs. Fused achieves an average improvement of 2.1{\%} based on existing labeling and 5.5{\%} from scratch on several mainstream datasets, demonstrating its effectiveness.},
 address = {Miami, Florida, USA},
 author = {Wang, Dingzirui  and
Dou, Longxu  and
Zhang, Xuanliang  and
Zhu, Qingfu  and
Che, Wanxiang},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 doi = {10.18653/v1/2024.findings-emnlp.65},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {1193--1207},
 publisher = {Association for Computational Linguistics},
 title = {Improving Demonstration Diversity by Human-Free Fusing for Text-to-{SQL}},
 url = {https://aclanthology.org/2024.findings-emnlp.65},
 year = {2024}
}
