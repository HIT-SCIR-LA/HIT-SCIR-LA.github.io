@inproceedings{wang2020multi,
 abstract = {Most existing approaches to disfluency detection heavily rely on human-annotated data, which is expensive to obtain in practice. To tackle the training data bottleneck, we investigate methods for combining multiple self-supervised tasks-ie, supervised tasks where data can be collected without manual labeling. First, we construct large-scale pseudo training data by randomly adding or deleting words from unlabeled news data, and propose two self-supervised pre-training tasks:(i) tagging task to detect the added noisy words.(ii)},
 author = {Wang, Shaolei and Che, Wanxiang and Liu, Qi and Qin, Pengda and Liu, Ting and Wang, William Yang},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {05},
 pages = {9193--9200},
 pub_year = {2020},
 title = {Multi-task self-supervised learning for disfluency detection},
 url = {https://ojs.aaai.org/index.php/AAAI/article/download/6456/6312},
 venue = {Proceedings of the AAAI â€¦},
 volume = {34}
}

