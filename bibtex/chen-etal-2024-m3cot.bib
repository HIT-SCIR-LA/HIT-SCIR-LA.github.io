@inproceedings{chen-etal-2024-m3cot,
 abstract = {Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there is a large gap between VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.},
 address = {Bangkok, Thailand},
 author = {Chen, Qiguang  and
Qin, Libo  and
Zhang, Jin  and
Chen, Zhi  and
Xu, Xiao  and
Che, Wanxiang},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.acl-long.446},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {8199--8221},
 publisher = {Association for Computational Linguistics},
 title = {{M}$^3${C}o{T}: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought},
 url = {https://aclanthology.org/2024.acl-long.446},
 year = {2024}
}
