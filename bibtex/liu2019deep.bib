@article{liu2019deep,
 abstract = {Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem—universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and},
 author = {Liu, Yijia and Che, Wanxiang and Wang, Yuxuan and Zheng, Bo and Qin, Bing and Liu, Ting},
 journal = {ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
 number = {1},
 pages = {1--17},
 pub_year = {2019},
 publisher = {ACM New York, NY, USA},
 title = {Deep contextualized word embeddings for universal dependency parsing},
 url = {http://ir.hit.edu.cn/~car/papers/tallip2019-liu.pdf},
 venue = {ACM Transactions on …},
 volume = {19}
}

