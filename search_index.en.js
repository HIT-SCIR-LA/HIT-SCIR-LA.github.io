window.searchIndex = {"fields":["title","body"],"pipeline":["trimmer","stopWordFilter","stemmer"],"ref":"id","version":"0.9.5","index":{"body":{"root":{"docs":{},"df":0,"1":{"docs":{},"df":0,"%":{"docs":{},"df":0,"（":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0}},"df":1}}}}}},"3":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}},"4":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"6":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3},"7":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"7":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}},"）":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"机":{"docs":{},"df":0,"理":{"docs":{},"df":0,"，":{"docs":{},"df":0,"以":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"能":{"docs":{},"df":0,"力":{"docs":{},"df":0,"的":{"docs":{},"df":0,"涌":{"docs":{},"df":0,"现":{"docs":{},"df":0,"来":{"docs":{},"df":0,"源":{"docs":{},"df":0,"、":{"docs":{},"df":0,"行":{"docs":{},"df":0,"为":{"docs":{},"df":0,"规":{"docs":{},"df":0,"律":{"docs":{},"df":0,"与":{"docs":{},"df":0,"运":{"docs":{},"df":0,"行":{"docs":{},"df":0,"机":{"docs":{},"df":0,"制":{"docs":{},"df":0,"，":{"docs":{},"df":0,"从":{"docs":{},"df":0,"而":{"docs":{},"df":0,"指":{"docs":{},"df":0,"导":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"的":{"docs":{},"df":0,"优":{"docs":{},"df":0,"化":{"docs":{},"df":0,"；":{"docs":{},"df":0,"（":{"docs":{},"df":0,"2":{"docs":{},"df":0,"）":{"docs":{},"df":0,"增":{"docs":{},"df":0,"强":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"，":{"docs":{},"df":0,"以":{"docs":{},"df":0,"优":{"docs":{},"df":0,"化":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"能":{"docs":{},"df":0,"力":{"docs":{},"df":0,"的":{"docs":{},"df":0,"逻":{"docs":{},"df":0,"辑":{"docs":{},"df":0,"深":{"docs":{},"df":0,"度":{"docs":{},"df":0,"，":{"docs":{},"df":0,"思":{"docs":{},"df":0,"维":{"docs":{},"df":0,"广":{"docs":{},"df":0,"度":{"docs":{},"df":0,"与":{"docs":{},"df":0,"适":{"docs":{},"df":0,"度":{"docs":{},"df":0,"反":{"docs":{},"df":0,"思":{"docs":{},"df":0,"，":{"docs":{},"df":0,"从":{"docs":{},"df":0,"而":{"docs":{},"df":0,"促":{"docs":{},"df":0,"进":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"的":{"docs":{},"df":0,"应":{"docs":{},"df":0,"用":{"docs":{},"df":0,"；":{"docs":{},"df":0,"（":{"docs":{},"df":0,"3":{"docs":{"/research/方向介绍/tui-li/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"2":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3,"0":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":2,"1":{"docs":{},"df":0,"0":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0}},"df":1},"1":{"docs":{"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":1},"2":{"docs":{"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":1},"6":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0}},"df":1},"7":{"docs":{"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"tf":1.0}},"df":4},"8":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"tf":1.0},"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"tf":1.0},"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"tf":1.0},"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"tf":1.0},"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":8},"9":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"tf":1.0},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0}},"df":8}},"2":{"docs":{},"df":0,"0":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0},"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"tf":1.0},"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":9},"1":{"docs":{"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":12,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"s":{"docs":{},"df":0,"和":{"docs":{},"df":0,"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}},"2":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0},"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"tf":1.0}},"df":5,"|":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"tf":1.0}},"df":1}}}}}}}}}},"3":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0}},"df":3,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}},"届":{"docs":{},"df":0,"2":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"tf":1.0}},"df":1}}}},"4":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/ha-gong-da-xun-fei-rong-huo-2024nian-du-wu-wen-jun-ren-gong-zhi-neng-ke-xue-ji-shu-jiang-ke-ji-jin-bu-jiang-yi-deng-jiang/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"tf":1.0}},"df":3,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":2}}}}}}}},"5":{"docs":{"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0}},"df":5,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":2}}}}}}}}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}},"2":{"docs":{},"df":0,"多":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"任":{"docs":{},"df":0,"务":{"docs":{},"df":0,"型":{"docs":{},"df":0,"对":{"docs":{},"df":0,"话":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"评":{"docs":{},"df":0,"测":{"docs":{},"df":0,"取":{"docs":{},"df":0,"得":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"9":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}},"3":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing-3-0ban-ben/":{"tf":1.0}},"df":1},"5":{"docs":{"/news/ha-gong-da-kai-yuan-huo-zi-3-5-dui-hua-da-mo-xing/":{"tf":1.0}},"df":1,"5":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"l":{"docs":{"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"4":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0},"/resources/开源项目/pyltp/":{"tf":1.0}},"df":4,".":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}},"！":{"docs":{},"df":0,"单":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"完":{"docs":{},"df":0,"成":{"docs":{},"df":0,"6":{"docs":{"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"tf":1.0}},"df":1}}}}}}}},"1":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}},"2":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}},"]":{"docs":{},"df":0,"(":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{"/resources/开源项目/pyltp/":{"tf":1.0}},"df":1}}}}}},"5":{"docs":{"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":1},"6":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/resources/开源项目/ltp/":{"tf":1.4142135623730951}},"df":3,"篇":{"docs":{},"df":0,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"2":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"s":{"docs":{},"df":0,"/":{"docs":{},"df":0,"1":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"o":{"docs":{"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"8":{"docs":{"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0}},"df":1,"x":{"docs":{},"df":0,"7":{"docs":{},"df":0,"b":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":2}}}},"9":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0}},"df":2,"0":{"docs":{},"df":0,"0":{"docs":{"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"tf":1.0}},"df":1}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"tf":1.0}},"df":5,"2":{"docs":{},"df":0,"0":{"docs":{"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai20-yong-yu-lian-he-jian-mo-dui-hua-xing-wei-shi-bie-he-qing-gan-fen-lei-de-shen-du-jiao-hu-guan-xi-wang-luo/":{"tf":1.0}},"df":2,"2":{"docs":{},"df":0,"1":{"docs":{"/news/sai-er-yuan-chuang-aaai2021-xiao-yang-ben-xue-xi-xia-de-duo-biao-qian-fen-lei-wen-ti-chu-tan/":{"tf":1.0}},"df":1}}}}}}},"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{"/resources/开源项目/abacus/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":2}}},"l":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"tf":1.0}},"df":7,"2":{"docs":{},"df":0,"0":{"docs":{"/news/sai-er-yuan-chuang-acl20-ji-yu-dui-hua-tu-pu-de-kai-fang-yu-duo-lun-dui-hua-ce-lue-xue-xi/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-ji-yu-tu-zhu-yi-li-wang-luo-de-duo-li-du-ji-qi-yue-du-li-jie-wen-dang-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-rang-mo-xing-shi-ban-gong-bei-tan-jiu-shao-yang-ben-xu-lie-biao-zhu-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-yong-yu-duo-ling-yu-duan-dao-duan-ren-wu-xing-dui-hua-xi-tong-de-dong-tai-rong-he-wang-luo/":{"tf":1.0}},"df":4}},"@":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"2":{"docs":{"/news/acl-2022-fan-xiang-yu-ce-geng-hao-ji-yu-fan-xiang-ti-shi-de-xiao-yang-ben-cao-wei-biao-zhu-fang-fa/":{"tf":1.0}},"df":1}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0}},"df":2}}}},"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":9}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0}},"df":4}}}}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/research/方向介绍/ju-shen/":{"tf":1.0}},"df":1}}},"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1}}},"i":{"docs":{"/news/ha-gong-da-jiu-da-aimo-xing-deng-chang-jie-suo-qian-xing-bai-ye-zhi-neng-xin-fan-shi/":{"tf":1.0},"/news/qing-chun-de-xuan-ze-gun-ha-gong-da-zhe-ge-tuan-dui-li-yu-sheng-cheng-shi-aichao-tou/":{"tf":1.0},"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":4},"l":{"docs":{},"df":0,"g":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":3}}}}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":5}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"r":{"docs":{"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":1,"i":{"docs":{"/about/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0}},"df":4}}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":1}}}}}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0}},"df":2}}},"s":{"docs":{},"df":0,"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":2}}}},"y":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"i":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":5},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":6}}}}}}},"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0}},"df":1,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0}},"df":2}}}}}}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/demo/演示系统/zhu-suan-sql/":{"tf":1.0}},"df":1,"最":{"docs":{},"df":0,"热":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"：":{"docs":{},"df":0,"做":{"docs":{},"df":0,"到":{"docs":{},"df":0,"头":{"docs":{},"df":0,"了":{"docs":{},"df":0,"！":{"docs":{},"df":0,"清":{"docs":{},"df":0,"华":{"docs":{},"df":0,"和":{"docs":{},"df":0,"哈":{"docs":{},"df":0,"工":{"docs":{},"df":0,"大":{"docs":{},"df":0,"把":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"量":{"docs":{},"df":0,"化":{"docs":{},"df":0,"做":{"docs":{},"df":0,"到":{"docs":{},"df":0,"了":{"docs":{},"df":0,"1":{"docs":{"/news/jin-ri-arxivzui-re-nlpda-mo-xing-lun-wen-zuo-dao-tou-liao-qing-hua-he-ha-gong-da-ba-da-mo-xing-liang-hua-zuo-dao-liao-1bi-te/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0}},"df":4}}}}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.4142135623730951},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":10}}}}},"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":2}}}}}}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":4}},"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0}},"df":2}}}}}},"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":26,"1":{"docs":{},"df":0,"9":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"2":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"7":{"docs":{},"df":0,"3":{"docs":{},"df":0,"9":{"docs":{},"df":0,"6":{"docs":{},"df":0,".":{"docs":{},"df":0,"3":{"docs":{},"df":0,"9":{"docs":{},"df":0,"7":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"8":{"docs":{},"df":0,"8":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"5":{"docs":{},"df":0,"7":{"docs":{},"df":0,"7":{"docs":{},"df":0,"6":{"docs":{},"df":0,".":{"docs":{},"df":0,"5":{"docs":{},"df":0,"7":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"2":{"docs":{},"df":0,"9":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"8":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"6":{"docs":{},"df":0,"9":{"docs":{},"df":0,"9":{"docs":{},"df":0,"5":{"docs":{},"df":0,".":{"docs":{},"df":0,"9":{"docs":{},"df":0,"7":{"docs":{},"df":0,"7":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"9":{"docs":{},"df":0,"9":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{},"df":0,"7":{"docs":{},"df":0,"6":{"docs":{},"df":0,".":{"docs":{},"df":0,"6":{"docs":{},"df":0,"2":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"7":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"5":{"docs":{},"df":0,"9":{"docs":{},"df":0,"5":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"8":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"6":{"docs":{},"df":0,"8":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"5":{"docs":{},"df":0,"7":{"docs":{},"df":0,"5":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"3":{"docs":{},"df":0,"9":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"2":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"n":{"docs":{"/resources/社区资源/la-beginner/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":4}}}}}}},"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":7}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}}}},"y":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":6}}}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}}},"t":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0}},"df":1}}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}}}}}}},"u":{"docs":{},"df":0,"g":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3,"/":{"docs":{},"df":0,"c":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}},"2":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0}},"df":1}},"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":3}},"c":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"@":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,".":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,".":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,".":{"docs":{},"df":0,"c":{"docs":{},"df":0,"n":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/resources/开源项目/ltp/":{"tf":1.4142135623730951}},"df":3}}}}}}}}}}}}}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"l":{"docs":{"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"tf":1.0}},"df":2,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{},"df":0,"学":{"docs":{},"df":0,"生":{"docs":{},"df":0,"研":{"docs":{},"df":0,"讨":{"docs":{},"df":0,"会":{"docs":{},"df":0,"！":{"docs":{},"df":0,"如":{"docs":{},"df":0,"何":{"docs":{},"df":0,"诞":{"docs":{},"df":0,"生":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"！":{"docs":{},"df":0,"如":{"docs":{},"df":0,"何":{"docs":{},"df":0,"跟":{"docs":{},"df":0,"审":{"docs":{},"df":0,"稿":{"docs":{},"df":0,"人":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"t":{"docs":{"/news/ccl2021xue-sheng-yan-tao-hui-ru-he-dan-sheng-idea-ru-he-gen-shen-gao-ren-rebuttal-ru-he-xie-zi-ji-de-di-yi-pian-ding-hui-wen-zhang-deng-qiang-xian-kan/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"df":0,"c":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":4}}}},"t":{"docs":{},"df":0,"g":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/news/ha-gong-da-zi-ran-yu-yan-chu-li-yan-jiu-suo-gong-kai-chatgptdiao-yan-bao-gao-nei-ce-ha-gong-da-huo-zi-dui-hua-da-mo-xing/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0}},"df":2}}}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":42,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":1}}}}}}}}}},"i":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1,"r":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":7,"i":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"e":{"docs":{},"df":0,"f":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}}},"z":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.4142135623730951},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.4142135623730951}},"df":4}}}}}},"o":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":2,"d":{"docs":{},"df":0,"e":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":2}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0}},"df":3}}},"l":{"docs":{},"df":0,"e":{"docs":{"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"tf":1.0}},"df":4},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"4":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling2024-mian-xiang-bian-cheng-de-zi-ran-yu-yan-chu-li-zong-shu/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"tf":1.0}},"df":3}}},"4":{"docs":{"/news/sai-er-yuan-chuang-coling24-ji-cha-ji-yong-zi-dong-ti-qu-ling-yu-xiang-guan-te-zheng-ti-sheng-fan-hua-neng-li/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"tf":1.0}},"df":2}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"n":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":5}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":2}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":11}}}},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":3}}}},"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0}},"df":3}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":6}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0}},"df":2}}}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":5,"u":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":6}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"u":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.4142135623730951},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0}},"df":9}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":7}}}}}},"r":{"docs":{},"df":0,"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}},"u":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":4}}}}},"s":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}},"t":{"docs":{"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0}},"df":2}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0}},"df":1}}},"f":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0}},"df":1},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":13}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"c":{"docs":{"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0}},"df":1}},"y":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"a":{"docs":{"/resources/社区资源/da/":{"tf":1.0}},"df":1,"t":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.4142135623730951},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":10,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":5}}}}}},"c":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0}},"df":1},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0}},"df":1}}}}}},"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.4142135623730951}},"df":1}}}}},"e":{"docs":{},"df":0,"p":{"docs":{"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":7,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{"/news/ha-gong-da-ju-ban-deepseekji-shu-qian-yan-yu-ying-yong-zhu-ti-jiang-zuo/":{"tf":1.0},"/research/方向介绍/tui-li/":{"tf":1.0}},"df":2}}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"o":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/demo/演示系统/zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0}},"df":3,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1}}}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.4142135623730951},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0}},"df":30}}},"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":14}}}}},"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":4}}}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.4142135623730951},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.4142135623730951},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":8,"u":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.4142135623730951},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0}},"df":12}}}}},"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0}},"df":1}}}}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":2}}}}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0}},"df":1}}}}}},"f":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":7}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0}},"df":2}}},"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0}},"df":2}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":2}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.4142135623730951},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":13}}}},"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":4}}}},"u":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":1}}}}}},"y":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":4}}}}},"e":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":3}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":4}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/resources/开源项目/ltp/":{"tf":1.4142135623730951}},"df":3}}}}},"l":{"docs":{},"df":0,"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{"/resources/社区资源/elmoformanylangs/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":7}}},"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"tf":1.0}},"df":6}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}},"d":{"docs":{"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.4142135623730951},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.4142135623730951},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.4142135623730951},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.4142135623730951}},"df":5,"2":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"）":{"docs":{},"df":0,"，":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"可":{"docs":{},"df":0,"以":{"docs":{},"df":0,"适":{"docs":{},"df":0,"应":{"docs":{},"df":0,"特":{"docs":{},"df":0,"定":{"docs":{},"df":0,"领":{"docs":{},"df":0,"域":{"docs":{},"df":0,"的":{"docs":{},"df":0,"数":{"docs":{},"df":0,"据":{"docs":{},"df":0,"库":{"docs":{},"df":0,"模":{"docs":{},"df":0,"式":{"docs":{},"df":0,"，":{"docs":{},"df":0,"进":{"docs":{},"df":0,"一":{"docs":{},"df":0,"步":{"docs":{},"df":0,"提":{"docs":{},"df":0,"高":{"docs":{},"df":0,"生":{"docs":{},"df":0,"成":{"docs":{},"df":0,"s":{"docs":{},"df":0,"q":{"docs":{},"df":0,"l":{"docs":{"/research/方向介绍/sheng-cheng/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2}}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":8}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":2}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":8}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{},"df":0,"i":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"a":{"docs":{"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":6}}}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":5}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0}},"df":2}}}},"x":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":2}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0}},"df":4}},"r":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":6}}},"m":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":7}}},"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":2}}}}}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.4142135623730951}},"df":1},"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":5}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":5}}}},"w":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":7,"j":{"docs":{},"df":0,"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0}},"df":2}}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"l":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":6}},"n":{"docs":{},"df":0,"d":{"docs":{"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"tf":1.0},"/news/sai-er-yuan-chuang-findings-ji-yu-dong-tai-tu-jiao-hu-wang-luo-de-duo-yi-tu-kou-yu-yu-yan-li-jie-kuang-jia/":{"tf":1.0},"/news/sai-er-yuan-chuang-findings-zhong-wen-yu-xun-lian-yu-yan-mo-xing-hui-gu/":{"tf":1.0}},"df":4},"e":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":10}}}}}}},"e":{"docs":{},"df":0,"e":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":3},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":2}}}}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":10}}}},"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"u":{"docs":{},"df":0,"b":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/demo/演示系统/zhu-suan-sql/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":4}}}}},"l":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":2,"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":3}}},"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"、":{"docs":{},"df":0,"c":{"docs":{},"df":0,"p":{"docs":{},"df":0,"u":{"docs":{"/research/方向介绍/bu-shu/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":2}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0}},"df":6},"t":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":4}}}},"p":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.4142135623730951}},"df":16,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0}},"df":1}}}}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":4}},"p":{"docs":{"/research/方向介绍/tui-li/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":1}}}}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}},"c":{"docs":{"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0}},"df":1},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":3}}}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":2}}}}}}},"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}},"t":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":2.8284271247461903}},"df":14}},"o":{"docs":{},"df":0,"p":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"，":{"docs":{},"df":0,"支":{"docs":{},"df":0,"持":{"docs":{},"df":0,"自":{"docs":{},"df":0,"动":{"docs":{},"df":0,"下":{"docs":{},"df":0,"载":{"docs":{},"df":0,"，":{"docs":{},"df":0,"下":{"docs":{},"df":0,"载":{"docs":{},"df":0,"速":{"docs":{},"df":0,"度":{"docs":{},"df":0,"更":{"docs":{},"df":0,"快":{"docs":{},"df":0,"，":{"docs":{},"df":0,"并":{"docs":{},"df":0,"且":{"docs":{},"df":0,"支":{"docs":{},"df":0,"持":{"docs":{},"df":0,"用":{"docs":{},"df":0,"户":{"docs":{},"df":0,"自":{"docs":{},"df":0,"行":{"docs":{},"df":0,"上":{"docs":{},"df":0,"传":{"docs":{},"df":0,"自":{"docs":{},"df":0,"己":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"的":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"供":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.4142135623730951}},"df":1,"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":3}}}},"y":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":4}}}},"d":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"p":{"docs":{"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"tf":1.0}},"df":1}}}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}}}},"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.4142135623730951}},"df":2}},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":13}}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0}},"df":1}}}}}}},"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0}},"df":1}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0}},"df":3}}},"u":{"docs":{},"df":0,"s":{"docs":{"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1}}},"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":1},"t":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":7}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":7}}},"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0}},"df":2}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}},"r":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"u":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":4}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":2}}}},"j":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"v":{"docs":{},"df":0,"a":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0}},"df":14,"l":{"docs":{},"df":0,"i":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":2}}}}}}},"k":{"docs":{},"df":0,"b":{"docs":{"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0}},"df":1},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0}},"df":6}}}},"y":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":2}}}}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.4142135623730951}},"df":7}}}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{"/resources/社区资源/la-beginner/":{"tf":1.0}},"df":1,"b":{"docs":{"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":15}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/about/":{"tf":1.0},"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0},"/research/方向介绍/ju-shen/":{"tf":1.0},"/resources/开源项目/pyltp/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":41}}}}},"r":{"docs":{},"df":0,"g":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":7,"（":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{"/research/方向介绍/tui-li/":{"tf":1.0}},"df":1}}}}},"y":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"m":{"docs":{},"df":0,"v":{"docs":{},"df":0,"2":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}}}}}}},"）":{"docs":{},"df":0,"是":{"docs":{},"df":0,"将":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"文":{"docs":{},"df":0,"本":{"docs":{},"df":0,"转":{"docs":{},"df":0,"换":{"docs":{},"df":0,"为":{"docs":{},"df":0,"机":{"docs":{},"df":0,"器":{"docs":{},"df":0,"内":{"docs":{},"df":0,"部":{"docs":{},"df":0,"表":{"docs":{},"df":0,"达":{"docs":{},"df":0,"的":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"过":{"docs":{},"df":0,"程":{"docs":{},"df":0,"，":{"docs":{},"df":0,"旨":{"docs":{},"df":0,"在":{"docs":{},"df":0,"帮":{"docs":{},"df":0,"助":{"docs":{},"df":0,"计":{"docs":{},"df":0,"算":{"docs":{},"df":0,"机":{"docs":{},"df":0,"通":{"docs":{},"df":0,"语":{"docs":{},"df":0,"法":{"docs":{},"df":0,"、":{"docs":{},"df":0,"解":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"、":{"docs":{},"df":0,"知":{"docs":{},"df":0,"语":{"docs":{},"df":0,"用":{"docs":{},"df":0,"，":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"，":{"docs":{},"df":0,"构":{"docs":{},"df":0,"建":{"docs":{},"df":0,"智":{"docs":{},"df":0,"能":{"docs":{},"df":0,"。":{"docs":{},"df":0,"哈":{"docs":{},"df":0,"工":{"docs":{},"df":0,"大":{"docs":{},"df":0,"社":{"docs":{},"df":0,"会":{"docs":{},"df":0,"计":{"docs":{},"df":0,"算":{"docs":{},"df":0,"与":{"docs":{},"df":0,"信":{"docs":{},"df":0,"息":{"docs":{},"df":0,"检":{"docs":{},"df":0,"索":{"docs":{},"df":0,"研":{"docs":{},"df":0,"究":{"docs":{},"df":0,"中":{"docs":{},"df":0,"心":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"组":{"docs":{},"df":0,"（":{"docs":{},"df":0,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"/about/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"长":{"docs":{},"df":0,"期":{"docs":{},"df":0,"深":{"docs":{},"df":0,"入":{"docs":{},"df":0,"研":{"docs":{},"df":0,"究":{"docs":{},"df":0,"通":{"docs":{},"df":0,"用":{"docs":{},"df":0,"和":{"docs":{},"df":0,"专":{"docs":{},"df":0,"用":{"docs":{},"df":0,"场":{"docs":{},"df":0,"景":{"docs":{},"df":0,"下":{"docs":{},"df":0,"的":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"。":{"docs":{},"df":0,"通":{"docs":{},"df":0,"用":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"从":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"学":{"docs":{},"df":0,"角":{"docs":{},"df":0,"度":{"docs":{},"df":0,"解":{"docs":{},"df":0,"析":{"docs":{},"df":0,"语":{"docs":{},"df":0,"句":{"docs":{},"df":0,"，":{"docs":{},"df":0,"以":{"docs":{},"df":0,"词":{"docs":{},"df":0,"法":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"（":{"docs":{},"df":0,"分":{"docs":{},"df":0,"词":{"docs":{},"df":0,"、":{"docs":{},"df":0,"词":{"docs":{},"df":0,"性":{"docs":{},"df":0,"标":{"docs":{},"df":0,"注":{"docs":{},"df":0,"、":{"docs":{},"df":0,"命":{"docs":{},"df":0,"名":{"docs":{},"df":0,"实":{"docs":{},"df":0,"体":{"docs":{},"df":0,"识":{"docs":{},"df":0,"别":{"docs":{},"df":0,"）":{"docs":{},"df":0,"、":{"docs":{},"df":0,"句":{"docs":{},"df":0,"法":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"（":{"docs":{},"df":0,"依":{"docs":{},"df":0,"存":{"docs":{},"df":0,"句":{"docs":{},"df":0,"法":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"）":{"docs":{},"df":0,"、":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"（":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"角":{"docs":{},"df":0,"色":{"docs":{},"df":0,"标":{"docs":{},"df":0,"注":{"docs":{},"df":0,"、":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"依":{"docs":{},"df":0,"存":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"）":{"docs":{},"df":0,"为":{"docs":{},"df":0,"代":{"docs":{},"df":0,"表":{"docs":{},"df":0,"。":{"docs":{},"df":0,"专":{"docs":{},"df":0,"用":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"为":{"docs":{},"df":0,"各":{"docs":{},"df":0,"种":{"docs":{},"df":0,"实":{"docs":{},"df":0,"用":{"docs":{},"df":0,"场":{"docs":{},"df":0,"景":{"docs":{},"df":0,"的":{"docs":{},"df":0,"人":{"docs":{},"df":0,"机":{"docs":{},"df":0,"交":{"docs":{},"df":0,"互":{"docs":{},"df":0,"提":{"docs":{},"df":0,"供":{"docs":{},"df":0,"支":{"docs":{},"df":0,"持":{"docs":{},"df":0,"，":{"docs":{},"df":0,"涉":{"docs":{},"df":0,"及":{"docs":{},"df":0,"任":{"docs":{},"df":0,"务":{"docs":{},"df":0,"型":{"docs":{},"df":0,"对":{"docs":{},"df":0,"话":{"docs":{},"df":0,"系":{"docs":{},"df":0,"统":{"docs":{},"df":0,"、":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"解":{"docs":{},"df":0,"析":{"docs":{},"df":0,"、":{"docs":{},"df":0,"代":{"docs":{},"df":0,"码":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"与":{"docs":{},"df":0,"生":{"docs":{},"df":0,"成":{"docs":{},"df":0,"等":{"docs":{},"df":0,"领":{"docs":{},"df":0,"域":{"docs":{},"df":0,"。":{"docs":{},"df":0,"研":{"docs":{},"df":0,"究":{"docs":{},"df":0,"组":{"docs":{},"df":0,"目":{"docs":{},"df":0,"前":{"docs":{},"df":0,"承":{"docs":{},"df":0,"担":{"docs":{},"df":0,"国":{"docs":{},"df":0,"家":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"科":{"docs":{},"df":0,"学":{"docs":{},"df":0,"基":{"docs":{},"df":0,"金":{"docs":{},"df":0,"重":{"docs":{},"df":0,"点":{"docs":{},"df":0,"项":{"docs":{},"df":0,"目":{"docs":{},"df":0,"、":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"3":{"docs":{},"df":0,"0":{"docs":{},"df":0,"“":{"docs":{},"df":0,"新":{"docs":{},"df":0,"一":{"docs":{},"df":0,"代":{"docs":{},"df":0,"人":{"docs":{},"df":0,"工":{"docs":{},"df":0,"智":{"docs":{},"df":0,"能":{"docs":{},"df":0,"”":{"docs":{},"df":0,"重":{"docs":{},"df":0,"大":{"docs":{},"df":0,"项":{"docs":{},"df":0,"目":{"docs":{},"df":0,"课":{"docs":{},"df":0,"题":{"docs":{},"df":0,"等":{"docs":{},"df":0,"多":{"docs":{},"df":0,"项":{"docs":{},"df":0,"科":{"docs":{},"df":0,"研":{"docs":{},"df":0,"项":{"docs":{},"df":0,"目":{"docs":{},"df":0,"。":{"docs":{},"df":0,"组":{"docs":{},"df":0,"内":{"docs":{},"df":0,"研":{"docs":{},"df":0,"发":{"docs":{},"df":0,"的":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"平":{"docs":{},"df":0,"台":{"docs":{},"df":0,"（":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{},"df":0,"）":{"docs":{},"df":0,"已":{"docs":{},"df":0,"被":{"docs":{},"df":0,"6":{"docs":{},"df":0,"0":{"docs":{},"df":0,"0":{"docs":{},"df":0,"余":{"docs":{},"df":0,"家":{"docs":{},"df":0,"单":{"docs":{},"df":0,"位":{"docs":{},"df":0,"共":{"docs":{},"df":0,"享":{"docs":{},"df":0,"，":{"docs":{},"df":0,"并":{"docs":{},"df":0,"授":{"docs":{},"df":0,"权":{"docs":{},"df":0,"给":{"docs":{},"df":0,"百":{"docs":{},"df":0,"度":{"docs":{},"df":0,"、":{"docs":{},"df":0,"腾":{"docs":{},"df":0,"讯":{"docs":{},"df":0,"、":{"docs":{},"df":0,"华":{"docs":{},"df":0,"为":{"docs":{},"df":0,"等":{"docs":{},"df":0,"公":{"docs":{},"df":0,"司":{"docs":{},"df":0,"使":{"docs":{},"df":0,"用":{"docs":{},"df":0,"，":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"6":{"docs":{},"df":0,"年":{"docs":{},"df":0,"获":{"docs":{},"df":0,"得":{"docs":{},"df":0,"黑":{"docs":{},"df":0,"龙":{"docs":{},"df":0,"江":{"docs":{},"df":0,"省":{"docs":{},"df":0,"科":{"docs":{},"df":0,"技":{"docs":{},"df":0,"进":{"docs":{},"df":0,"步":{"docs":{},"df":0,"一":{"docs":{},"df":0,"等":{"docs":{},"df":0,"奖":{"docs":{},"df":0,"，":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"8":{"docs":{},"df":0,"、":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{},"df":0,"连":{"docs":{},"df":0,"续":{"docs":{},"df":0,"两":{"docs":{},"df":0,"年":{"docs":{},"df":0,"获":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{"/about/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.4142135623730951},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.4142135623730951},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.4142135623730951},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":28}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3},"y":{"docs":{},"df":0,"9":{"docs":{},"df":0,"7":{"docs":{},"df":0,".":{"docs":{},"df":0,"9":{"docs":{},"df":0,"3":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{},"df":0,"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"8":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{},"df":0,"5":{"docs":{},"df":0,"8":{"docs":{},"df":0,"1":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"8":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":4},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1}}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0}},"df":3}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0}},"df":1}}},"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.4142135623730951},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":12}}}},"k":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}}}}}}},"m":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":2},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"g":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"k":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}},"w":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"m":{"docs":{"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0}},"df":2}}},"t":{"docs":{},"df":0,";":{"docs":{},"df":0,"&":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,";":{"docs":{},"df":0,"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"@":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,".":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,".":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,".":{"docs":{},"df":0,"c":{"docs":{},"df":0,"n":{"docs":{},"df":0,"&":{"docs":{},"df":0,"g":{"docs":{},"df":0,"t":{"docs":{},"df":0,";":{"docs":{},"df":0,"&":{"docs":{},"df":0,"g":{"docs":{},"df":0,"t":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{"/demo/演示系统/ltp/":{"tf":3.1622776601683795},"/news/ha-gong-da-ltpyu-yan-ji-shu-ping-tai-zheng-shi-shang-xian-guo-jia-zhi-hui-jiao-yu-gong-gong-fu-wu-ping-tai/":{"tf":1.0},"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"tf":1.0},"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"tf":1.0},"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"tf":1.0},"/news/yu-yan-ji-shu-ping-tai-ltp-you-xin-jia-la/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":3.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":3.1622776601683795},"/resources/开源项目/pyltp/":{"tf":1.4142135623730951}},"df":11,"3":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3},"（":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}},"m":{"docs":{},"df":0,"3":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":1}}}},"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":11}}}},"k":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"k":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"k":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"u":{"docs":{},"df":0,"m":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"b":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":2}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":2}}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"：":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"元":{"docs":{},"df":0,"学":{"docs":{},"df":0,"习":{"docs":{},"df":0,"的":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":3}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":2,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":1}}}}}}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0}},"df":1}}},"x":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":2}}}}}},"l":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":4}},"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":32,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.4142135623730951}},"df":1}}}}}},"u":{"docs":{},"df":0,"l":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":2}}}}},"r":{"docs":{},"df":0,"e":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"p":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.4142135623730951},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.7320508075688772},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":17,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0}},"df":3}}}}},"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"r":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}}}},"n":{"docs":{"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":2,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0}},"df":2,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0}},"df":7}}}},"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":9}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":9}},"i":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0}},"df":1}}}},"w":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":3}},"l":{"docs":{"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0}},"df":1,"g":{"docs":{"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":2},"p":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"c":{"docs":{"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":1,"i":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}},"n":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":2}}}},"u":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":2}}}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":6,"s":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":2}}}},"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":11}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0}},"df":1,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":1}}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0}},"df":1}}}}}},"s":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.4142135623730951},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":36,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.4142135623730951},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":6}}}},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":2}}}}},"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":4}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":3},"t":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0},"/resources/开源项目/pyltp/":{"tf":1.0}},"df":6}}}}}}},"o":{"docs":{"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0}},"df":7,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"公":{"docs":{},"df":0,"开":{"docs":{},"df":0,"！":{"docs":{},"df":0,"“":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"前":{"docs":{},"df":0,"沿":{"docs":{},"df":0,"与":{"docs":{},"df":0,"应":{"docs":{},"df":0,"用":{"docs":{},"df":0,"”":{"docs":{},"df":0,"专":{"docs":{},"df":0,"题":{"docs":{},"df":0,"讲":{"docs":{},"df":0,"座":{"docs":{},"df":0,"，":{"docs":{},"df":0,"1":{"docs":{},"df":0,"小":{"docs":{},"df":0,"时":{"docs":{},"df":0,"快":{"docs":{},"df":0,"速":{"docs":{},"df":0,"掌":{"docs":{},"df":0,"握":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{"/news/ha-gong-da-pptgong-kai-deepseekji-shu-qian-yan-yu-ying-yong-zhuan-ti-jiang-zuo-1xiao-shi-kuai-su-zhang-wo-deepseekji-ben-yuan-li/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":10,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":3}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0}},"df":1}}}}}}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}},"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":5}}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":2}},"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":1}}}}},"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":7}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0}},"df":1}}}}}}},"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{"/resources/开源项目/pyltp/":{"tf":2.0}},"df":1,"将":{"docs":{},"df":0,"会":{"docs":{},"df":0,"只":{"docs":{},"df":0,"有":{"docs":{},"df":0,"非":{"docs":{},"df":0,"常":{"docs":{},"df":0,"有":{"docs":{},"df":0,"限":{"docs":{},"df":0,"的":{"docs":{},"df":0,"维":{"docs":{},"df":0,"护":{"docs":{},"df":0,"，":{"docs":{},"df":0,"请":{"docs":{},"df":0,"大":{"docs":{},"df":0,"家":{"docs":{},"df":0,"移":{"docs":{},"df":0,"步":{"docs":{},"df":0,"使":{"docs":{},"df":0,"用":{"docs":{},"df":0,"[":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{"/resources/开源项目/pyltp/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/publications/2016-python-cheng-xu-she-ji/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.4142135623730951},"/resources/开源项目/pyltp/":{"tf":1.0}},"df":5}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/resources/开源项目/ltp/":{"tf":1.4142135623730951}},"df":3,"的":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{},"df":0,"4":{"docs":{"/resources/开源项目/pyltp/":{"tf":1.0}},"df":1}}}}}}}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"i":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":2}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"1":{"docs":{"/research/方向介绍/tui-li/":{"tf":1.0}},"df":1},"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":12},"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.4142135623730951},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.4142135623730951},"/research/方向介绍/tui-li/":{"tf":1.0}},"df":6}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":6}}}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":4}}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1}}}},"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":2}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":6}},"e":{"docs":{},"df":0,"v":{"docs":{"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":2}}}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}}},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":11}}}}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":3}}}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":5}}}}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0}},"df":2}}}}},"w":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":12}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/demo/演示系统/ltp/":{"tf":2.23606797749979},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":2.23606797749979},"/resources/开源项目/ltp/":{"tf":2.23606797749979}},"df":3}}}},"s":{"docs":{"/demo/演示系统/ltp/":{"tf":1.4142135623730951},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.4142135623730951},"/resources/开源项目/ltp/":{"tf":1.4142135623730951}},"df":3,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":2}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}},"r":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"tf":1.0},"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-da-yu-yan-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"tf":1.0},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-huo-de-di-liu-jie-bai-du-jiang-xue-jin/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-yin-qing-yu-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-de-di-ba-jie-bai-du-jiang-xue-jin/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-xu-jun-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirche-mo-xiang-liu-ting-zi-ran-yu-yan-chu-li-xin-fan-shi-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"tf":1.0},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-jing-dong-wang-yu-xuan-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-shi-xiao-ming-hou-yu-tai-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirsan-wei-bo-shi-sheng-li-jia-qi-yuan-jian-hua-liu-ze-ming-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0},"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"tf":1.0},"/news/zui-xin-ha-gong-da-scirzai-guo-ji-duo-yu-yan-tong-yong-yi-cun-fen-xi-ping-ce-zhong-duo-de-guan-jun/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":2.8284271247461903}},"df":34,"&":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"&":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}},"《":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"：":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"预":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"的":{"docs":{},"df":0,"方":{"docs":{},"df":0,"法":{"docs":{},"df":0,"》":{"docs":{},"df":0,"一":{"docs":{},"df":0,"书":{"docs":{},"df":0,"入":{"docs":{},"df":0,"选":{"docs":{},"df":0,"中":{"docs":{},"df":0,"国":{"docs":{},"df":0,"知":{"docs":{},"df":0,"网":{"docs":{},"df":0,"“":{"docs":{},"df":0,"高":{"docs":{},"df":0,"被":{"docs":{},"df":0,"引":{"docs":{},"df":0,"图":{"docs":{},"df":0,"书":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"三":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"tf":1.0}},"df":1}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"九":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"八":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"tf":1.0}},"df":1}}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"六":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"文":{"docs":{},"df":0,"章":{"docs":{},"df":0,"被":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"tf":1.0}},"df":1}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"十":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"博":{"docs":{},"df":0,"士":{"docs":{},"df":0,"生":{"docs":{},"df":0,"覃":{"docs":{},"df":0,"立":{"docs":{},"df":0,"波":{"docs":{},"df":0,"获":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-2021nian-wei-ruan-xue-zhe-cheng-hao/":{"tf":1.0}},"df":1}}}}}}}}}}},"发":{"docs":{},"df":0,"布":{"docs":{},"df":0,"首":{"docs":{},"df":0,"个":{"docs":{},"df":0,"中":{"docs":{},"df":0,"文":{"docs":{},"df":0,"扩":{"docs":{},"df":0,"词":{"docs":{},"df":0,"表":{"docs":{},"df":0,"增":{"docs":{},"df":0,"量":{"docs":{},"df":0,"预":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"混":{"docs":{},"df":0,"合":{"docs":{},"df":0,"专":{"docs":{},"df":0,"家":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"取":{"docs":{},"df":0,"得":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0}},"df":1}}}},"国":{"docs":{},"df":0,"家":{"docs":{},"df":0,"电":{"docs":{},"df":0,"网":{"docs":{},"df":0,"调":{"docs":{},"df":0,"控":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"创":{"docs":{},"df":0,"新":{"docs":{},"df":0,"大":{"docs":{},"df":0,"赛":{"docs":{},"df":0,"赛":{"docs":{},"df":0,"道":{"docs":{},"df":0,"2":{"docs":{},"df":0,"（":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"x":{"docs":{},"df":0,"t":{"docs":{},"df":0,"2":{"docs":{},"df":0,"s":{"docs":{},"df":0,"q":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirqu-de-guo-jia-dian-wang-diao-kong-aichuang-xin-da-sai-sai-dao-2-text2sql-guan-jun/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"tf":1.0},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"tf":1.0}},"df":2}}}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0}},"df":1}}}}}},"多":{"docs":{},"df":0,"位":{"docs":{},"df":0,"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"受":{"docs":{},"df":0,"邀":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"第":{"docs":{},"df":0,"一":{"docs":{},"df":0,"届":{"docs":{},"df":0,"中":{"docs":{},"df":0,"国":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"学":{"docs":{},"df":0,"生":{"docs":{},"df":0,"研":{"docs":{},"df":0,"讨":{"docs":{},"df":0,"会":{"docs":{},"df":0,"（":{"docs":{},"df":0,"c":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"名":{"docs":{},"df":0,"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"tf":1.0}},"df":1}}}}}}}}},"教":{"docs":{},"df":0,"师":{"docs":{},"df":0,"受":{"docs":{},"df":0,"邀":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"tf":1.0}},"df":1}},"l":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"tf":1.0}},"df":2}}},"第":{"docs":{},"df":0,"八":{"docs":{},"df":0,"届":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"和":{"docs":{},"df":0,"中":{"docs":{},"df":0,"文":{"docs":{},"df":0,"计":{"docs":{},"df":0,"算":{"docs":{},"df":0,"会":{"docs":{},"df":0,"议":{"docs":{},"df":0,"（":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"车":{"docs":{},"df":0,"万":{"docs":{},"df":0,"翔":{"docs":{},"df":0,"教":{"docs":{},"df":0,"授":{"docs":{},"df":0,"入":{"docs":{},"df":0,"选":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-ru-xuan-2019nian-du-long-jiang-xue-zhe-qing-nian-xue-zhe/":{"tf":1.0}},"df":1}}}}}},"当":{"docs":{},"df":0,"选":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"tf":1.0}},"df":1}}}}}}}}}}}}},"d":{"docs":{},"df":0,"p":{"docs":{},"df":0,"和":{"docs":{},"df":0,"s":{"docs":{},"df":0,"d":{"docs":{},"df":0,"p":{"docs":{},"df":0,"g":{"docs":{},"df":0,"很":{"docs":{},"df":0,"大":{"docs":{},"df":0,"一":{"docs":{},"df":0,"部":{"docs":{},"df":0,"分":{"docs":{},"df":0,"是":{"docs":{},"df":0,"重":{"docs":{},"df":0,"叠":{"docs":{},"df":0,"的":{"docs":{},"df":0,"，":{"docs":{},"df":0,"重":{"docs":{},"df":0,"用":{"docs":{},"df":0,"可":{"docs":{},"df":0,"以":{"docs":{},"df":0,"加":{"docs":{},"df":0,"快":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"速":{"docs":{},"df":0,"度":{"docs":{},"df":0,"）":{"docs":{},"df":0,"，":{"docs":{},"df":0,"使":{"docs":{},"df":0,"用":{"docs":{},"df":0,"说":{"docs":{},"df":0,"明":{"docs":{},"df":0,"参":{"docs":{},"df":0,"见":{"docs":{},"df":0,"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"u":{"docs":{},"df":0,"b":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":5}}}},"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.4142135623730951},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":9}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0}},"df":1}}},"f":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.4142135623730951},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":6}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":27}}},"e":{"docs":{},"df":0,"v":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":2}},"i":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":4}},"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":5,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0}},"df":7}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":3}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":2}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.4142135623730951},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.4142135623730951}},"df":3}}}}},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":2}},"t":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":11,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}}}}}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":3}},"u":{"docs":{},"df":0,"l":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":9}},"u":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0}},"df":1}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"9":{"docs":{},"df":0,"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"3":{"docs":{},"df":0,"7":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"4":{"docs":{},"df":0,"8":{"docs":{},"df":0,"8":{"docs":{},"df":0,".":{"docs":{},"df":0,"3":{"docs":{},"df":0,"7":{"docs":{},"df":0,"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"7":{"docs":{},"df":0,"4":{"docs":{},"df":0,"3":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"3":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":3}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0}},"df":2}},"n":{"docs":{"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":2}}}},"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":2}}},"l":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":10}}}}},"q":{"docs":{},"df":0,"l":{"docs":{"/demo/演示系统/zhu-suan-sql/":{"tf":1.4142135623730951},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0},"/research/方向介绍/sheng-cheng/":{"tf":1.0}},"df":9,"任":{"docs":{},"df":0,"务":{"docs":{},"df":0,"旨":{"docs":{},"df":0,"在":{"docs":{},"df":0,"将":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"问":{"docs":{},"df":0,"题":{"docs":{},"df":0,"自":{"docs":{},"df":0,"动":{"docs":{},"df":0,"转":{"docs":{},"df":0,"化":{"docs":{},"df":0,"为":{"docs":{},"df":0,"可":{"docs":{},"df":0,"执":{"docs":{},"df":0,"行":{"docs":{},"df":0,"的":{"docs":{},"df":0,"s":{"docs":{},"df":0,"q":{"docs":{},"df":0,"l":{"docs":{},"df":0,"查":{"docs":{},"df":0,"询":{"docs":{},"df":0,"语":{"docs":{},"df":0,"句":{"docs":{},"df":0,"，":{"docs":{},"df":0,"从":{"docs":{},"df":0,"而":{"docs":{},"df":0,"实":{"docs":{},"df":0,"现":{"docs":{},"df":0,"用":{"docs":{},"df":0,"户":{"docs":{},"df":0,"通":{"docs":{},"df":0,"过":{"docs":{},"df":0,"日":{"docs":{},"df":0,"常":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"与":{"docs":{},"df":0,"数":{"docs":{},"df":0,"据":{"docs":{},"df":0,"库":{"docs":{},"df":0,"交":{"docs":{},"df":0,"互":{"docs":{},"df":0,"的":{"docs":{},"df":0,"目":{"docs":{},"df":0,"标":{"docs":{},"df":0,"。":{"docs":{},"df":0,"这":{"docs":{},"df":0,"一":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"在":{"docs":{},"df":0,"数":{"docs":{},"df":0,"据":{"docs":{},"df":0,"分":{"docs":{},"df":0,"析":{"docs":{},"df":0,"、":{"docs":{},"df":0,"商":{"docs":{},"df":0,"业":{"docs":{},"df":0,"智":{"docs":{},"df":0,"能":{"docs":{},"df":0,"等":{"docs":{},"df":0,"领":{"docs":{},"df":0,"域":{"docs":{},"df":0,"具":{"docs":{},"df":0,"有":{"docs":{},"df":0,"重":{"docs":{},"df":0,"要":{"docs":{},"df":0,"应":{"docs":{},"df":0,"用":{"docs":{},"df":0,"价":{"docs":{},"df":0,"值":{"docs":{},"df":0,"。":{"docs":{},"df":0,"随":{"docs":{},"df":0,"着":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"（":{"docs":{},"df":0,"如":{"docs":{},"df":0,"g":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"、":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"等":{"docs":{},"df":0,"）":{"docs":{},"df":0,"的":{"docs":{},"df":0,"兴":{"docs":{},"df":0,"起":{"docs":{},"df":0,"，":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"x":{"docs":{},"df":0,"t":{"docs":{"/research/方向介绍/sheng-cheng/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"df":0,"能":{"docs":{},"df":0,"力":{"docs":{},"df":0,"得":{"docs":{},"df":0,"到":{"docs":{},"df":0,"了":{"docs":{},"df":0,"显":{"docs":{},"df":0,"著":{"docs":{},"df":0,"提":{"docs":{},"df":0,"升":{"docs":{},"df":0,"。":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"凭":{"docs":{},"df":0,"借":{"docs":{},"df":0,"其":{"docs":{},"df":0,"强":{"docs":{},"df":0,"大":{"docs":{},"df":0,"的":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"能":{"docs":{},"df":0,"力":{"docs":{},"df":0,"和":{"docs":{},"df":0,"上":{"docs":{},"df":0,"下":{"docs":{},"df":0,"文":{"docs":{},"df":0,"学":{"docs":{},"df":0,"习":{"docs":{},"df":0,"能":{"docs":{},"df":0,"力":{"docs":{},"df":0,"，":{"docs":{},"df":0,"能":{"docs":{},"df":0,"够":{"docs":{},"df":0,"更":{"docs":{},"df":0,"好":{"docs":{},"df":0,"地":{"docs":{},"df":0,"解":{"docs":{},"df":0,"析":{"docs":{},"df":0,"复":{"docs":{},"df":0,"杂":{"docs":{},"df":0,"语":{"docs":{},"df":0,"义":{"docs":{},"df":0,"、":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"多":{"docs":{},"df":0,"表":{"docs":{},"df":0,"关":{"docs":{},"df":0,"联":{"docs":{},"df":0,"以":{"docs":{},"df":0,"及":{"docs":{},"df":0,"生":{"docs":{},"df":0,"成":{"docs":{},"df":0,"高":{"docs":{},"df":0,"质":{"docs":{},"df":0,"量":{"docs":{},"df":0,"的":{"docs":{},"df":0,"s":{"docs":{},"df":0,"q":{"docs":{},"df":0,"l":{"docs":{},"df":0,"语":{"docs":{},"df":0,"句":{"docs":{},"df":0,"。":{"docs":{},"df":0,"此":{"docs":{},"df":0,"外":{"docs":{},"df":0,"，":{"docs":{},"df":0,"通":{"docs":{},"df":0,"过":{"docs":{},"df":0,"微":{"docs":{},"df":0,"调":{"docs":{},"df":0,"或":{"docs":{},"df":0,"提":{"docs":{},"df":0,"示":{"docs":{},"df":0,"工":{"docs":{},"df":0,"程":{"docs":{},"df":0,"（":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/research/方向介绍/sheng-cheng/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"df":0,"l":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":2}},"g":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2}}}}},"t":{"docs":{},"df":0,"e":{"docs":{"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0}},"df":1,"2":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"q":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":2}}}}},"e":{"docs":{},"df":0,"p":{"docs":{"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":1}},"h":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":5}}}}}},"u":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0}},"df":1}}},"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":11}}}}}},"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"y":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":9}}}}},"w":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}}}},"y":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":6}},"x":{"docs":{"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":11}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}},"c":{"docs":{"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":1},"g":{"docs":{"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":9},"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"k":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":18}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0},"/resources/开源项目/pyltp/":{"tf":1.0}},"df":8}}}}}}},"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"t":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0},"/research/方向介绍/sheng-cheng/":{"tf":1.4142135623730951}},"df":12,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.4142135623730951},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"y":{"docs":{},"df":0,"9":{"docs":{},"df":0,"6":{"docs":{},"df":0,".":{"docs":{},"df":0,"8":{"docs":{},"df":0,"9":{"docs":{},"df":0,"7":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{},"df":0,"1":{"docs":{},"df":0,".":{"docs":{},"df":0,"6":{"docs":{},"df":0,"7":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"9":{"docs":{},"df":0,"8":{"docs":{},"df":0,"3":{"docs":{},"df":0,".":{"docs":{},"df":0,"8":{"docs":{},"df":0,"7":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"1":{"docs":{},"df":0,"5":{"docs":{},"df":0,"3":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{},"df":0,"2":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"df":0,"d":{"docs":{"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":2},"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1,"k":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":2}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":6}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":17},"t":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0}},"df":5}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0}},"df":2}}}},"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":6}}}}},"e":{"docs":{},"df":0,"e":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0}},"df":5,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":6}}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":4}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0}},"df":1}}},"o":{"docs":{"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":1}},"y":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0}},"df":4}}}},"u":{"docs":{},"df":0,"d":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":1},"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":21}}}}}}}},"i":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1,"f":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":4}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":1}}},"t":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0}},"df":4}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":3}}}}}}}}},"s":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":15,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"3":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3},"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":7},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"基":{"docs":{},"df":0,"座":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"：":{"docs":{},"df":0,"探":{"docs":{},"df":0,"索":{"docs":{},"df":0,"v":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"（":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"u":{"docs":{"/research/方向介绍/ju-shen/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}}}}}}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"/demo/演示系统/zhu-suan-vscodecha-jian/":{"tf":1.4142135623730951},"/news/ha-gong-da-zhu-suan-da-mo-xing-tui-chu-vscodecha-jian/":{"tf":1.0}},"df":2}}}}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":2,"l":{"docs":{},"df":0,"i":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}}}},"b":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/projects/yu-yan-ji-shu-ping-tai/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.4142135623730951}},"df":1}}}}}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.4142135623730951},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.4142135623730951}},"df":19}}},"s":{"docs":{},"df":0,"d":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.4142135623730951},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0}},"df":4}}},"y":{"docs":{},"df":0,"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{"/news/yef2021yan-jiang-shi-lu-ha-er-bin-gong-ye-da-xue-che-mo-xiang-zi-ran-yu-yan-chu-li-xin-fan-shi/":{"tf":1.0}},"df":1}}}}}}},"z":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0}},"df":5,"仅":{"docs":{},"df":0,"仅":{"docs":{},"df":0,"通":{"docs":{},"df":0,"过":{"docs":{},"df":0,"强":{"docs":{},"df":0,"化":{"docs":{},"df":0,"学":{"docs":{},"df":0,"习":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"，":{"docs":{},"df":0,"使":{"docs":{},"df":0,"数":{"docs":{},"df":0,"学":{"docs":{},"df":0,"推":{"docs":{},"df":0,"理":{"docs":{},"df":0,"的":{"docs":{},"df":0,"准":{"docs":{},"df":0,"确":{"docs":{},"df":0,"率":{"docs":{},"df":0,"从":{"docs":{},"df":0,"1":{"docs":{},"df":0,"5":{"docs":{},"df":0,".":{"docs":{},"df":0,"6":{"docs":{},"df":0,"%":{"docs":{},"df":0,"跃":{"docs":{},"df":0,"升":{"docs":{},"df":0,"至":{"docs":{},"df":0,"7":{"docs":{},"df":0,"1":{"docs":{"/research/方向介绍/tui-li/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"title":{"root":{"docs":{},"df":0,"1":{"docs":{},"df":0,"%":{"docs":{},"df":0,"（":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0}},"df":1}}}}}},"3":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}},"4":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}}}},"2":{"docs":{},"df":0,"0":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":2,"1":{"docs":{},"df":0,"0":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0}},"df":1},"1":{"docs":{"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":1},"2":{"docs":{"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":1},"6":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0}},"df":1},"7":{"docs":{"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"tf":1.0}},"df":4},"8":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"tf":1.0},"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"tf":1.0},"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"tf":1.0},"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"tf":1.0},"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":8},"9":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"tf":1.0},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0}},"df":8}},"2":{"docs":{},"df":0,"0":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0},"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"tf":1.0},"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":9},"1":{"docs":{"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":12,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"s":{"docs":{},"df":0,"和":{"docs":{},"df":0,"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}},"2":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0},"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"tf":1.0}},"df":5,"|":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"tf":1.0}},"df":1}}}}}}}}}},"3":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0}},"df":3,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}},"届":{"docs":{},"df":0,"2":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"tf":1.0}},"df":1}}}},"4":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/ha-gong-da-xun-fei-rong-huo-2024nian-du-wu-wen-jun-ren-gong-zhi-neng-ke-xue-ji-shu-jiang-ke-ji-jin-bu-jiang-yi-deng-jiang/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"tf":1.0}},"df":3,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":2}}}}}}}},"5":{"docs":{"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0}},"df":5,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":2}}}}}}}}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}},"2":{"docs":{},"df":0,"多":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"任":{"docs":{},"df":0,"务":{"docs":{},"df":0,"型":{"docs":{},"df":0,"对":{"docs":{},"df":0,"话":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"理":{"docs":{},"df":0,"解":{"docs":{},"df":0,"评":{"docs":{},"df":0,"测":{"docs":{},"df":0,"取":{"docs":{},"df":0,"得":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"9":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0}},"df":1}}}}}}}}},"3":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing-3-0ban-ben/":{"tf":1.0}},"df":1},"5":{"docs":{"/news/ha-gong-da-kai-yuan-huo-zi-3-5-dui-hua-da-mo-xing/":{"tf":1.0}},"df":1}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"l":{"docs":{"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"0":{"docs":{},"df":0,"！":{"docs":{},"df":0,"单":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"完":{"docs":{},"df":0,"成":{"docs":{},"df":0,"6":{"docs":{"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"tf":1.0}},"df":1}}}}}}}}}},"5":{"docs":{"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":1},"6":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"主":{"docs":{},"df":0,"会":{"docs":{},"df":0,"/":{"docs":{},"df":0,"2":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"s":{"docs":{},"df":0,"/":{"docs":{},"df":0,"1":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"o":{"docs":{"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"8":{"docs":{"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0}},"df":1,"x":{"docs":{},"df":0,"7":{"docs":{},"df":0,"b":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":2}}}},"9":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0}},"df":2,"0":{"docs":{},"df":0,"0":{"docs":{"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"tf":1.0}},"df":1}},"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"tf":1.0}},"df":5,"2":{"docs":{},"df":0,"0":{"docs":{"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"tf":1.0},"/news/sai-er-yuan-chuang-aaai20-yong-yu-lian-he-jian-mo-dui-hua-xing-wei-shi-bie-he-qing-gan-fen-lei-de-shen-du-jiao-hu-guan-xi-wang-luo/":{"tf":1.0}},"df":2,"2":{"docs":{},"df":0,"1":{"docs":{"/news/sai-er-yuan-chuang-aaai2021-xiao-yang-ben-xue-xi-xia-de-duo-biao-qian-fen-lei-wen-ti-chu-tan/":{"tf":1.0}},"df":1}}}}}}},"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{"/resources/开源项目/abacus/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":2}}},"l":{"docs":{"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"tf":1.0}},"df":7,"2":{"docs":{},"df":0,"0":{"docs":{"/news/sai-er-yuan-chuang-acl20-ji-yu-dui-hua-tu-pu-de-kai-fang-yu-duo-lun-dui-hua-ce-lue-xue-xi/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-ji-yu-tu-zhu-yi-li-wang-luo-de-duo-li-du-ji-qi-yue-du-li-jie-wen-dang-jian-mo/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-rang-mo-xing-shi-ban-gong-bei-tan-jiu-shao-yang-ben-xu-lie-biao-zhu-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl20-yong-yu-duo-ling-yu-duan-dao-duan-ren-wu-xing-dui-hua-xi-tong-de-dong-tai-rong-he-wang-luo/":{"tf":1.0}},"df":4}},"@":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"2":{"docs":{"/news/acl-2022-fan-xiang-yu-ce-geng-hao-ji-yu-fan-xiang-ti-shi-de-xiao-yang-ben-cao-wei-biao-zhu-fang-fa/":{"tf":1.0}},"df":1}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0}},"df":2}}}},"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":9}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0}},"df":4}}}}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1}}},"i":{"docs":{"/news/ha-gong-da-jiu-da-aimo-xing-deng-chang-jie-suo-qian-xing-bai-ye-zhi-neng-xin-fan-shi/":{"tf":1.0},"/news/qing-chun-de-xuan-ze-gun-ha-gong-da-zhe-ge-tuan-dui-li-yu-sheng-cheng-shi-aichao-tou/":{"tf":1.0},"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":4},"l":{"docs":{},"df":0,"g":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":3}}}}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":5}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"r":{"docs":{"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":1,"i":{"docs":{"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":1}}}}}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0}},"df":2}}},"s":{"docs":{},"df":0,"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":2}}}},"y":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"i":{"docs":{"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":2},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":6}}}}}}},"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0}},"df":1,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0}},"df":2}}}}}}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"最":{"docs":{},"df":0,"热":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"：":{"docs":{},"df":0,"做":{"docs":{},"df":0,"到":{"docs":{},"df":0,"头":{"docs":{},"df":0,"了":{"docs":{},"df":0,"！":{"docs":{},"df":0,"清":{"docs":{},"df":0,"华":{"docs":{},"df":0,"和":{"docs":{},"df":0,"哈":{"docs":{},"df":0,"工":{"docs":{},"df":0,"大":{"docs":{},"df":0,"把":{"docs":{},"df":0,"大":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"量":{"docs":{},"df":0,"化":{"docs":{},"df":0,"做":{"docs":{},"df":0,"到":{"docs":{},"df":0,"了":{"docs":{},"df":0,"1":{"docs":{"/news/jin-ri-arxivzui-re-nlpda-mo-xing-lun-wen-zuo-dao-tou-liao-qing-hua-he-ha-gong-da-ba-da-mo-xing-liang-hua-zuo-dao-liao-1bi-te/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0}},"df":4}}}}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.4142135623730951},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":10}}}}},"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":2}}}}}}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":4}},"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0}},"df":2}}}}}},"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":26}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"n":{"docs":{"/resources/社区资源/la-beginner/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":4}}}}}}},"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":7}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}}}},"y":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":6}}}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}}},"t":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0}},"df":1}}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}}}}}}},"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"2":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0}},"df":1}},"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":3}},"c":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"l":{"docs":{"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"tf":1.0}},"df":2,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{},"df":0,"学":{"docs":{},"df":0,"生":{"docs":{},"df":0,"研":{"docs":{},"df":0,"讨":{"docs":{},"df":0,"会":{"docs":{},"df":0,"！":{"docs":{},"df":0,"如":{"docs":{},"df":0,"何":{"docs":{},"df":0,"诞":{"docs":{},"df":0,"生":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"！":{"docs":{},"df":0,"如":{"docs":{},"df":0,"何":{"docs":{},"df":0,"跟":{"docs":{},"df":0,"审":{"docs":{},"df":0,"稿":{"docs":{},"df":0,"人":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"t":{"docs":{"/news/ccl2021xue-sheng-yan-tao-hui-ru-he-dan-sheng-idea-ru-he-gen-shen-gao-ren-rebuttal-ru-he-xie-zi-ji-de-di-yi-pian-ding-hui-wen-zhang-deng-qiang-xian-kan/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"t":{"docs":{},"df":0,"c":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":4}}}},"t":{"docs":{},"df":0,"g":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/news/ha-gong-da-zi-ran-yu-yan-chu-li-yan-jiu-suo-gong-kai-chatgptdiao-yan-bao-gao-nei-ce-ha-gong-da-huo-zi-dui-hua-da-mo-xing/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0}},"df":2}}}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":42,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":1}}}}}}}}}},"i":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1,"r":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":7,"i":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"e":{"docs":{},"df":0,"f":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}}},"z":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"tf":1.4142135623730951},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.4142135623730951}},"df":4}}}}}},"o":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":2,"d":{"docs":{},"df":0,"e":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":2}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0}},"df":3}}},"l":{"docs":{},"df":0,"e":{"docs":{"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"tf":1.0}},"df":4},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"4":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling2024-mian-xiang-bian-cheng-de-zi-ran-yu-yan-chu-li-zong-shu/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"tf":1.0}},"df":3}}},"4":{"docs":{"/news/sai-er-yuan-chuang-coling24-ji-cha-ji-yong-zi-dong-ti-qu-ling-yu-xiang-guan-te-zheng-ti-sheng-fan-hua-neng-li/":{"tf":1.0},"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"tf":1.0}},"df":2}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"n":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":5}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":2}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0}},"df":11}}}},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":3}}}},"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0}},"df":3}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":6}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0}},"df":2}}}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":5,"u":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":6}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"u":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.4142135623730951},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0}},"df":9}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":7}}}}}},"r":{"docs":{},"df":0,"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":1}}},"u":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":4}}}}},"s":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}},"t":{"docs":{"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0}},"df":2}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0}},"df":1}}},"f":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0}},"df":1},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":13}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"c":{"docs":{"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0}},"df":1}},"y":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"a":{"docs":{"/resources/社区资源/da/":{"tf":1.0}},"df":1,"t":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.4142135623730951},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":10,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":5}}}}}},"c":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0}},"df":1},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0}},"df":1}}}}}},"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.4142135623730951}},"df":1}}}}},"e":{"docs":{},"df":0,"p":{"docs":{"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":7,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{"/news/ha-gong-da-ju-ban-deepseekji-shu-qian-yan-yu-ying-yong-zhu-ti-jiang-zuo/":{"tf":1.0}},"df":1}}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"o":{"docs":{"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0}},"df":1,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1}}}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.4142135623730951},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0}},"df":30}}},"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":14}}}}},"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0}},"df":4}}}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.4142135623730951},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.4142135623730951},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":8,"u":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.4142135623730951},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0}},"df":12}}}}},"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0}},"df":1}}}}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0}},"df":2}}}}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0}},"df":1}}}}}},"f":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":7}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0}},"df":2}}},"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0}},"df":2}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":2}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.4142135623730951},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":13}}}},"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":4}}}},"u":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0}},"df":1}}}}}},"y":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":4}}}}},"e":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":3}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":4}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{"/resources/社区资源/elmoformanylangs/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":7}}},"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"tf":1.0},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"tf":1.0},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"tf":1.0}},"df":6}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0}},"df":2}}},"d":{"docs":{"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.4142135623730951},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.4142135623730951},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.4142135623730951},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.4142135623730951}},"df":5,"2":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2}}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":8}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":2}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":8}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{},"df":0,"i":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"a":{"docs":{"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":6}}}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":5}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0}},"df":2}}}},"x":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":2}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0}},"df":4}},"r":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":6}}},"m":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":7}}},"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":2}}}}}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":5}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":5}}}},"w":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":7,"j":{"docs":{},"df":0,"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0}},"df":2}}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"l":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":6}},"n":{"docs":{},"df":0,"d":{"docs":{"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"tf":1.0},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"tf":1.0},"/news/sai-er-yuan-chuang-findings-ji-yu-dong-tai-tu-jiao-hu-wang-luo-de-duo-yi-tu-kou-yu-yu-yan-li-jie-kuang-jia/":{"tf":1.0},"/news/sai-er-yuan-chuang-findings-zhong-wen-yu-xun-lian-yu-yan-mo-xing-hui-gu/":{"tf":1.0}},"df":4},"e":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":10}}}}}}},"e":{"docs":{},"df":0,"e":{"docs":{"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":3},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":2}}}}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":10}}}},"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1}},"l":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":2,"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":3}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":2}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0}},"df":6},"t":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":4}}}},"p":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.4142135623730951}},"df":16,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0}},"df":1}}}}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":4}}}}},"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":1}}}}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}},"c":{"docs":{"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0}},"df":1},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":3}}}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":2}}}}}}},"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}},"t":{"docs":{"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.0}},"df":11}},"o":{"docs":{},"df":0,"p":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0}},"df":3}}}},"y":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":4}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"p":{"docs":{"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"tf":1.0}},"df":1}}}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}}}},"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.4142135623730951}},"df":2}},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":13}}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0}},"df":1}}}}}}},"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0}},"df":1}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0}},"df":3}}},"u":{"docs":{},"df":0,"s":{"docs":{"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1}}},"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":1},"t":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":7}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":7}}},"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0}},"df":2}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"c":{"docs":{"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":1}}}},"r":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"u":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":2}}}},"j":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1,"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0}},"df":14,"l":{"docs":{},"df":0,"i":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":2}}}}}}},"k":{"docs":{},"df":0,"b":{"docs":{"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0}},"df":1},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0}},"df":6}}}},"y":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":2}}}}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.4142135623730951}},"df":7}}}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{"/resources/社区资源/la-beginner/":{"tf":1.0}},"df":1,"b":{"docs":{"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":15}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":38}}}}},"r":{"docs":{},"df":0,"g":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":7}},"y":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"m":{"docs":{},"df":0,"v":{"docs":{},"df":0,"2":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.4142135623730951},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.4142135623730951},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.4142135623730951},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":28}}},"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":4},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":1}}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0}},"df":3}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0}},"df":1}}},"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.4142135623730951},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0}},"df":12}}}},"k":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":1}}}}}}}},"m":{"docs":{"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":2},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"g":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"k":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}},"w":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"m":{"docs":{"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0}},"df":2}}},"t":{"docs":{},"df":0,"p":{"docs":{"/demo/演示系统/ltp/":{"tf":1.0},"/news/ha-gong-da-ltpyu-yan-ji-shu-ping-tai-zheng-shi-shang-xian-guo-jia-zhi-hui-jiao-yu-gong-gong-fu-wu-ping-tai/":{"tf":1.0},"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"tf":1.0},"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"tf":1.0},"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"tf":1.0},"/news/yu-yan-ji-shu-ping-tai-ltp-you-xin-jia-la/":{"tf":1.0},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/resources/开源项目/ltp/":{"tf":1.0}},"df":9}}},"m":{"docs":{},"df":0,"3":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":1}}}},"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":11}}}},"k":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"k":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"k":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"u":{"docs":{},"df":0,"m":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0}},"df":2}}}}}},"b":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":2}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":2}}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"：":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"元":{"docs":{},"df":0,"学":{"docs":{},"df":0,"习":{"docs":{},"df":0,"的":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"/publications/2011-a-graph-based-method-for-entity-linking/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":3}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":2,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":1}}}}}}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0}},"df":1}}},"x":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0},"/resources/开源项目/chinese-mixtral-8x7b/":{"tf":1.0}},"df":2}}}}}},"l":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0}},"df":4}},"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":32}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":2}}}}},"r":{"docs":{},"df":0,"e":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"p":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.4142135623730951},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.7320508075688772},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":17,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0}},"df":3}}}}},"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"r":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}}}},"n":{"docs":{"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":2,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0}},"df":2,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0}},"df":7}}}},"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0}},"df":9}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"tf":1.0},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":9}},"i":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0}},"df":1}}}},"w":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":3}},"l":{"docs":{"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0}},"df":1,"g":{"docs":{"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":2},"p":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"c":{"docs":{"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":1,"i":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}},"n":{"docs":{"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0}},"df":1},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":2}}}},"u":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":2}}}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0}},"df":1}}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":6,"s":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":2}}}},"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":11}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0}},"df":1,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":1}}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0}},"df":1}}}}}},"s":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"tf":1.0},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.4142135623730951},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0},"/publications/2013-chinese-parsing-exploiting-characters/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-character-level-chinese-dependency-parsing/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0}},"df":36,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.4142135623730951},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":6}}}},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"h":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0}},"df":2}}}}},"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0}},"df":1}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"tf":1.0},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":3},"t":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":2}}}}}}},"o":{"docs":{"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0}},"df":7,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"公":{"docs":{},"df":0,"开":{"docs":{},"df":0,"！":{"docs":{},"df":0,"“":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{},"df":0,"技":{"docs":{},"df":0,"术":{"docs":{},"df":0,"前":{"docs":{},"df":0,"沿":{"docs":{},"df":0,"与":{"docs":{},"df":0,"应":{"docs":{},"df":0,"用":{"docs":{},"df":0,"”":{"docs":{},"df":0,"专":{"docs":{},"df":0,"题":{"docs":{},"df":0,"讲":{"docs":{},"df":0,"座":{"docs":{},"df":0,"，":{"docs":{},"df":0,"1":{"docs":{},"df":0,"小":{"docs":{},"df":0,"时":{"docs":{},"df":0,"快":{"docs":{},"df":0,"速":{"docs":{},"df":0,"掌":{"docs":{},"df":0,"握":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{"/news/ha-gong-da-pptgong-kai-deepseekji-shu-qian-yan-yu-ying-yong-zhuan-ti-jiang-zuo-1xiao-shi-kuai-su-zhang-wo-deepseekji-ben-yuan-li/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0}},"df":10,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":3}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0}},"df":1}}}}}}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}},"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":5}}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":2}},"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":1}}}}},"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":7}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0}},"df":1}}}}}}},"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"p":{"docs":{"/resources/开源项目/pyltp/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2016-python-cheng-xu-she-ji/":{"tf":1.0}},"df":1}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"i":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2010-coherent-dialog-generation-with-query-graph/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":2}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0}},"df":1}}}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"tf":1.0},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"tf":1.0},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":12},"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.4142135623730951},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.4142135623730951}},"df":5}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"tf":1.0},"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":6}}}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"tf":1.0},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"tf":1.0}},"df":4}}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1}}}},"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":2}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"tf":1.0},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":6}},"e":{"docs":{},"df":0,"v":{"docs":{"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-reliable-dependency-arc-recognition/":{"tf":1.0},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"tf":1.0}},"df":2}}}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}}},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"tf":1.0},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"tf":1.0}},"df":11}}}}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":3}}}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-introduction-to-information-retrieval/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":5}}}}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2017-a-review-on-entity-relation-extraction/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0}},"df":2}}}}},"w":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":12}}}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":2}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"tf":1.0}},"df":1}}},"r":{"docs":{"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"tf":1.0},"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"tf":1.0},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-da-yu-yan-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"tf":1.0},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-huo-de-di-liu-jie-bai-du-jiang-xue-jin/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-yin-qing-yu-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-de-di-ba-jie-bai-du-jiang-xue-jin/":{"tf":1.0},"/news/ha-gong-da-scirbo-shi-sheng-xu-jun-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirche-mo-xiang-liu-ting-zi-ran-yu-yan-chu-li-xin-fan-shi-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"tf":1.0},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-jing-dong-wang-yu-xuan-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-shi-xiao-ming-hou-yu-tai-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirsan-wei-bo-shi-sheng-li-jia-qi-yuan-jian-hua-liu-ze-ming-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"tf":1.0},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"tf":1.0},"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"tf":1.0},"/news/zui-xin-ha-gong-da-scirzai-guo-ji-duo-yu-yan-tong-yong-yi-cun-fen-xi-ping-ce-zhong-duo-de-guan-jun/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"tf":1.0}},"df":31,"&":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"&":{"docs":{},"df":0,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}},"《":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"：":{"docs":{},"df":0,"基":{"docs":{},"df":0,"于":{"docs":{},"df":0,"预":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"的":{"docs":{},"df":0,"方":{"docs":{},"df":0,"法":{"docs":{},"df":0,"》":{"docs":{},"df":0,"一":{"docs":{},"df":0,"书":{"docs":{},"df":0,"入":{"docs":{},"df":0,"选":{"docs":{},"df":0,"中":{"docs":{},"df":0,"国":{"docs":{},"df":0,"知":{"docs":{},"df":0,"网":{"docs":{},"df":0,"“":{"docs":{},"df":0,"高":{"docs":{},"df":0,"被":{"docs":{},"df":0,"引":{"docs":{},"df":0,"图":{"docs":{},"df":0,"书":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"三":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"tf":1.0}},"df":1}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"九":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"八":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"论":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"tf":1.0}},"df":1}}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"六":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"文":{"docs":{},"df":0,"章":{"docs":{},"df":0,"被":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"tf":1.0}},"df":1}}}}}},"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"tf":1.0}},"df":1}}}}}}}},"十":{"docs":{},"df":0,"篇":{"docs":{},"df":0,"长":{"docs":{},"df":0,"文":{"docs":{},"df":0,"被":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"tf":1.0}},"df":1}}}}}}}}}},"博":{"docs":{},"df":0,"士":{"docs":{},"df":0,"生":{"docs":{},"df":0,"覃":{"docs":{},"df":0,"立":{"docs":{},"df":0,"波":{"docs":{},"df":0,"获":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-2021nian-wei-ruan-xue-zhe-cheng-hao/":{"tf":1.0}},"df":1}}}}}}}}}}},"发":{"docs":{},"df":0,"布":{"docs":{},"df":0,"首":{"docs":{},"df":0,"个":{"docs":{},"df":0,"中":{"docs":{},"df":0,"文":{"docs":{},"df":0,"扩":{"docs":{},"df":0,"词":{"docs":{},"df":0,"表":{"docs":{},"df":0,"增":{"docs":{},"df":0,"量":{"docs":{},"df":0,"预":{"docs":{},"df":0,"训":{"docs":{},"df":0,"练":{"docs":{},"df":0,"混":{"docs":{},"df":0,"合":{"docs":{},"df":0,"专":{"docs":{},"df":0,"家":{"docs":{},"df":0,"模":{"docs":{},"df":0,"型":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"取":{"docs":{},"df":0,"得":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"tf":1.0}},"df":1}}}},"国":{"docs":{},"df":0,"家":{"docs":{},"df":0,"电":{"docs":{},"df":0,"网":{"docs":{},"df":0,"调":{"docs":{},"df":0,"控":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"创":{"docs":{},"df":0,"新":{"docs":{},"df":0,"大":{"docs":{},"df":0,"赛":{"docs":{},"df":0,"赛":{"docs":{},"df":0,"道":{"docs":{},"df":0,"2":{"docs":{},"df":0,"（":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"x":{"docs":{},"df":0,"t":{"docs":{},"df":0,"2":{"docs":{},"df":0,"s":{"docs":{},"df":0,"q":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirqu-de-guo-jia-dian-wang-diao-kong-aichuang-xin-da-sai-sai-dao-2-text2sql-guan-jun/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"在":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"tf":1.0},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"tf":1.0}},"df":2}}}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"tf":1.0}},"df":1}}}}}},"多":{"docs":{},"df":0,"位":{"docs":{},"df":0,"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"受":{"docs":{},"df":0,"邀":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"第":{"docs":{},"df":0,"一":{"docs":{},"df":0,"届":{"docs":{},"df":0,"中":{"docs":{},"df":0,"国":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"学":{"docs":{},"df":0,"生":{"docs":{},"df":0,"研":{"docs":{},"df":0,"讨":{"docs":{},"df":0,"会":{"docs":{},"df":0,"（":{"docs":{},"df":0,"c":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"名":{"docs":{},"df":0,"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"j":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"tf":1.0}},"df":1}}}}}}}}},"教":{"docs":{},"df":0,"师":{"docs":{},"df":0,"受":{"docs":{},"df":0,"邀":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"师":{"docs":{},"df":0,"生":{"docs":{},"df":0,"参":{"docs":{},"df":0,"加":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"tf":1.0}},"df":1}},"l":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"tf":1.0},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"tf":1.0}},"df":2}}},"第":{"docs":{},"df":0,"八":{"docs":{},"df":0,"届":{"docs":{},"df":0,"自":{"docs":{},"df":0,"然":{"docs":{},"df":0,"语":{"docs":{},"df":0,"言":{"docs":{},"df":0,"处":{"docs":{},"df":0,"理":{"docs":{},"df":0,"和":{"docs":{},"df":0,"中":{"docs":{},"df":0,"文":{"docs":{},"df":0,"计":{"docs":{},"df":0,"算":{"docs":{},"df":0,"会":{"docs":{},"df":0,"议":{"docs":{},"df":0,"（":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"tf":1.0}},"df":1}}}}}}}}}}}}}}}}}}}}}}}}}},"车":{"docs":{},"df":0,"万":{"docs":{},"df":0,"翔":{"docs":{},"df":0,"教":{"docs":{},"df":0,"授":{"docs":{},"df":0,"入":{"docs":{},"df":0,"选":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"1":{"docs":{},"df":0,"9":{"docs":{"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-ru-xuan-2019nian-du-long-jiang-xue-zhe-qing-nian-xue-zhe/":{"tf":1.0}},"df":1}}}}}},"当":{"docs":{},"df":0,"选":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"tf":1.0}},"df":1}}}}}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"tf":1.0},"/publications/2016-hc-search-for-incremental-parsing/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":5}}}},"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"tf":1.4142135623730951},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":9}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0}},"df":1}}},"f":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.4142135623730951},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"tf":1.0}},"df":6}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"tf":1.0}},"df":27}}},"e":{"docs":{},"df":0,"v":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0}},"df":2}},"i":{"docs":{"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"tf":1.0}},"df":4}},"n":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":5,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0}},"df":7}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"tf":1.0},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"tf":1.0}},"df":3}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"tf":1.0}},"df":2}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.4142135623730951},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.4142135623730951}},"df":3}}}}},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"tf":1.0}},"df":1}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":2}},"t":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0}},"df":11,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}}}}}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"l":{"docs":{"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"tf":1.0},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"tf":1.0}},"df":3}},"u":{"docs":{},"df":0,"l":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"tf":1.0},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":9}},"u":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":3}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"tf":1.0}},"df":2}},"n":{"docs":{"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0}},"df":2}}}},"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0}},"df":2}}},"l":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":10}}}}},"q":{"docs":{},"df":0,"l":{"docs":{"/demo/演示系统/zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"tf":1.0},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":8}},"r":{"docs":{},"df":0,"l":{"docs":{"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":2}},"g":{"docs":{},"df":0,"e":{"docs":{"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2}}}}},"t":{"docs":{},"df":0,"e":{"docs":{"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0}},"df":1,"2":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"q":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"c":{"docs":{"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"t":{"docs":{"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0}},"df":2}}}}},"e":{"docs":{},"df":0,"p":{"docs":{"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0}},"df":1}},"h":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"tf":1.0},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"tf":1.0},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":5}}}}}},"u":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"tf":1.0}},"df":1}}},"y":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"tf":1.0},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":11}}}}}},"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"y":{"docs":{"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"tf":1.0},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"tf":1.0},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"tf":1.0},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":9}}}}},"w":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0}},"df":1}}}}},"y":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"tf":1.0},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0}},"df":6}},"x":{"docs":{"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"tf":1.0},"/publications/2008-introduction-to-information-retrieval-system/":{"tf":1.0},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":11}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":1}},"c":{"docs":{"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"tf":1.0}},"df":1},"g":{"docs":{"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"tf":1.0},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"tf":1.0},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"tf":1.0}},"df":9},"i":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"k":{"docs":{"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"tf":1.0},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.0},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"tf":1.0},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"tf":1.0},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"tf":1.0},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"tf":1.0},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/resources/社区资源/task-oriented-dialog-research-progress/":{"tf":1.0}},"df":18}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"/publications/2010-ltp-a-chinese-language-technology-platform/":{"tf":1.0},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"tf":1.0},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"tf":1.0}},"df":4}}}}}}},"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"tf":1.0}},"df":1}},"x":{"docs":{},"df":0,"t":{"docs":{"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"tf":1.0},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"tf":1.0},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"tf":1.0},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"tf":1.0},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"tf":1.0},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"tf":1.0}},"df":11,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"tf":1.0}},"df":1}}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.4142135623730951},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":2}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"l":{"docs":{},"df":0,"p":{"docs":{"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"d":{"docs":{"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0}},"df":2},"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":1,"k":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":2}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"tf":1.0},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"tf":1.0},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"tf":1.0},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"tf":1.0}},"df":6}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"tf":1.0},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"tf":1.0},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"tf":1.0},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0}},"df":17},"t":{"docs":{"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"tf":1.0},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0}},"df":5}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"tf":1.0}},"df":2}}}},"i":{"docs":{},"df":0,"t":{"docs":{"/publications/2015-transition-based-syntactic-linearization/":{"tf":1.0},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"tf":1.0},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"tf":1.0}},"df":6}}}}},"e":{"docs":{},"df":0,"e":{"docs":{"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"tf":1.0},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0}},"df":5,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":6}}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"tf":1.0},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"tf":1.0},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0}},"df":4}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0}},"df":1}}},"o":{"docs":{"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"tf":1.0}},"df":1}},"y":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"tf":1.0},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"tf":1.0},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"tf":1.0}},"df":4}}}},"u":{"docs":{},"df":0,"d":{"docs":{"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0}},"df":1},"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"tf":1.0},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"tf":1.0},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"tf":1.0},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"tf":1.0},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"tf":1.0},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"tf":1.0},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"tf":1.0},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"tf":1.0},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"tf":1.0},"/resources/社区资源/slu/":{"tf":1.0}},"df":21}}}}}}}},"i":{"docs":{"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":1,"f":{"docs":{},"df":0,"i":{"docs":{"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"tf":1.0}},"df":4}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"tf":1.0}},"df":1}}},"t":{"docs":{"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"tf":1.0}},"df":1},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"tf":1.0},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"tf":1.0},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0}},"df":4}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"tf":1.0},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"tf":1.0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"tf":1.0}},"df":3}}}}}}}}},"s":{"docs":{"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"tf":1.0},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"tf":1.0},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"tf":1.0},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"tf":1.0},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"tf":1.0},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"tf":1.0},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"tf":1.0},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"tf":1.0}},"df":15,"e":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"4":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"a":{"docs":{"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"tf":1.0},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"tf":1.0},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"tf":1.0},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"tf":1.0}},"df":7},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"tf":1.0},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"tf":1.0}},"df":1}}}}}}}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"/demo/演示系统/zhu-suan-vscodecha-jian/":{"tf":1.0},"/news/ha-gong-da-zhu-suan-da-mo-xing-tui-chu-vscodecha-jian/":{"tf":1.0}},"df":2}}}}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"tf":1.0},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"tf":1.0}},"df":2,"l":{"docs":{},"df":0,"i":{"docs":{"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"tf":1.0}},"df":1}}}},"b":{"docs":{"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"tf":1.0},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"tf":1.0}},"df":2},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.4142135623730951},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"tf":1.0},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"tf":1.0},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"tf":1.0},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"tf":1.0},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"tf":1.0},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"tf":1.0},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"tf":1.0},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"tf":1.0},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"tf":1.0},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"tf":1.0},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"tf":1.0},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"tf":1.0},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"tf":1.0},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"tf":1.0},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"tf":1.0},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"tf":1.0},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"tf":1.4142135623730951}},"df":19}}},"s":{"docs":{},"df":0,"d":{"docs":{"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"tf":1.4142135623730951},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"tf":1.0},"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"tf":1.0},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"tf":1.0}},"df":4}}},"y":{"docs":{},"df":0,"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"2":{"docs":{},"df":0,"0":{"docs":{},"df":0,"2":{"docs":{},"df":0,"1":{"docs":{"/news/yef2021yan-jiang-shi-lu-ha-er-bin-gong-ye-da-xue-che-mo-xiang-zi-ran-yu-yan-chu-li-xin-fan-shi/":{"tf":1.0}},"df":1}}}}}}},"z":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"tf":1.0},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"tf":1.0},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"tf":1.0},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"tf":1.0},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"tf":1.0}},"df":5}}}}}}},"documentStore":{"save":true,"docs":{"/":{"body":"","id":"/","title":""},"/about/":{"body":"语言分析（Language Analysis, LA）是将自然语言文本转换为机器内部表达的分析过程，旨在帮助计算机通语法、解语义、知语用，理解语言，构建智能。哈工大社会计算与信息检索研究中心语言分析组（SCIR-LA）长期深入研究通用和专用场景下的语言分析技术。通用语言分析从语言学角度解析语句，以词法分析（分词、词性标注、命名实体识别）、句法分析（依存句法分析）、语义分析（语义角色标注、语义依存分析）为代表。专用语言分析为各种实用场景的人机交互提供支持，涉及任务型对话系统、语义解析、代码理解与生成等领域。研究组目前承担国家自然科学基金重点项目、2030“新一代人工智能”重大项目课题等多项科研项目。组内研发的语言技术平台（LTP）已被600余家单位共享，并授权给百度、腾讯、华为等公司使用，2016年获得黑龙江省科技进步一等奖，2018、2019连续两年获CoNLL国际句法和语义分析评测第一名。\n","id":"/about/","title":"语言分析组简介"},"/demo/":{"body":"","id":"/demo/","title":"演示系统"},"/demo/演示系统/":{"body":"\n","id":"/demo/演示系统/","title":"演示系统"},"/demo/演示系统/codegen/":{"body":"\n","id":"/demo/演示系统/codegen/","title":"代码生成"},"/demo/演示系统/csc/":{"body":"\n","id":"/demo/演示系统/csc/","title":"文本校对"},"/demo/演示系统/huozi/":{"body":"\n\n","id":"/demo/演示系统/huozi/","title":"活字"},"/demo/演示系统/ltp/":{"body":"LTP 4\nLTP（Language Technology Platform） 提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。\nDemo链接\n点这里试一试\n引用\n如果您在工作中使用了 LTP，您可以引用这篇论文\n\n参考书：\n由哈工大社会计算与信息检索研究中心（HIT-SCIR）的多位学者共同编著的《自然语言处理：基于预训练模型的方法\n》（作者：车万翔、郭江、崔一鸣；主审：刘挺）一书现已正式出版，该书重点介绍了新的基于预训练模型的自然语言处理技术，包括基础知识、预训练词向量和预训练模型三大部分，可供广大LTP用户学习参考。\n更新说明\n\n4.2.0\n\n[结构性变化] 将 LTP 拆分成 2 个部分，维护和训练更方便，结构更清晰\n\n[Legacy 模型] 针对广大用户对于推理速度的需求，使用 Rust 重写了基于感知机的算法，准确率与 LTP3 版本相当，速度则是 LTP v3 的 3.55 倍，开启多线程更可获得 17.17 倍的速度提升，但目前仅支持分词、词性、命名实体三大任务\n[深度学习模型] 即基于 PyTorch 实现的深度学习模型，支持全部的6大任务（分词/词性/命名实体/语义角色/依存句法/语义依存）\n\n\n[其他改进] 改进了模型训练方法\n\n[共同] 提供了训练脚本和训练样例，使得用户能够更方便地使用私有的数据，自行训练个性化的模型\n[深度学习模型] 采用 hydra 对训练过程进行配置，方便广大用户修改模型训练参数以及对 LTP 进行扩展（比如使用其他包中的 Module）\n\n\n[其他变化] 分词、依存句法分析 (Eisner) 和 语义依存分析 (Eisner) 任务的解码算法使用 Rust 实现，速度更快\n[新特性] 模型上传至 Huggingface Hub，支持自动下载，下载速度更快，并且支持用户自行上传自己训练的模型供LTP进行推理使用\n[破坏性变更] 改用 Pipeline API 进行推理，方便后续进行更深入的性能优化（如SDP和SDPG很大一部分是重叠的，重用可以加快推理速度），使用说明参见Github快速使用部分\n\n\n4.1.0\n\n提供了自定义分词等功能\n修复了一些bug\n\n\n4.0.0\n\n基于Pytorch 开发，原生 Python 接口\n可根据需要自由选择不同速度和指标的模型\n分词、词性、命名实体、依存句法、语义角色、语义依存6大任务\n\n\n\n快速使用\nPython\n\n注： 如果遇到任何错误，请尝试使用上述命令重新安装 ltp，如果依然报错，请在 Github issues 中反馈。\n\n详细说明\nRust\n\n模型性能以及下载地址\n深度学习模型分词词性命名实体语义角色依存句法语义依存速度(句/S)\nBase98.798.595.480.689.575.239.12\nBase199.2298.7396.3979.2889.5776.57--.--\nBase299.1898.6995.9779.4990.1976.62--.--\nSmall98.498.294.378.488.374.743.13\nTiny96.897.191.670.983.870.153.22\n\n感知机算法分词词性命名实体速度(句/s)备注\nLegacy97.9398.4194.2821581.48性能详情\n\n注：感知机算法速度为开启16线程速度\n构建 Wheel 包\n\n其他语言绑定\n感知机算法\n\nRust\nC/C++\n\n深度学习算法\n\nRust\nC++\nJava\n\n作者信息\n\n冯云龙 &lt;&lt;ylfeng@ir.hit.edu.cn&gt;&gt;\n\n开源协议\n\n语言技术平台面向国内外大学、中科院各研究所以及个人研究者免费开放源代码，但如上述机构和个人将该平台用于商业目的（如企业合作项目等）则需要付费。\n除上述机构以外的企事业单位，如申请使用该平台，需付费。\n凡涉及付费问题，请发邮件到 car@ir.hit.edu.cn 洽商。\n如果您在 LTP 基础上发表论文或取得科研成果，请您在发表论文和申报成果时声明“使用了哈工大社会计算与信息检索研究中心研制的语言技术平台（LTP）”.\n同时，发信给car@ir.hit.edu.cn，说明发表论文或申报成果的题目、出处等。\n\n","id":"/demo/演示系统/ltp/","title":"LTP"},"/demo/演示系统/zhu-suan-sql/":{"body":"珠算-SQL\n相关链接\nArxiv\nGithub\nDemo链接\n点这里试一试\n演示视频\n\n\n  \n  你的浏览器不支持视频播放。\n\n","id":"/demo/演示系统/zhu-suan-sql/","title":"珠算-SQL"},"/demo/演示系统/zhu-suan-vscodecha-jian/":{"body":"珠算-VSCode\n演示视频\n\n\n  \n  你的浏览器不支持视频播放。\n\n","id":"/demo/演示系统/zhu-suan-vscodecha-jian/","title":"珠算-VSCode插件"},"/joinus/":{"body":"加入我们\n哈工大社会计算与信息检索研究中心语言分析组面向全国高校学生招收实习生、硕士生及博士生，有意者请发送邮件至车老师。\n","id":"/joinus/","title":"加入我们"},"/news/":{"body":"","id":"/news/","title":"新闻列表"},"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"body":"","id":"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/","title":"ACL 2010-2020研究趋势总结"},"/news/acl-2022-fan-xiang-yu-ce-geng-hao-ji-yu-fan-xiang-ti-shi-de-xiao-yang-ben-cao-wei-biao-zhu-fang-fa/":{"body":"","id":"/news/acl-2022-fan-xiang-yu-ce-geng-hao-ji-yu-fan-xiang-ti-shi-de-xiao-yang-ben-cao-wei-biao-zhu-fang-fa/","title":"ACL@2022 | 反向预测更好？基于反向提示的小样本槽位标注方法"},"/news/ccl2021xue-sheng-yan-tao-hui-ru-he-dan-sheng-idea-ru-he-gen-shen-gao-ren-rebuttal-ru-he-xie-zi-ji-de-di-yi-pian-ding-hui-wen-zhang-deng-qiang-xian-kan/":{"body":"","id":"/news/ccl2021xue-sheng-yan-tao-hui-ru-he-dan-sheng-idea-ru-he-gen-shen-gao-ren-rebuttal-ru-he-xie-zi-ji-de-di-yi-pian-ding-hui-wen-zhang-deng-qiang-xian-kan/","title":"CCL2021学生研讨会！如何诞生Idea！如何跟审稿人Rebuttal！如何写自己的第一篇顶会文章等抢先看！"},"/news/cong-jing-tai-dao-dong-tai-ci-biao-zheng-jin-ji-shi-nian-fa-zhan-hui-gu/":{"body":"","id":"/news/cong-jing-tai-dao-dong-tai-ci-biao-zheng-jin-ji-shi-nian-fa-zhan-hui-gu/","title":"从静态到动态，词表征近几十年发展回顾"},"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"body":"","id":"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/","title":"第二届THUNLP & HIT-SCIR学术联谊会成功举办"},"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"body":"","id":"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/","title":"第二十届中国计算语言学大会（CCL 2021） 征稿启事"},"/news/ha-gong-da-fa-bu-zhu-suan-dai-ma-da-mo-xing/":{"body":"","id":"/news/ha-gong-da-fa-bu-zhu-suan-dai-ma-da-mo-xing/","title":"哈工大发布“珠算”代码大模型"},"/news/ha-gong-da-jiu-da-aimo-xing-deng-chang-jie-suo-qian-xing-bai-ye-zhi-neng-xin-fan-shi/":{"body":"","id":"/news/ha-gong-da-jiu-da-aimo-xing-deng-chang-jie-suo-qian-xing-bai-ye-zhi-neng-xin-fan-shi/","title":"哈工大九大AI模型登场 解锁千行百业智能新范式"},"/news/ha-gong-da-ju-ban-deepseekji-shu-qian-yan-yu-ying-yong-zhu-ti-jiang-zuo/":{"body":"","id":"/news/ha-gong-da-ju-ban-deepseekji-shu-qian-yan-yu-ying-yong-zhu-ti-jiang-zuo/","title":"哈工大举办“DeepSeek技术前沿与应用”主题讲座"},"/news/ha-gong-da-kai-yuan-huo-zi-3-5-dui-hua-da-mo-xing/":{"body":"","id":"/news/ha-gong-da-kai-yuan-huo-zi-3-5-dui-hua-da-mo-xing/","title":"哈工大开源“活字3.5”对话大模型"},"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing-3-0ban-ben/":{"body":"","id":"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing-3-0ban-ben/","title":"哈工大开源“活字”对话大模型3.0版本"},"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing/":{"body":"","id":"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing/","title":"哈工大开源“活字”对话大模型"},"/news/ha-gong-da-ltpyu-yan-ji-shu-ping-tai-zheng-shi-shang-xian-guo-jia-zhi-hui-jiao-yu-gong-gong-fu-wu-ping-tai/":{"body":"","id":"/news/ha-gong-da-ltpyu-yan-ji-shu-ping-tai-zheng-shi-shang-xian-guo-jia-zhi-hui-jiao-yu-gong-gong-fu-wu-ping-tai/","title":"哈工大LTP语言技术平台正式上线国家智慧教育公共服务平台"},"/news/ha-gong-da-pptgong-kai-deepseekji-shu-qian-yan-yu-ying-yong-zhuan-ti-jiang-zuo-1xiao-shi-kuai-su-zhang-wo-deepseekji-ben-yuan-li/":{"body":"","id":"/news/ha-gong-da-pptgong-kai-deepseekji-shu-qian-yan-yu-ying-yong-zhuan-ti-jiang-zuo-1xiao-shi-kuai-su-zhang-wo-deepseekji-ben-yuan-li/","title":"哈工大｜PPT公开！“DeepSeek技术前沿与应用”专题讲座，1小时快速掌握DeepSeek基本原理"},"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"body":"","id":"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/","title":"哈工大SCIR 13篇长文被ACL 2023主会/Findings录用"},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"body":"","id":"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/","title":"哈工大SCIR 14篇长文被ACL 2021主会/Findings和IJCAI 2021录用"},"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"body":"","id":"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/","title":"哈工大SCIR 14篇长文被EMNLP 2024主会/Findings录用"},"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"body":"","id":"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/","title":"哈工大SCIR 2023届29名同学顺利通过硕士答辩"},"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"body":"","id":"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/","title":"哈工大SCIR 2025元旦晚会成功举办"},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"body":"","id":"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/","title":"哈工大SCIR 20篇长文被ACL 2024主会/Findings录用"},"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"body":"","id":"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/","title":"哈工大SCIR 22篇长文被EMNLP 2025主会/Findings录用"},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"body":"","id":"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/","title":"哈工大SCIR 29篇长文被ACL 2025主会/Findings录用"},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"body":"","id":"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/","title":"哈工大SCIR 6篇主会/2篇Findings/1篇Demo 共9篇长文被EMNLP 2021录用"},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"body":"","id":"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/","title":"哈工大SCIR 8篇长文被 COLING 2025录用"},"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"body":"","id":"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/","title":"哈工大SCIR 9篇论文被 NeurIPS 2024录用"},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"body":"","id":"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/","title":"哈工大SCIR 发布珠算-SQL"},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"body":"","id":"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/","title":"哈工大SCIR 珠算-SQL 被ACL 2025 Demo录用"},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-da-yu-yan-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"body":"","id":"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-da-yu-yan-mo-xing-de-fang-fa-yi-shu-chu-ban/","title":"哈工大SCIR《自然语言处理：基于大语言模型的方法》一书出版"},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"body":"","id":"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-chu-ban/","title":"哈工大SCIR《自然语言处理：基于预训练模型的方法》一书出版"},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"body":"","id":"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/","title":"哈工大SCIR《自然语言处理：基于预训练模型的方法》一书入选中国知网“高被引图书TOP 1%（2019-2023）”"},"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"body":"","id":"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/","title":"哈工大SCIR八篇长文被ACL 2020录用"},"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"body":"","id":"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/","title":"哈工大SCIR八篇论文被AAAI-20录用"},"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"body":"","id":"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/","title":"哈工大SCIR八篇论文被EMNLP-IJCNLP 2019录用"},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-huo-de-di-liu-jie-bai-du-jiang-xue-jin/":{"body":"","id":"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-huo-de-di-liu-jie-bai-du-jiang-xue-jin/","title":"哈工大SCIR博士生刘一佳获得第六届百度奖学金"},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-yin-qing-yu-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":"","id":"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-yin-qing-yu-shun-li-tong-guo-bo-shi-xue-wei-da-bian/","title":"哈工大SCIR博士生刘一佳、尹庆宇顺利通过博士学位答辩"},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-2021nian-wei-ruan-xue-zhe-cheng-hao/":{"body":"","id":"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-2021nian-wei-ruan-xue-zhe-cheng-hao/","title":"哈工大SCIR博士生覃立波获2021年“微软学者”称号"},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-de-di-ba-jie-bai-du-jiang-xue-jin/":{"body":"","id":"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-de-di-ba-jie-bai-du-jiang-xue-jin/","title":"哈工大SCIR博士生覃立波获得第八届百度奖学金"},"/news/ha-gong-da-scirbo-shi-sheng-xu-jun-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":"","id":"/news/ha-gong-da-scirbo-shi-sheng-xu-jun-shun-li-tong-guo-bo-shi-xue-wei-da-bian/","title":"哈工大SCIR博士生徐俊顺利通过博士学位答辩"},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"body":"","id":"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/","title":"哈工大SCIR车万翔教授当选ACL 2025程序委员会主席"},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-ru-xuan-2019nian-du-long-jiang-xue-zhe-qing-nian-xue-zhe/":{"body":"","id":"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-ru-xuan-2019nian-du-long-jiang-xue-zhe-qing-nian-xue-zhe/","title":"哈工大SCIR车万翔教授入选2019年度“龙江学者”青年学者"},"/news/ha-gong-da-scirche-mo-xiang-liu-ting-zi-ran-yu-yan-chu-li-xin-fan-shi-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"body":"","id":"/news/ha-gong-da-scirche-mo-xiang-liu-ting-zi-ran-yu-yan-chu-li-xin-fan-shi-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/","title":"哈工大SCIR车万翔、刘挺 | 自然语言处理新范式：基于预训练模型的方法"},"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"body":"","id":"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/","title":"哈工大SCIR多名教师受邀参加YSSNLP 2019"},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"body":"","id":"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/","title":"哈工大SCIR多名师生参加ACL 2018"},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"body":"","id":"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/","title":"哈工大SCIR多名师生参加COLING 2018"},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"body":"","id":"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/","title":"哈工大SCIR多名师生参加IJCAI 2018"},"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"body":"","id":"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/","title":"哈工大SCIR多位师生受邀参加第一届中国自然语言处理学生研讨会（CSSNLP 2020）"},"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"body":"","id":"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/","title":"哈工大SCIR九篇长文被EMNLP 2020及子刊录用"},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-jing-dong-wang-yu-xuan-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":"","id":"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-jing-dong-wang-yu-xuan-shun-li-tong-guo-bo-shi-xue-wei-da-bian/","title":"哈工大SCIR两位博士生景东、王宇轩顺利通过博士学位答辩"},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-shi-xiao-ming-hou-yu-tai-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":"","id":"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-shi-xiao-ming-hou-yu-tai-shun-li-tong-guo-bo-shi-xue-wei-da-bian/","title":"哈工大SCIR两位博士生施晓明、侯宇泰顺利通过博士学位答辩"},"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"body":"","id":"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/","title":"哈工大SCIR六篇长文被COLING 2018录用"},"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"body":"","id":"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/","title":"哈工大SCIR六篇文章被COLING 2020录用"},"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"body":"","id":"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/","title":"哈工大SCIR取得CCIR Cup 2022混合表格与文本数据问答赛道冠军"},"/news/ha-gong-da-scirqu-de-guo-jia-dian-wang-diao-kong-aichuang-xin-da-sai-sai-dao-2-text2sql-guan-jun/":{"body":"","id":"/news/ha-gong-da-scirqu-de-guo-jia-dian-wang-diao-kong-aichuang-xin-da-sai-sai-dao-2-text2sql-guan-jun/","title":"哈工大SCIR取得国家电网调控AI创新大赛赛道2（Text2SQL）冠军"},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"body":"","id":"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/","title":"哈工大SCIR三篇长文被AAAI 2021录用"},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"body":"","id":"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/","title":"哈工大SCIR三篇长文被IJCAI-PRICAI 2020录用"},"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"body":"","id":"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/","title":"哈工大SCIR三篇论文被ACL 2019录用"},"/news/ha-gong-da-scirsan-wei-bo-shi-sheng-li-jia-qi-yuan-jian-hua-liu-ze-ming-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":"","id":"/news/ha-gong-da-scirsan-wei-bo-shi-sheng-li-jia-qi-yuan-jian-hua-liu-ze-ming-shun-li-tong-guo-bo-shi-xue-wei-da-bian/","title":"哈工大SCIR三位博士生李家琦、袁建华、柳泽明顺利通过博士学位答辩"},"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"body":"","id":"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/","title":"哈工大SCIR十篇长文被EMNLP 2022主会及子刊录用"},"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"body":"","id":"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/","title":"哈工大SCIR师生参加CCIR 2019"},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"body":"","id":"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/","title":"哈工大SCIR师生参加CCL 2018"},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"body":"","id":"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/","title":"哈工大SCIR师生参加CCL 2019"},"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"body":"","id":"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/","title":"哈工大SCIR师生参加第八届自然语言处理和中文计算会议（NLPCC 2019）"},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"body":"","id":"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/","title":"哈工大SCIR实验室师生参加 ACL 2025 学术会议"},"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"body":"","id":"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/","title":"哈工大SCIR在CoNLL-2017多语言通用依存句法分析评测中取得佳绩"},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"body":"","id":"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/","title":"哈工大SCIR在CoNLL-2019国际跨框架语义分析评测中取得第一名"},"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"body":"","id":"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/","title":"哈工大SCIR在MMNLU-22多语言任务型对话自然语言理解评测取得Full Dataset赛道第一名"},"/news/ha-gong-da-xun-fei-lian-he-shi-yan-shi-yan-zhi-de-wen-ben-shun-hua-ji-shu-cheng-gong-jie-ru-xun-fei-ting-jian-xi-tong/":{"body":"","id":"/news/ha-gong-da-xun-fei-lian-he-shi-yan-shi-yan-zhi-de-wen-ben-shun-hua-ji-shu-cheng-gong-jie-ru-xun-fei-ting-jian-xi-tong/","title":"哈工大讯飞联合实验室研制的文本顺滑技术成功接入“讯飞听见”系统"},"/news/ha-gong-da-xun-fei-rong-huo-2024nian-du-wu-wen-jun-ren-gong-zhi-neng-ke-xue-ji-shu-jiang-ke-ji-jin-bu-jiang-yi-deng-jiang/":{"body":"","id":"/news/ha-gong-da-xun-fei-rong-huo-2024nian-du-wu-wen-jun-ren-gong-zhi-neng-ke-xue-ji-shu-jiang-ke-ji-jin-bu-jiang-yi-deng-jiang/","title":"哈工大、讯飞荣获2024年度吴文俊人工智能科学技术奖 - 科技进步奖一等奖"},"/news/ha-gong-da-zhu-suan-da-mo-xing-tui-chu-vscodecha-jian/":{"body":"","id":"/news/ha-gong-da-zhu-suan-da-mo-xing-tui-chu-vscodecha-jian/","title":"哈工大珠算大模型推出VSCode插件"},"/news/ha-gong-da-zi-ran-yu-yan-chu-li-yan-jiu-suo-gong-kai-chatgptdiao-yan-bao-gao-nei-ce-ha-gong-da-huo-zi-dui-hua-da-mo-xing/":{"body":"","id":"/news/ha-gong-da-zi-ran-yu-yan-chu-li-yan-jiu-suo-gong-kai-chatgptdiao-yan-bao-gao-nei-ce-ha-gong-da-huo-zi-dui-hua-da-mo-xing/","title":"哈工大自然语言处理研究所公开《ChatGPT调研报告》，内测哈工大“活字”对话大模型"},"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"body":"","id":"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/","title":"HIT-SCIR发布首个中文扩词表增量预训练混合专家模型Chinese-Mixtral-8x7B"},"/news/jin-ri-arxivzui-re-nlpda-mo-xing-lun-wen-zuo-dao-tou-liao-qing-hua-he-ha-gong-da-ba-da-mo-xing-liang-hua-zuo-dao-liao-1bi-te/":{"body":"","id":"/news/jin-ri-arxivzui-re-nlpda-mo-xing-lun-wen-zuo-dao-tou-liao-qing-hua-he-ha-gong-da-ba-da-mo-xing-liang-hua-zuo-dao-liao-1bi-te/","title":"今日arXiv最热NLP大模型论文：做到头了！清华和哈工大把大模型量化做到了1比特"},"/news/jin-tian-ha-gong-da-zai-huo-guo-jia-biao-zhang/":{"body":"","id":"/news/jin-tian-ha-gong-da-zai-huo-guo-jia-biao-zhang/","title":"今天，哈工大再获国家表彰！"},"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"body":"","id":"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/","title":"LTP 4.0！单模型完成6项自然语言处理任务"},"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"body":"","id":"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/","title":"迈向推理时代：900+篇参考文献揭示长链思维的前世今生，哈工大 SCIR 推出全面综述"},"/news/qing-chun-de-xuan-ze-gun-ha-gong-da-zhe-ge-tuan-dui-li-yu-sheng-cheng-shi-aichao-tou/":{"body":"","id":"/news/qing-chun-de-xuan-ze-gun-ha-gong-da-zhe-ge-tuan-dui-li-yu-sheng-cheng-shi-aichao-tou/","title":"青春的选择丨哈工大这个团队立于生成式AI潮头！"},"/news/sai-er-bi-ji-da-mo-xing-shang-xia-wen-chang-du-kuo-zhan-zhong-de-jian-suo-zeng-qiang-ji-shu-jian-shu/":{"body":"","id":"/news/sai-er-bi-ji-da-mo-xing-shang-xia-wen-chang-du-kuo-zhan-zhong-de-jian-suo-zeng-qiang-ji-shu-jian-shu/","title":"赛尔笔记 | 大模型上下文长度扩展中的检索增强技术简述"},"/news/sai-er-bi-ji-mian-xiang-biao-ge-shu-ju-de-da-mo-xing-tui-li-zong-shu/":{"body":"","id":"/news/sai-er-bi-ji-mian-xiang-biao-ge-shu-ju-de-da-mo-xing-tui-li-zong-shu/","title":"赛尔笔记 | 面向表格数据的大模型推理综述"},"/news/sai-er-bi-ji-tou-ji-jie-ma-jie-suo-zi-hui-gui-tui-li-su-du-shang-xian/":{"body":"","id":"/news/sai-er-bi-ji-tou-ji-jie-ma-jie-suo-zi-hui-gui-tui-li-su-du-shang-xian/","title":"赛尔笔记｜投机解码：解锁自回归推理速度上限"},"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"body":"","id":"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/","title":"赛尔笔记 | 新分类！全总结！最新Awesome-SLU-Survey资源库开源！"},"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/","title":"赛尔原创@AAAI 2021 | 纠结于联合学习中的建模方法？快来看看图网络显式建模!"},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/","title":"赛尔原创@AAAI 2021 | 数据增强没效果？试试用Cluster-to-Cluster生成更多样化的新数据吧"},"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/","title":"赛尔原创@AAAI 2022|基于Profile信息的口语语言理解基准"},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/","title":"赛尔原创@AAAI 2023 | BridgeTower- 在视觉语言表示学习中建立编码器间的桥梁"},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/","title":"赛尔原创@AAAI 2024 |语义引导的生成式图像增广方法"},"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/","title":"赛尔原创 | AAAI20 基于Goal(话题)的开放域多轮对话规划"},"/news/sai-er-yuan-chuang-aaai20-yong-yu-lian-he-jian-mo-dui-hua-xing-wei-shi-bie-he-qing-gan-fen-lei-de-shen-du-jiao-hu-guan-xi-wang-luo/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai20-yong-yu-lian-he-jian-mo-dui-hua-xing-wei-shi-bie-he-qing-gan-fen-lei-de-shen-du-jiao-hu-guan-xi-wang-luo/","title":"赛尔原创 | AAAI20 用于联合建模对话行为识别和情感分类的深度交互关系网络"},"/news/sai-er-yuan-chuang-aaai2021-xiao-yang-ben-xue-xi-xia-de-duo-biao-qian-fen-lei-wen-ti-chu-tan/":{"body":"","id":"/news/sai-er-yuan-chuang-aaai2021-xiao-yang-ben-xue-xi-xia-de-duo-biao-qian-fen-lei-wen-ti-chu-tan/","title":"赛尔原创@AAAI2021 | 小样本学习下的多标签分类问题初探"},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/","title":"赛尔原创@ACL 2021 | 基于一致性正则的跨语言微调方法"},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"body":"","id":"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/","title":"赛尔原创@ACL 2021 | 开放域对话结构发现"},"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"body":"","id":"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/","title":"赛尔原创@ACL Findings | 基于高质量对抗样本的依存分析器鲁棒性探究"},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"body":"","id":"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/","title":"赛尔原创@ACL Findings | 任务共舞，小样本场景下的多任务联合学习方法初探"},"/news/sai-er-yuan-chuang-acl20-ji-yu-dui-hua-tu-pu-de-kai-fang-yu-duo-lun-dui-hua-ce-lue-xue-xi/":{"body":"","id":"/news/sai-er-yuan-chuang-acl20-ji-yu-dui-hua-tu-pu-de-kai-fang-yu-duo-lun-dui-hua-ce-lue-xue-xi/","title":"赛尔原创 | ACL20 基于对话图谱的开放域多轮对话策略学习"},"/news/sai-er-yuan-chuang-acl20-ji-yu-tu-zhu-yi-li-wang-luo-de-duo-li-du-ji-qi-yue-du-li-jie-wen-dang-jian-mo/":{"body":"","id":"/news/sai-er-yuan-chuang-acl20-ji-yu-tu-zhu-yi-li-wang-luo-de-duo-li-du-ji-qi-yue-du-li-jie-wen-dang-jian-mo/","title":"赛尔原创 | ACL20 基于图注意力网络的多粒度机器阅读理解文档建模"},"/news/sai-er-yuan-chuang-acl20-rang-mo-xing-shi-ban-gong-bei-tan-jiu-shao-yang-ben-xu-lie-biao-zhu-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-acl20-rang-mo-xing-shi-ban-gong-bei-tan-jiu-shao-yang-ben-xu-lie-biao-zhu-fang-fa/","title":"赛尔原创 | ACL20 让模型“事半功倍”，探究少样本序列标注方法"},"/news/sai-er-yuan-chuang-acl20-yong-yu-duo-ling-yu-duan-dao-duan-ren-wu-xing-dui-hua-xi-tong-de-dong-tai-rong-he-wang-luo/":{"body":"","id":"/news/sai-er-yuan-chuang-acl20-yong-yu-duo-ling-yu-duan-dao-duan-ren-wu-xing-dui-hua-xi-tong-de-dong-tai-rong-he-wang-luo/","title":"赛尔原创 | ACL20 用于多领域端到端任务型对话系统的动态融合网络"},"/news/sai-er-yuan-chuang-che-mo-xiang-jiao-shou-tuan-dui-zui-xin-zong-shu-dui-hua-xi-tong-zhong-kou-yu-yu-yan-li-jie-yan-jiu-de-xin-jin-zhan-yu-xin-ling-yu/":{"body":"","id":"/news/sai-er-yuan-chuang-che-mo-xiang-jiao-shou-tuan-dui-zui-xin-zong-shu-dui-hua-xi-tong-zhong-kou-yu-yu-yan-li-jie-yan-jiu-de-xin-jin-zhan-yu-xin-ling-yu/","title":"赛尔原创 | 车万翔教授团队最新综述：对话系统中口语语言理解研究的新进展与新领域"},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"body":"","id":"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/","title":"赛尔原创@COLING 2022 | CCTC：面向中文母语使用者的跨句子文本纠错数据集"},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/","title":"赛尔原创@COLING 2022 | MetaPrompting：基于元学习的soft prompt初始化方法"},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/","title":"赛尔原创@COLING 2022 | 融合自适应机制与自训练框架的无监督文本顺滑方法"},"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"body":"","id":"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/","title":"赛尔原创@COLING2024 | LM-Combiner：通过模型改写实现更精准的语法纠错"},"/news/sai-er-yuan-chuang-coling2024-mian-xiang-bian-cheng-de-zi-ran-yu-yan-chu-li-zong-shu/":{"body":"","id":"/news/sai-er-yuan-chuang-coling2024-mian-xiang-bian-cheng-de-zi-ran-yu-yan-chu-li-zong-shu/","title":"赛尔原创@COLING2024 | 面向编程的自然语言处理综述"},"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"body":"","id":"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/","title":"赛尔原创@COLING2024 | 人工智能助手API调用能力的动态评估方法"},"/news/sai-er-yuan-chuang-coling24-ji-cha-ji-yong-zi-dong-ti-qu-ling-yu-xiang-guan-te-zheng-ti-sheng-fan-hua-neng-li/":{"body":"","id":"/news/sai-er-yuan-chuang-coling24-ji-cha-ji-yong-zi-dong-ti-qu-ling-yu-xiang-guan-te-zheng-ti-sheng-fan-hua-neng-li/","title":"赛尔原创@COLING24 ｜即插即用！自动提取领域相关特征提升泛化能力"},"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"body":"","id":"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/","title":"赛尔原创@COLING24 ｜无需标注即可增强模型 COT 能力"},"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"body":"","id":"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/","title":"赛尔原创@EMNLP 2020 | 且回忆且学习：在更少的遗忘下精调深层预训练语言模型"},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"body":"","id":"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/","title":"赛尔原创｜EMNLP 2020 融合自训练和自监督方法的无监督文本顺滑研究"},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"body":"","id":"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/","title":"赛尔原创@EMNLP 2021 | 多语言和跨语言对话推荐"},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"body":"","id":"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/","title":"赛尔原创@EMNLP 2021 | 预训练跨语言模型中的大词表构建及使用"},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"body":"","id":"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/","title":"赛尔原创@EMNLP 2023 | 通过跨语言提示改进零样本 CoT 推理能力"},"/news/sai-er-yuan-chuang-findings-ji-yu-dong-tai-tu-jiao-hu-wang-luo-de-duo-yi-tu-kou-yu-yu-yan-li-jie-kuang-jia/":{"body":"","id":"/news/sai-er-yuan-chuang-findings-ji-yu-dong-tai-tu-jiao-hu-wang-luo-de-duo-yi-tu-kou-yu-yu-yan-li-jie-kuang-jia/","title":"赛尔原创@Findings | 基于动态图交互网络的多意图口语语言理解框架"},"/news/sai-er-yuan-chuang-findings-zhong-wen-yu-xun-lian-yu-yan-mo-xing-hui-gu/":{"body":"","id":"/news/sai-er-yuan-chuang-findings-zhong-wen-yu-xun-lian-yu-yan-mo-xing-hui-gu/","title":"赛尔原创@Findings | 中文预训练语言模型回顾"},"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"body":"","id":"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/","title":"赛尔原创@ICASSP 2021 | 首次探索中文词信息增强中文口语语言理解!"},"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"body":"","id":"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/","title":"赛尔原创 | N-LTP：基于预训练模型的中文自然语言处理平台"},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"body":"","id":"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/","title":"赛尔原创 | 首个任务型对话系统中生成模块资源库Awesome-TOD-NLG-Survey开源！"},"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"body":"","id":"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/","title":"SemEval-2016 Task 9中文语义依存图数据对外发布"},"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"body":"","id":"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/","title":"我中心3篇长文被AAAI 2018录用"},"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"body":"","id":"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/","title":"我中心3篇长文被ACL 2018录用"},"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"body":"","id":"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/","title":"我中心6篇长文被IJCAI-ECAI 2018录用"},"/news/wo-zhong-xin-bo-shi-sheng-guo-jiang-shun-li-tong-guo-bo-shi-da-bian/":{"body":"","id":"/news/wo-zhong-xin-bo-shi-sheng-guo-jiang-shun-li-tong-guo-bo-shi-da-bian/","title":"我中心博士生郭江顺利通过博士答辩"},"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"body":"","id":"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/","title":"我中心车万翔教授受邀参加第二届腾讯AI Lab学术论坛"},"/news/wo-zhong-xin-fan-yi-de-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li-yi-shu-zheng-shi-chu-ban/":{"body":"","id":"/news/wo-zhong-xin-fan-yi-de-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li-yi-shu-zheng-shi-chu-ban/","title":"我中心翻译的《基于深度学习的自然语言处理》一书正式出版"},"/news/wo-zhong-xin-qing-nian-jiao-shi-shou-yao-can-jia-di-shi-si-jie-zhong-guo-zi-ran-yu-yan-chu-li-qing-nian-xue-zhe-yan-tao-hui/":{"body":"","id":"/news/wo-zhong-xin-qing-nian-jiao-shi-shou-yao-can-jia-di-shi-si-jie-zhong-guo-zi-ran-yu-yan-chu-li-qing-nian-xue-zhe-yan-tao-hui/","title":"我中心青年教师受邀参加第十四届中国自然语言处理青年学者研讨会"},"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"body":"","id":"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/","title":"我中心师生参加第六届自然语言处理和中文计算会议（NLPCC 2017）"},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"body":"","id":"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/","title":"我中心师生参加第十六届全国计算语言学会议（CCL 2017）"},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"body":"","id":"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/","title":"我中心师生参加EMNLP 2017"},"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"body":"","id":"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/","title":"新闻 | 第三届HIT-SCIR&THUNLP&FudanNLP学术联谊会成功举办"},"/news/yef2021yan-jiang-shi-lu-ha-er-bin-gong-ye-da-xue-che-mo-xiang-zi-ran-yu-yan-chu-li-xin-fan-shi/":{"body":"","id":"/news/yef2021yan-jiang-shi-lu-ha-er-bin-gong-ye-da-xue-che-mo-xiang-zi-ran-yu-yan-chu-li-xin-fan-shi/","title":"YEF2021演讲实录｜哈尔滨工业大学车万翔：自然语言处理新范式"},"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"body":"","id":"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/","title":"语言技术平台（LTP）推出 v4.2 版本！"},"/news/yu-yan-ji-shu-ping-tai-ltp-you-xin-jia-la/":{"body":"","id":"/news/yu-yan-ji-shu-ping-tai-ltp-you-xin-jia-la/","title":"语言技术平台（LTP）有新家啦！"},"/news/zhu-he-wo-zhong-xin-che-mo-xiang-lao-shi-jin-sheng-jiao-shou/":{"body":"","id":"/news/zhu-he-wo-zhong-xin-che-mo-xiang-lao-shi-jin-sheng-jiao-shou/","title":"祝贺我中心车万翔老师晋升教授"},"/news/zui-xin-ha-gong-da-scirzai-guo-ji-duo-yu-yan-tong-yong-yi-cun-fen-xi-ping-ce-zhong-duo-de-guan-jun/":{"body":"","id":"/news/zui-xin-ha-gong-da-scirzai-guo-ji-duo-yu-yan-tong-yong-yi-cun-fen-xi-ping-ce-zhong-duo-de-guan-jun/","title":"最新！哈工大SCIR在国际多语言通用依存分析评测中夺得冠军"},"/people/":{"body":"","id":"/people/","title":"成员列表"},"/people/在读博士生/":{"body":"","id":"/people/在读博士生/","title":"在读博士生"},"/people/在读博士生/chen-qi-guang/":{"body":"","id":"/people/在读博士生/chen-qi-guang/","title":"陈麒光"},"/people/在读博士生/feng-yun-long/":{"body":"","id":"/people/在读博士生/feng-yun-long/","title":"冯云龙"},"/people/在读博士生/guan-jian-nan/":{"body":"","id":"/people/在读博士生/guan-jian-nan/","title":"管健男"},"/people/在读博士生/he-ye/":{"body":"","id":"/people/在读博士生/he-ye/","title":"何烨"},"/people/在读博士生/li-bo-han/":{"body":"","id":"/people/在读博士生/li-bo-han/","title":"李博涵"},"/people/在读博士生/li-shang-zhan/":{"body":"","id":"/people/在读博士生/li-shang-zhan/","title":"李尚展"},"/people/在读博士生/liu-yi-jun/":{"body":"","id":"/people/在读博士生/liu-yi-jun/","title":"刘议骏"},"/people/在读博士生/luo-xian-zhen/":{"body":"","id":"/people/在读博士生/luo-xian-zhen/","title":"罗先镇"},"/people/在读博士生/mou-hong-lin/":{"body":"","id":"/people/在读博士生/mou-hong-lin/","title":"牟虹霖"},"/people/在读博士生/teng-de-chuan/":{"body":"","id":"/people/在读博士生/teng-de-chuan/","title":"滕德川"},"/people/在读博士生/wang-ding-zi-rui/":{"body":"","id":"/people/在读博士生/wang-ding-zi-rui/","title":"王丁子睿"},"/people/在读博士生/wang-yi-xuan/":{"body":"","id":"/people/在读博士生/wang-yi-xuan/","title":"王一轩"},"/people/在读博士生/xu-yang/":{"body":"","id":"/people/在读博士生/xu-yang/","title":"徐阳"},"/people/在读博士生/xu-yu-zhuang/":{"body":"","id":"/people/在读博士生/xu-yu-zhuang/","title":"徐玉庄"},"/people/在读博士生/zhang-wen-bin/":{"body":"","id":"/people/在读博士生/zhang-wen-bin/","title":"张文斌"},"/people/在读本科生/":{"body":"","id":"/people/在读本科生/","title":"在读本科生"},"/people/在读硕士生/":{"body":"","id":"/people/在读硕士生/","title":"在读硕士生"},"/people/在读硕士生/guo-chuan-zhe/":{"body":"","id":"/people/在读硕士生/guo-chuan-zhe/","title":"郭传哲"},"/people/在读硕士生/han-zi-yu/":{"body":"","id":"/people/在读硕士生/han-zi-yu/","title":"韩子玙"},"/people/在读硕士生/ji-shi-yu/":{"body":"","id":"/people/在读硕士生/ji-shi-yu/","title":"季世宇"},"/people/在读硕士生/niu-tian-hao/":{"body":"","id":"/people/在读硕士生/niu-tian-hao/","title":"牛天昊"},"/people/在读硕士生/peng-deng-yun/":{"body":"","id":"/people/在读硕士生/peng-deng-yun/","title":"彭登云"},"/people/在读硕士生/xu-ke-yan/":{"body":"","id":"/people/在读硕士生/xu-ke-yan/","title":"徐柯炎"},"/people/在读硕士生/yan-zheng/":{"body":"","id":"/people/在读硕士生/yan-zheng/","title":"严铮"},"/people/在读硕士生/zhang-xuan-jing/":{"body":"","id":"/people/在读硕士生/zhang-xuan-jing/","title":"张玄靓"},"/people/在读硕士生/zhang-zhi-ming/":{"body":"","id":"/people/在读硕士生/zhang-zhi-ming/","title":"张致铭"},"/people/在读硕士生/zhou-shi-qi/":{"body":"","id":"/people/在读硕士生/zhou-shi-qi/","title":"周士祺"},"/people/教师/":{"body":"","id":"/people/教师/","title":"教师"},"/people/教师/che-mo-xiang/":{"body":"","id":"/people/教师/che-mo-xiang/","title":"车万翔"},"/people/教师/shi-qi/":{"body":"","id":"/people/教师/shi-qi/","title":"施琦"},"/people/教师/zhu-qing-fu/":{"body":"","id":"/people/教师/zhu-qing-fu/","title":"朱庆福"},"/people/毕业博士/":{"body":"","id":"/people/毕业博士/","title":"毕业博士"},"/people/毕业博士/chen-san-yuan/":{"body":"","id":"/people/毕业博士/chen-san-yuan/","title":"陈三元"},"/people/毕业博士/dou-long-xu/":{"body":"","id":"/people/毕业博士/dou-long-xu/","title":"窦隆绪"},"/people/毕业博士/guo-jiang/":{"body":"","id":"/people/毕业博士/guo-jiang/","title":"郭江"},"/people/毕业博士/hou-yu-tai/":{"body":"","id":"/people/毕业博士/hou-yu-tai/","title":"侯宇泰"},"/people/毕业博士/li-zheng-hua/":{"body":"","id":"/people/毕业博士/li-zheng-hua/","title":"李正华"},"/people/毕业博士/liu-yi-jia/":{"body":"","id":"/people/毕业博士/liu-yi-jia/","title":"刘一佳"},"/people/毕业博士/liu-ze-ming/":{"body":"","id":"/people/毕业博士/liu-ze-ming/","title":"柳泽明"},"/people/毕业博士/shi-xiao-ming/":{"body":"","id":"/people/毕业博士/shi-xiao-ming/","title":"施晓明"},"/people/毕业博士/tan-li-bo/":{"body":"","id":"/people/毕业博士/tan-li-bo/","title":"覃立波"},"/people/毕业博士/wang-bao-xin/":{"body":"","id":"/people/毕业博士/wang-bao-xin/","title":"王宝鑫"},"/people/毕业博士/wang-shao-lei/":{"body":"","id":"/people/毕业博士/wang-shao-lei/","title":"王少磊"},"/people/毕业博士/wang-yu-xuan/":{"body":"","id":"/people/毕业博士/wang-yu-xuan/","title":"王宇轩"},"/people/毕业博士/xu-jun/":{"body":"","id":"/people/毕业博士/xu-jun/","title":"徐俊"},"/people/毕业博士/xu-xiao/":{"body":"","id":"/people/毕业博士/xu-xiao/","title":"徐啸"},"/people/毕业博士/zhang-mei-shan/":{"body":"","id":"/people/毕业博士/zhang-mei-shan/","title":"张梅山"},"/people/毕业博士/zheng-bo/":{"body":"","id":"/people/毕业博士/zheng-bo/","title":"郑博"},"/people/毕业学士/":{"body":"","id":"/people/毕业学士/","title":"毕业学士"},"/people/毕业学士/niu-guo-cheng/":{"body":"","id":"/people/毕业学士/niu-guo-cheng/","title":"牛国成"},"/people/毕业学士/wang-zhe/":{"body":"","id":"/people/毕业学士/wang-zhe/","title":"王哲"},"/people/毕业学士/wen-hao-yang/":{"body":"","id":"/people/毕业学士/wen-hao-yang/","title":"文灏洋"},"/people/毕业学士/xie-tian-bao/":{"body":"","id":"/people/毕业学士/xie-tian-bao/","title":"谢天宝"},"/people/毕业学士/xu-yi-heng/":{"body":"","id":"/people/毕业学士/xu-yi-heng/","title":"徐毅恒"},"/people/毕业硕士/":{"body":"","id":"/people/毕业硕士/","title":"毕业硕士"},"/people/毕业硕士/chen-cheng/":{"body":"","id":"/people/毕业硕士/chen-cheng/","title":"陈成"},"/people/毕业硕士/chen-xin/":{"body":"","id":"/people/毕业硕士/chen-xin/","title":"陈鑫"},"/people/毕业硕士/deng-wen-chao/":{"body":"","id":"/people/毕业硕士/deng-wen-chao/","title":"邓文超"},"/people/毕业硕士/dong-hong-yuan/":{"body":"","id":"/people/毕业硕士/dong-hong-yuan/","title":"董泓源"},"/people/毕业硕士/han-bing/":{"body":"","id":"/people/毕业硕士/han-bing/","title":"韩冰"},"/people/毕业硕士/han-yu/":{"body":"","id":"/people/毕业硕士/han-yu/","title":"韩宇"},"/people/毕业硕士/han-zhong-hua/":{"body":"","id":"/people/毕业硕士/han-zhong-hua/","title":"韩中华"},"/people/毕业硕士/hu-xiao/":{"body":"","id":"/people/毕业硕士/hu-xiao/","title":"胡啸"},"/people/毕业硕士/ji-yu-qiu/":{"body":"","id":"/people/毕业硕士/ji-yu-qiu/","title":"季雨秋"},"/people/毕业硕士/lai-yong-kui/":{"body":"","id":"/people/毕业硕士/lai-yong-kui/","title":"赖勇魁"},"/people/毕业硕士/lei-zhi-lin/":{"body":"","id":"/people/毕业硕士/lei-zhi-lin/","title":"雷志林"},"/people/毕业硕士/li-qi-xin/":{"body":"","id":"/people/毕业硕士/li-qi-xin/","title":"李祺欣"},"/people/毕业硕士/li-yong-qiang/":{"body":"","id":"/people/毕业硕士/li-yong-qiang/","title":"李永强"},"/people/毕业硕士/li-zhou-yang/":{"body":"","id":"/people/毕业硕士/li-zhou-yang/","title":"黎州扬"},"/people/毕业硕士/liu-yang/":{"body":"","id":"/people/毕业硕士/liu-yang/","title":"刘洋"},"/people/毕业硕士/mao-jia-feng/":{"body":"","id":"/people/毕业硕士/mao-jia-feng/","title":"茅佳峰"},"/people/毕业硕士/pan-ming-yang/":{"body":"","id":"/people/毕业硕士/pan-ming-yang/","title":"潘名扬"},"/people/毕业硕士/qiao-zhen-hao/":{"body":"","id":"/people/毕业硕士/qiao-zhen-hao/","title":"乔振浩"},"/people/毕业硕士/ren-bin/":{"body":"","id":"/people/毕业硕士/ren-bin/","title":"任彬"},"/people/毕业硕士/sun-bo/":{"body":"","id":"/people/毕业硕士/sun-bo/","title":"孙博"},"/people/毕业硕士/tang-guo-hua/":{"body":"","id":"/people/毕业硕士/tang-guo-hua/","title":"唐国华"},"/people/毕业硕士/wang-li-jie/":{"body":"","id":"/people/毕业硕士/wang-li-jie/","title":"王丽杰"},"/people/毕业硕士/wang-xing-hao/":{"body":"","id":"/people/毕业硕士/wang-xing-hao/","title":"王兴昊"},"/people/毕业硕士/wang-zhong-yuan/":{"body":"","id":"/people/毕业硕士/wang-zhong-yuan/","title":"王重元"},"/people/毕业硕士/wei-fu-xuan/":{"body":"","id":"/people/毕业硕士/wei-fu-xuan/","title":"魏福煊"},"/people/毕业硕士/xia-wen-tian/":{"body":"","id":"/people/毕业硕士/xia-wen-tian/","title":"夏闻添"},"/people/毕业硕士/xu-wei/":{"body":"","id":"/people/毕业硕士/xu-wei/","title":"徐伟"},"/people/毕业硕士/xu-zi-xiang/":{"body":"","id":"/people/毕业硕士/xu-zi-xiang/","title":"徐梓翔"},"/people/毕业硕士/zhang-yi/":{"body":"","id":"/people/毕业硕士/zhang-yi/","title":"张毅"},"/people/毕业硕士/zhao-huai-peng/":{"body":"","id":"/people/毕业硕士/zhao-huai-peng/","title":"赵怀鹏"},"/people/毕业硕士/zhao-jing/":{"body":"","id":"/people/毕业硕士/zhao-jing/","title":"赵静"},"/people/毕业硕士/zhu-jia-qi/":{"body":"","id":"/people/毕业硕士/zhu-jia-qi/","title":"朱嘉琪"},"/projects/":{"body":"","id":"/projects/","title":"项目列表"},"/projects/da-mo-xing-zeng-liang-xue-xi-ji-shu-yan-jiu/":{"body":"","id":"/projects/da-mo-xing-zeng-liang-xue-xi-ji-shu-yan-jiu/","title":"大模型增量学习技术研究"},"/projects/jian-suo-zeng-qiang-kai-fang-yu-dui-hua-sheng-cheng/":{"body":"","id":"/projects/jian-suo-zeng-qiang-kai-fang-yu-dui-hua-sheng-cheng/","title":"检索增强开放域对话生成"},"/projects/kai-fang-yu-duo-yuan-zhi-shi-huo-qu-yu-shen-ceng-jie-gou-hua-yu-yi-fen-xi/":{"body":"","id":"/projects/kai-fang-yu-duo-yuan-zhi-shi-huo-qu-yu-shen-ceng-jie-gou-hua-yu-yi-fen-xi/","title":"开放域多元知识获取与深层结构化语义分析"},"/projects/kua-yu-yan-yu-yi-yi-cun-fen-xi-yan-jiu/":{"body":"","id":"/projects/kua-yu-yan-yu-yi-yi-cun-fen-xi-yan-jiu/","title":"跨语言语义依存分析研究"},"/projects/mian-xiang-san-yuan-kong-jian-de-hu-lian-wang-zhong-wen-xin-xi-chu-li-li-lun-yu-fang-fa/":{"body":"","id":"/projects/mian-xiang-san-yuan-kong-jian-de-hu-lian-wang-zhong-wen-xin-xi-chu-li-li-lun-yu-fang-fa/","title":"面向三元空间的互联网中文信息处理理论与方法"},"/projects/shao-biao-zhu-zi-ran-yu-yan-chu-li-li-lun-yu-fang-fa/":{"body":"","id":"/projects/shao-biao-zhu-zi-ran-yu-yan-chu-li-li-lun-yu-fang-fa/","title":"少标注自然语言处理理论与方法"},"/projects/sheng-cheng-shi-da-mo-xing-zhong-de-si-wei-lian-ji-shu-ji-li-ji-ying-yong-yan-jiu/":{"body":"","id":"/projects/sheng-cheng-shi-da-mo-xing-zhong-de-si-wei-lian-ji-shu-ji-li-ji-ying-yong-yan-jiu/","title":"生成式大模型中的思维链技术机理及应用研究"},"/projects/yi-cun-ju-fa-fen-xi-zi-jie-gou-ke-xin-du-ji-suan-yan-jiu/":{"body":"","id":"/projects/yi-cun-ju-fa-fen-xi-zi-jie-gou-ke-xin-du-ji-suan-yan-jiu/","title":"依存句法分析子结构可信度计算研究"},"/projects/yi-yu-yi-cun-ju-fa-fen-xi-ruo-gan-guan-jian-ji-shu-yan-jiu/":{"body":"","id":"/projects/yi-yu-yi-cun-ju-fa-fen-xi-ruo-gan-guan-jian-ji-shu-yan-jiu/","title":"汉语依存句法分析若干关键技术研究"},"/projects/yu-yan-ji-shu-ping-tai/":{"body":"LTP 4\nLTP（Language Technology Platform） 提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。\n引用\n如果您在工作中使用了 LTP，您可以引用这篇论文\n\n参考书：\n由哈工大社会计算与信息检索研究中心（HIT-SCIR）的多位学者共同编著的《自然语言处理：基于预训练模型的方法\n》（作者：车万翔、郭江、崔一鸣；主审：刘挺）一书现已正式出版，该书重点介绍了新的基于预训练模型的自然语言处理技术，包括基础知识、预训练词向量和预训练模型三大部分，可供广大LTP用户学习参考。\n更新说明\n\n4.2.0\n\n[结构性变化] 将 LTP 拆分成 2 个部分，维护和训练更方便，结构更清晰\n\n[Legacy 模型] 针对广大用户对于推理速度的需求，使用 Rust 重写了基于感知机的算法，准确率与 LTP3 版本相当，速度则是 LTP v3 的 3.55 倍，开启多线程更可获得 17.17 倍的速度提升，但目前仅支持分词、词性、命名实体三大任务\n[深度学习模型] 即基于 PyTorch 实现的深度学习模型，支持全部的6大任务（分词/词性/命名实体/语义角色/依存句法/语义依存）\n\n\n[其他改进] 改进了模型训练方法\n\n[共同] 提供了训练脚本和训练样例，使得用户能够更方便地使用私有的数据，自行训练个性化的模型\n[深度学习模型] 采用 hydra 对训练过程进行配置，方便广大用户修改模型训练参数以及对 LTP 进行扩展（比如使用其他包中的 Module）\n\n\n[其他变化] 分词、依存句法分析 (Eisner) 和 语义依存分析 (Eisner) 任务的解码算法使用 Rust 实现，速度更快\n[新特性] 模型上传至 Huggingface Hub，支持自动下载，下载速度更快，并且支持用户自行上传自己训练的模型供LTP进行推理使用\n[破坏性变更] 改用 Pipeline API 进行推理，方便后续进行更深入的性能优化（如SDP和SDPG很大一部分是重叠的，重用可以加快推理速度），使用说明参见Github快速使用部分\n\n\n4.1.0\n\n提供了自定义分词等功能\n修复了一些bug\n\n\n4.0.0\n\n基于Pytorch 开发，原生 Python 接口\n可根据需要自由选择不同速度和指标的模型\n分词、词性、命名实体、依存句法、语义角色、语义依存6大任务\n\n\n\n快速使用\nPython\n\n注： 如果遇到任何错误，请尝试使用上述命令重新安装 ltp，如果依然报错，请在 Github issues 中反馈。\n\n详细说明\nRust\n\n模型性能以及下载地址\n深度学习模型分词词性命名实体语义角色依存句法语义依存速度(句/S)\nBase98.798.595.480.689.575.239.12\nBase199.2298.7396.3979.2889.5776.57--.--\nBase299.1898.6995.9779.4990.1976.62--.--\nSmall98.498.294.378.488.374.743.13\nTiny96.897.191.670.983.870.153.22\n\n感知机算法分词词性命名实体速度(句/s)备注\nLegacy97.9398.4194.2821581.48性能详情\n\n注：感知机算法速度为开启16线程速度\n构建 Wheel 包\n\n其他语言绑定\n感知机算法\n\nRust\nC/C++\n\n深度学习算法\n\nRust\nC++\nJava\n\n作者信息\n\n冯云龙 &lt;&lt;ylfeng@ir.hit.edu.cn&gt;&gt;\n\n开源协议\n\n语言技术平台面向国内外大学、中科院各研究所以及个人研究者免费开放源代码，但如上述机构和个人将该平台用于商业目的（如企业合作项目等）则需要付费。\n除上述机构以外的企事业单位，如申请使用该平台，需付费。\n凡涉及付费问题，请发邮件到 car@ir.hit.edu.cn 洽商。\n如果您在 LTP 基础上发表论文或取得科研成果，请您在发表论文和申报成果时声明“使用了哈工大社会计算与信息检索研究中心研制的语言技术平台（LTP）”.\n同时，发信给car@ir.hit.edu.cn，说明发表论文或申报成果的题目、出处等。\n\n","id":"/projects/yu-yan-ji-shu-ping-tai/","title":"语言技术平台"},"/publications/":{"body":"","id":"/publications/","title":""},"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"body":"","id":"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/","title":"Fast deletion algorithm for large scale duplicated web pages"},"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"body":"","id":"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/","title":"A New Chinese Natural Language Understanding Architecture Based on Multilayer Search Mechanism"},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"body":"","id":"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/","title":"Similar Chinese sentence retrieval based on improved edit-distance"},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"body":"","id":"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/","title":"Improved-Edit-Distance Kernel for Chinese Relation Extraction"},"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"body":"","id":"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/","title":"Semantic Role Labeling System Using Maximum Entropy Classifier"},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"body":"","id":"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/","title":"A Hybrid Convolution Tree Kernel for Semantic Role Labeling"},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"body":"","id":"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/","title":"A fast clustering algorithm for abnormal and short texts"},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"body":"","id":"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/","title":"A Grammar-driven Convolution Tree Kernel for Semantic Role Classification"},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"body":"","id":"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/","title":"Feature engineering for Chinese semantic role labeling"},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"body":"","id":"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/","title":"HIT-IR-WSD A WSD System for English Lexical Sample Task"},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"body":"","id":"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/","title":"Semantic role labeling with maximum entropy classifier"},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"body":"","id":"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/","title":"A Cascaded Syntactic and Semantic Dependency Parsing System"},"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"body":"","id":"/publications/2008-a-study-on-constituentto-dependency-conversion/","title":"A Study on Constituentto-Dependency Conversion"},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"body":"","id":"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/","title":"Fast Computing Grammar-driven Convolution Tree Kernel for Semantic Role Labeling"},"/publications/2008-introduction-to-information-retrieval-system/":{"body":"","id":"/publications/2008-introduction-to-information-retrieval-system/","title":"Introduction to Information Retrieval System"},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"body":"","id":"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/","title":"Semantic role labeling using a grammar-driven convolution tree kernel"},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"body":"","id":"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/","title":"Using a hybrid convolution tree kernel for semantic role labeling"},"/publications/2008-xin-xi-jian-suo-xi-tong-dao-lun/":{"body":"","id":"/publications/2008-xin-xi-jian-suo-xi-tong-dao-lun/","title":"信息检索系统导论"},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"body":"","id":"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/","title":"Language Specific Issue and Feature Exploration in Chinese Event Extraction"},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"body":"","id":"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/","title":"Multilingual Dependency-based Syntactic and Semantic Parsing"},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"body":"","id":"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/","title":"Appraisal expression recognition with syntactic path for sentence sentiment classification"},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"body":"","id":"/publications/2010-beam-search-based-high-order-dependency-parser-j/","title":"Beam-search based high-order dependency parser J"},"/publications/2010-coherent-dialog-generation-with-query-graph/":{"body":"","id":"/publications/2010-coherent-dialog-generation-with-query-graph/","title":"Coherent Dialog Generation with Query Graph"},"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"body":"","id":"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/","title":"Combining Self-supervised Learning and Active Learning for Disfluency Detection"},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"body":"","id":"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/","title":"HIT-CIR An Unsupervised WSD System Based on Domain Most Frequent Sense Estimation"},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"body":"","id":"/publications/2010-improving-dependency-parsing-using-punctuation/","title":"Improving dependency parsing using punctuation"},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"body":"","id":"/publications/2010-improving-semantic-role-labeling-with-word-sense/","title":"Improving Semantic Role Labeling with Word Sense"},"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"body":"","id":"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/","title":"Interactive Gated Decoder for Machine Reading Comprehension"},"/publications/2010-introduction-to-information-retrieval/":{"body":"","id":"/publications/2010-introduction-to-information-retrieval/","title":"Introduction to information retrieval"},"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"body":"","id":"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/","title":"Jointly Modeling WSD and SRL with Markov Logic"},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"body":"","id":"/publications/2010-ltp-a-chinese-language-technology-platform/","title":"LTP A Chinese Language Technology Platform"},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"body":"","id":"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/","title":"Semi-supervised domain adaptation for WSD Using a word-by-word model selection approach"},"/publications/2010-sou-suo-yin-qing-xin-xi-jian-suo-shi-jian/":{"body":"","id":"/publications/2010-sou-suo-yin-qing-xin-xi-jian-suo-shi-jian/","title":"搜索引擎：信息检索实践"},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"body":"","id":"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/","title":"Using word sense disambiguation for semantic role labeling"},"/publications/2011-a-graph-based-method-for-entity-linking/":{"body":"","id":"/publications/2011-a-graph-based-method-for-entity-linking/","title":"A Graph-based Method for Entity Linking"},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"body":"","id":"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/","title":"HIT Approaches to Entity Linking at TAC 2011"},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"body":"","id":"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/","title":"Improving Chinese POS Tagging with Dependency Parsing"},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"body":"","id":"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/","title":"Joint Models for Chinese POS Tagging and Dependency Parsing"},"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"body":"","id":"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/","title":"The data paper a mechanism to incentivize data publishing in biodiversity science"},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"body":"","id":"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/","title":"Word Sense Disambiguation Corpora Acquisition via Confirmation Code"},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"body":"","id":"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/","title":"A Comparison of Chinese Parsers for Stanford Dependencies"},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"body":"","id":"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/","title":"A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing"},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"body":"","id":"/publications/2012-active-learning-for-chinese-dependency-parsing/","title":"Active learning for Chinese dependency parsing"},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"body":"","id":"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/","title":"Combining statistical model and dictionary for domain adaption of Chinese word segmentation"},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"body":"","id":"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/","title":"Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars"},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"body":"","id":"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/","title":"Hit dependency parsing Bootstrap aggregating heterogeneous parsers"},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"body":"","id":"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/","title":"Improve Chinese Semantic Dependency Parsing via Syntactic Dependency Parsing"},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"body":"","id":"/publications/2012-micro-blogs-oriented-word-segmentation-system/","title":"Micro blogs Oriented Word Segmentation System"},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"body":"","id":"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/","title":"Multiple TreeBanks Integration for Chinese Phrase Structure Grammar Parsing Using Bagging"},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"body":"","id":"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/","title":"SemEval-2012 Task 5 Chinese Semantic Dependency Parsing"},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"body":"","id":"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/","title":"Stacking Heterogeneous Joint Models of Chinese POS Tagging and Dependency Parsing"},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"body":"","id":"/publications/2012-stanfords-system-for-parsing-the-english-web/","title":"Stanfords system for parsing the English web"},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"body":"","id":"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/","title":"A Comparison Sthdy of Sequence Labeling Methods for ChineseWords Segmentation POS Tagging Models"},"/publications/2013-chinese-parsing-exploiting-characters/":{"body":"","id":"/publications/2013-chinese-parsing-exploiting-characters/","title":"Chinese Parsing Exploiting Characters"},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"body":"","id":"/publications/2013-convolution-neural-network-for-relation-extraction/","title":"Convolution neural network for relation extraction"},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"body":"","id":"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/","title":"Effective Bilingual Constraints for Semi-Supervised Learning of NamedEntity Recognizers"},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"body":"","id":"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/","title":"Enhancing Chinese Word Segmentation with Character Clustering"},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"body":"","id":"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/","title":"Joint optimization for Chinese pos tagging and dependency parsing"},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"body":"","id":"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/","title":"Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition"},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"body":"","id":"/publications/2013-named-entity-recognition-with-bilingual-constraints/","title":"Named Entity Recognition with Bilingual Constraints"},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"body":"","id":"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/","title":"A semantics oriented grammar for chinese treebanking"},"/publications/2014-character-level-chinese-dependency-parsing/":{"body":"","id":"/publications/2014-character-level-chinese-dependency-parsing/","title":"Character-Level Chinese Dependency Parsing"},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"body":"","id":"/publications/2014-dependency-graph-based-chinese-semantic-parsing/","title":"Dependency graph based chinese semantic parsing"},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"body":"","id":"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/","title":"Domain Adaptation for CRF-based Chinese Word Segmentation using Free Annotations"},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"body":"","id":"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/","title":"Jointly or Separately Which is Better for Parsing Heterogeneous Dependencies"},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"body":"","id":"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/","title":"Learning Semantic Hierarchies via Word Embeddings"},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"body":"","id":"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/","title":"Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources"},"/publications/2014-reliable-dependency-arc-recognition/":{"body":"","id":"/publications/2014-reliable-dependency-arc-recognition/","title":"Reliable dependency arc recognition"},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"body":"","id":"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/","title":"Revisiting Embedding Features for Simple Semi-supervised Learning"},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"body":"","id":"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/","title":"Sentence Compression for Target-Polarity Word Collocation Extraction"},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"body":"","id":"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/","title":"Type-Supervised Domain Adaptation for Joint Segmentation and POS-Tagging"},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"body":"","id":"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/","title":"Cross-lingual Dependency Parsing Based on Distributed Representations"},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"body":"","id":"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/","title":"Sentence compression for aspect-based sentiment analysis"},"/publications/2015-transition-based-syntactic-linearization/":{"body":"","id":"/publications/2015-transition-based-syntactic-linearization/","title":"Transition-Based Syntactic Linearization"},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"body":"","id":"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/","title":"A distributed representation-based framework for cross-lingual transfer parsing"},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"body":"","id":"/publications/2016-a-neural-attention-model-for-disfluency-detection/","title":"A Neural Attention Model for Disfluency Detection"},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"body":"","id":"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/","title":"A Representation Learning Framework for Multi-Source Transfer Parsing"},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"body":"","id":"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/","title":"A Unified Architecture for Semantic Role Labeling and Relation Classification"},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"body":"","id":"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/","title":"A Universal Framework for Inductive Transfer Parsing across Multi-typed Treebanks"},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"body":"","id":"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/","title":"Chinese Grammatical Error Diagnosis with Long Short-Term Memory Networks"},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"body":"","id":"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/","title":"Enhancing Neural Disfluency Detection with Hand-Crafted Features"},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"body":"","id":"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/","title":"Exploiting multi-typed treebanks for parsing with deep multi-task learning"},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"body":"","id":"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/","title":"Exploring Segment Representations for Neural Segmentation Models"},"/publications/2016-hc-search-for-incremental-parsing/":{"body":"","id":"/publications/2016-hc-search-for-incremental-parsing/","title":"HC-Search for Incremental Parsing"},"/publications/2016-python-cheng-xu-she-ji/":{"body":"","id":"/publications/2016-python-cheng-xu-she-ji/","title":"Python 程序设计"},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"body":"","id":"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/","title":"Transition-based chinese semantic dependency graph parsing"},"/publications/2017-a-review-on-entity-relation-extraction/":{"body":"","id":"/publications/2017-a-review-on-entity-relation-extraction/","title":"A review on entity relation extraction"},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"body":"","id":"/publications/2017-benben-a-chinese-intelligent-conversational-robot/","title":"Benben A Chinese Intelligent Conversational Robot"},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"body":"","id":"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/","title":"Deep Learning in Lexical Analysis and Parsing"},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"body":"","id":"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/","title":"Enhancing LSTM-based word segmentation using unlabeled data"},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"body":"","id":"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/","title":"The first evaluation of Chinese human-computer dialogue technology"},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"body":"","id":"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/","title":"The HIT-SCIR System for End-to-End Parsing of Universal Dependencies"},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"body":"","id":"/publications/2017-transition-based-disfluency-detection-using-lstms/","title":"Transition-Based Disfluency Detection using LSTMs"},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"body":"","id":"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/","title":"A Neural Transition-Based Approach for Semantic Dependency Graph Parsing"},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"body":"","id":"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/","title":"An AMR Aligner Tuned by Transition-based Parser"},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"body":"","id":"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/","title":"Chinese Grammatical Error Diagnosis using Statistical and Prior Knowledge driven Features with Probabilistic Ensemble Enhancement"},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"body":"","id":"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/","title":"Deep Learning in Lexical Analysis and Parsing"},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"body":"","id":"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/","title":"Distilling Knowledge for Search-based Structured Prediction"},"/publications/2018-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li/":{"body":"","id":"/publications/2018-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li/","title":"基于深度学习的自然语言处理"},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"body":"","id":"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/","title":"Joint Extraction of Entities and Relations Based on a Novel GraphScheme"},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"body":"","id":"/publications/2018-parsing-tweets-into-universal-dependencies/","title":"Parsing Tweets into Universal Dependencies"},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"body":"","id":"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/","title":"Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding"},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"body":"","id":"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/","title":"Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation"},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"body":"","id":"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/","title":"Towards Better UD Parsing Deep Contextualized Word Embeddings Ensemble and Treebank Concatenation"},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"body":"","id":"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/","title":"A corpus-free state2seq user simulator for task-oriented dialogue"},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"body":"","id":"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/","title":"A Key-Phrase Aware End2end Neural Response Generation Model"},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"body":"","id":"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"body":"","id":"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/","title":"A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding"},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"body":"","id":"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/","title":"An evaluation of chinese human-computer dialogue technology"},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"body":"","id":"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/","title":"Contextual recurrent units for cloze-style reading comprehension"},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"body":"","id":"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/","title":"Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing"},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"body":"","id":"/publications/2019-cross-lingual-machine-reading-comprehension/","title":"Cross-Lingual Machine Reading Comprehension"},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"body":"","id":"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/","title":"Deep contextualized word embeddings for universal dependency parsing"},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"body":"","id":"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/","title":"Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever"},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"body":"","id":"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/","title":"Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency"},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"body":"","id":"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/","title":"HIT-SCIR at MRP 2019 A Unified Pipeline for Meaning Representation Parsing via Efficient Training and Effective Encoding"},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"body":"","id":"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/","title":"Improving machine reading comprehension via adversarial training"},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"body":"","id":"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/","title":"Learning semantic hierarchies A continuous vector space approach"},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"body":"","id":"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/","title":"Pre-training with whole word masking for chinese bert"},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"body":"","id":"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/","title":"A co-interactive transformer for joint slot filling and intent detection"},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"body":"","id":"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/","title":"A Sentence Cloze Dataset for Chinese Machine Reading Comprehension"},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"body":"","id":"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/","title":"AGIF An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling"},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"body":"","id":"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/","title":"Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection"},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"body":"","id":"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/","title":"Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation"},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"body":"","id":"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/","title":"CoSDA-ML Multi-Lingual Code-Switching Data Augmentation for Zero-ShotCross-Lingual NLP"},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"body":"","id":"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/","title":"Dcr-net A deep co-interactive relation network for joint dialog act recognition and sentiment classification"},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"body":"","id":"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/","title":"Discovering dialog structure graph for open-domain dialog generation"},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"body":"","id":"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/","title":"Discriminative Sentence Modeling for Story Ending Prediction"},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"body":"","id":"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/","title":"Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension"},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"body":"","id":"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/","title":"Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog"},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"body":"","id":"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/","title":"Enhancing Dialog Coherence with Event Graph Grounded Content Planning"},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"body":"","id":"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/","title":"Exploring segment representations for neural semi-Markov conditional random fields"},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"body":"","id":"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/","title":"Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network"},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"body":"","id":"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/","title":"Fewjoint a few-shot learning benchmark for joint language understanding"},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"body":"","id":"/publications/2020-from-static-to-dynamic-word-representations-a-survey/","title":"From static to dynamic word representations a survey"},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"body":"","id":"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/","title":"HIT-SCIR at MRP 2020 Transition-based Parser and Iterative Inference Parser"},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"body":"","id":"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/","title":"Injecting word information with multi-level word adapter for chinese spoken language understanding"},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"body":"","id":"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/","title":"Keywords Generation Improves E-Commerce Session-based Recommendation"},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"body":"","id":"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/","title":"Knowledge graph grounded goal planning for open-domain conversation generation"},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"body":"","id":"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/","title":"Multi-domain spoken language understanding using domain-and task-aware parameterization"},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"body":"","id":"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/","title":"Multi-task self-supervised learning for disfluency detection"},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"body":"","id":"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/","title":"Recall and Learn Fine-tuning Deep Pretrained Language Models with Less Forgetting"},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"body":"","id":"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/","title":"Revisiting Pre-Trained Models for Chinese Natural Language Processing"},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"body":"","id":"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/","title":"Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network"},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"body":"","id":"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/","title":"TextBrewer An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"body":"","id":"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/","title":"Towards Conversational Recommendation over Multi-Type Dialogs"},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"body":"","id":"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/","title":"Understanding medical conversations with scattered keyword attention and weak supervision from responses"},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"body":"","id":"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/","title":"A Closer Look into the Robustness of Neural Dependency Parsers Using Better Adversarial Examples"},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"body":"","id":"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/","title":"A survey on spoken language understanding Recent advances and new frontiers"},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"body":"","id":"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/","title":"Adversarial Training for Machine Reading Comprehension with Virtual Embeddings"},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"body":"","id":"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/","title":"Allocating large vocabulary capacity for cross-lingual language model pre-training"},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"body":"","id":"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/","title":"Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer"},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"body":"","id":"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/","title":"C2c-genda Cluster-to-cluster generation for data augmentation of slot filling"},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"body":"","id":"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/","title":"Character-Level Syntax Infusion in Pre-Trained Models for Chinese Semantic Role Labeling"},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"body":"","id":"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/","title":"Consistency Regularization for Cross-Lingual Fine-Tuning"},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"body":"","id":"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/","title":"Discovering Dialog Structure Graph for Coherent Dialog Generation"},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"body":"","id":"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/","title":"Discovering Drug-Target Interaction Knowledge from Biomedical Literature"},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"body":"","id":"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/","title":"Dont be Contradicted with Anything CI-ToD Towards Benchmarking Consistency for Task-oriented Dialogue System"},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"body":"","id":"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/","title":"Durecdial 20 A bilingual parallel corpus for conversational recommendation"},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"body":"","id":"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/","title":"Dynamic Connected Networks for Chinese Spelling Check"},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"body":"","id":"/publications/2021-few-shot-learning-for-multi-label-intent-detection/","title":"Few-shot learning for multi-label intent detection"},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"body":"","id":"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/","title":"GL-GIN Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling"},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"body":"","id":"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/","title":"Knowing where to leverage Context-aware graph convolutional network with an adaptive fusion layer for contextual spoken language understanding"},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"body":"","id":"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/","title":"LayoutLMv2 Multi-modal Pre-training for Visually-rich Document Understanding"},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"body":"","id":"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/","title":"Learning to Bridge Metric Spaces Few-shot Joint Learning of Intent Detection and Slot Filling"},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"body":"","id":"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/","title":"N-LTP An Open-source Neural Language Technology Platform for Chinese"},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"body":"","id":"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/","title":"Nl-augmenter A framework for task-sensitive natural language augmentation"},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"body":"","id":"/publications/2021-understanding-attention-in-machine-reading-comprehension/","title":"Understanding attention in machine reading comprehension"},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"body":"","id":"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/","title":"Understanding Patient Query with Weak Supervision from Doctor Response"},"/publications/2021-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"body":"","id":"/publications/2021-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/","title":"自然语言处理：基于预训练模型的方法"},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"body":"","id":"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/","title":"Adaptive Unsupervised Self-training for Disfluency Detection"},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"body":"","id":"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/","title":"CCTC A Cross-Sentence Chinese Text Correction Dataset for Native Speakers"},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"body":"","id":"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/","title":"CGIM A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue"},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"body":"","id":"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/","title":"Data augmentation approaches in natural language processing A survey"},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"body":"","id":"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/","title":"ExpMRC explainability evaluation for machine reading comprehension"},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"body":"","id":"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/","title":"FewJoint few-shot learning for joint dialogue understanding"},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"body":"","id":"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/","title":"GL-CLeF A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding"},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"body":"","id":"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/","title":"Graph-Grounded Goal Planning for Conversational Recommendation"},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"body":"","id":"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/","title":"Improving Pre-trained Language Models with Syntactic Dependency Prediction Task for Chinese Semantic Error Recognition"},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"body":"","id":"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/","title":"InterHT Knowledge Graph Embeddings by Interaction between Head and Tail Entities"},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"body":"","id":"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/","title":"Inverse is Better Fast and Accurate Prompt for Few-shot Slot Tagging"},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"body":"","id":"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/","title":"Learning-based Hybrid Local Search for the Hard-label Textual Attack"},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"body":"","id":"/publications/2022-metaprompting-learning-to-learn-better-prompts/","title":"MetaPrompting Learning to Learn Better Prompts"},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"body":"","id":"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/","title":"Multilingual multi-aspect explainability analyses on machine reading comprehension models"},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"body":"","id":"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/","title":"Overview of CTC 2021 Chinese Text Correction for Native Speakers"},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"body":"","id":"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/","title":"Simple and Effective Graph-to-Graph Annotation Conversion"},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"body":"","id":"/publications/2022-teaching-machines-to-read-answer-and-explain/","title":"Teaching machines to read answer and explain"},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"body":"","id":"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/","title":"Text Is No More Enough A Benchmark for Profile-Based Spoken Language Understanding"},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"body":"","id":"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/","title":"Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge"},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"body":"","id":"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/","title":"UniSAr A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL"},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"body":"","id":"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/","title":"A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding"},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"body":"","id":"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/","title":"BridgeTower Building Bridges Between Encoders in Vision-Language Representation Learning"},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"body":"","id":"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/","title":"Combating with extremely noisy samples in weakly supervised slot filling for automatic diagnosis"},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"body":"","id":"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/","title":"Controllable Data Augmentation for Context-Dependent Text-to-SQL"},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"body":"","id":"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/","title":"Cross-lingual Prompting Improving Zero-shot Chain-of-Thought Reasoning across Languages"},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"body":"","id":"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/","title":"CSED A Chinese Semantic Error Diagnosis Corpus"},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"body":"","id":"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/","title":"Improving cross-lingual language understanding with consistency regularization-based fine-tuning"},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"body":"","id":"/publications/2023-language-anisotropic-cross-lingual-model-editing/","title":"Language Anisotropic Cross-Lingual Model Editing"},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"body":"","id":"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/","title":"ManagerTower Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning"},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"body":"","id":"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/","title":"MetricPrompt Prompting Model as a Relevance Metric for Few-shot Text Classification"},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"body":"","id":"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/","title":"MixPro Simple yet Effective Data Augmentation for Prompt-based Learning"},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"body":"","id":"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/","title":"Modularized Pre-Training for End-to-End Task-Oriented Dialogue"},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"body":"","id":"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/","title":"OpenSLU A Unified Modularized and Extensible Toolkit for Spoken Language Understanding"},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"body":"","id":"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/","title":"Semantic-Guided Image Augmentation with Pre-trained Models"},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"body":"","id":"/publications/2024-a-survey-on-natural-language-processing-for-programming/","title":"A Survey on Natural Language Processing for Programming"},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"body":"","id":"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/","title":"A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification"},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"body":"","id":"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/","title":"Beyond Static Evaluation A Dynamic Approach to Assessing AI Assistants API Invocation Capabilities"},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"body":"","id":"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/","title":"Concise and Precise Context Compression for Tool-Using Language Models"},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"body":"","id":"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/","title":"Decoupling breaks data barriers a decoupled pre-training framework for multi-intent spoken language understanding"},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"body":"","id":"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/","title":"Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes"},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"body":"","id":"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models"},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"body":"","id":"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/","title":"Exploring Hybrid Question Answering via Program-based Prompting"},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"body":"","id":"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/","title":"Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL"},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"body":"","id":"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/","title":"Improving Grammatical Error Correction via Contextual Data Augmentation"},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"body":"","id":"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/","title":"LM-Combiner A Contextual Rewriting Model for Chinese Grammatical Error Correction"},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"body":"","id":"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/","title":"M3CoT A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought"},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"body":"","id":"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/","title":"Make Some Noise Unlocking Language Model Parallel Inference Capability through Noisy Training"},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"body":"","id":"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/","title":"OneBit Towards Extremely Low-bit Large Language Models"},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"body":"","id":"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/","title":"Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement"},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"body":"","id":"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/","title":"Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification"},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"body":"","id":"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/","title":"Unlocking the Capabilities of Thought A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought"},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"body":"","id":"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/","title":"What Factors Affect Multi-Modal In-Context Learning An In-Depth Exploration"},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"body":"","id":"/publications/2025-a-survey-of-multilingual-large-language-models/","title":"A survey of multilingual large language models"},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"body":"","id":"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/","title":"Can Large Language Models Understand You Better An MBTI Personality Detection Dataset Aligned with Population Traits"},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"body":"","id":"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/","title":"MURRE Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL"},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"body":"","id":"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/","title":"Towards reasoning era A survey of long chain-of-thought for reasoning large language models"},"/publications/2025-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-copy/":{"body":"","id":"/publications/2025-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-copy/","title":"自然语言处理：基于大语言模型的方法"},"/research/":{"body":"主要研究方向\n\n    \n\n\n","id":"/research/","title":"研究方向"},"/research/方向介绍/":{"body":"方向介绍\n","id":"/research/方向介绍/","title":"方向介绍"},"/research/方向介绍/an-quan/":{"body":"安全对齐小组\n安全对齐小组隶属于赛尔实验室语言分析组，聚焦于复杂专业场景下的指令对齐与潜在安全性问题，深入典型应用场景开展研究。在此过程中，我们注重在专业性、便捷性与安全性之间寻求平衡，推动大模型能力矩阵向专业自动化与安全可信方向持续稳健进化。研究小组关注以下核心方向：\n\n专业场景复杂推理框架：构建适用于跨领域、多步骤任务的推理机制，提升模型在复杂专业需求下的响应能力与执行效果。\n自动数据治理：构建数据采集、生成与筛选的自动流程，确保数据高质量、低噪声，并满足多任务适配需求。\n自动架构设计：结合任务需求自动生成与优化模型结构，提升大模型在专业任务与特定需求下的适应性与表现力。\n安全评估：研究如何构建自动化评估框架，以系统检测模型的内容安全（如有害信息生成）和对抗性安全（如提示注入攻击）。同时，探索模型安全行为的可解释性方法，确保其决策过程透明、可追溯。\n安全对齐：研究模型在常规和对抗场景下的安全对齐方法，并针对智能体和强推理模型优化对齐技术。\n\n该小组与外部优势科研机构具有长期联合培养计划。欢迎对大模型安全对齐技术感兴趣的研究者及同学加入！\n","id":"/research/方向介绍/an-quan/","title":"安全"},"/research/方向介绍/bu-shu/":{"body":"部署小组\n模型高效部署研究小组隶属于赛尔实验室语言分析组，专注于大语言模型的高效推理、量化压缩、边缘部署与分布式计算等关键技术研究。我们致力于探索前沿算法与系统优化方案，以提升大模型在实际应用中的可用性、性能与成本效益。研究小组关注以下核心方向：\n\n模型优化：研究低比特量化、剪枝、蒸馏等方法，降低大模型计算与存储开销。\n高效推理：优化推理框架与解码技术，加速模型在GPU、CPU及边缘设备上的运行。\n分布式与异构计算：探索模型并行化、通信优化及跨设备协同推理，提高大规模模型的可扩展性。\n应用落地：推动模型高效部署在低资源设备、智能助手、迷你智能体等领域的实践。\n\n我们欢迎对模型优化与部署感兴趣的研究者及同学加入，共同推进大模型高效部署的创新研究！\n","id":"/research/方向介绍/bu-shu/","title":"部署"},"/research/方向介绍/ju-shen/":{"body":"具身小组\n具身智能研究小组隶属于赛尔实验室语言分析组，致力于深入探索具身智能领域的前沿问题，特别是在代码与智能交互、视觉推理、数据构建等方面的创新性研究。我们的小组以代码为切入点，研究如何通过技术推动具身智能的实现与发展，探索智能体在物理世界中的感知与认知能力。\n研究方向\n\n数据构建：我们专注于高质量数据集的构建和优化，旨在为具身智能研究提供强有力的基础支持。通过创新的数据构建方法，提升模型的泛化能力和推理精度。\n视觉推理：我们研究如何通过视觉推理技术将图像或视频信息转化为可操作的代码，推动智能体在视觉理解和行为执行之间的无缝衔接。\n视觉编码器：开发高效的视觉编码器，提升计算机视觉模型的理解与处理能力，以便更好地支持具身智能系统在复杂环境中的决策和执行能力。\nVLA基座模型：探索VLA（Visual Language Agent）模型，研究如何通过视觉与语言的结合提升智能体的跨模态推理能力，使其能够处理更复杂、更抽象的任务。\n基于代码的规划：我们研究如何利用代码来更好地完成具身智能的规划任务，通过代码驱动的规划方法，提高智能体在动态环境中决策、行动和适应的能力。\n\n我们的小组将在具身智能的广泛应用场景中，推动前沿技术的突破，力求实现更加智能、灵活的计算系统，推动人工智能走向更加真实和复杂的物理世界。\n","id":"/research/方向介绍/ju-shen/","title":"具身"},"/research/方向介绍/sheng-cheng/":{"body":"生成小组\n代码\n代码相关任务是大模型在软件开发领域的重要应用，涵盖代码生成、代码翻译、注释生成、代码补全和错误修复等。通过学习海量开源代码数据，大模型能够理解编程语言的语法、语义及开发者意图，从而生成高质量的代码或优化现有代码。例如，大模型可以根据自然语言描述生成对应的功能代码，将一种编程语言翻译为另一种，或为复杂代码段自动生成清晰的注释。这些能力显著提升了开发效率，降低了技术门槛，同时促进了跨语言、跨团队的协作。随着大模型的不断进化，其在代码领域的智能化水平正逐步接近甚至超越人类开发者，为软件工程带来更多可能性。\nText-to-SQL\nText-to-SQL任务旨在将自然语言问题自动转化为可执行的SQL查询语句，从而实现用户通过日常语言与数据库交互的目标。这一技术在数据分析、商业智能等领域具有重要应用价值。随着大模型（如GPT、LLaMA等）的兴起，Text-to-SQL的能力得到了显著提升。大模型凭借其强大的语言理解能力和上下文学习能力，能够更好地解析复杂语义、处理多表关联以及生成高质量的SQL语句。此外，通过微调或提示工程（Prompt Engineering），大模型可以适应特定领域的数据库模式，进一步提高生成SQL的准确性和效率，为用户提供更智能、便捷的数据查询体验。\n任务型对话系统\n任务型对话系统是指以人机对话形式提供信息或服务的系统，它能够帮助人们完成一些垂直领域的服务，例如查天气、酒店预订和订机票等。 近年来，任务型对话系统的研究主要分为两个流派：流水线任务型对话系统和端到端任务型对话系统。其中流水线任务型对话系统主要由自然语言理解模块、对话状态跟踪模块、对话策略学习模块和自然语言生成模块组成，并需要通过各个子模块协同工作一起生成对话系统回复。而端到端任务型对话系统则可以直接通过一个统一的序列到序列模型生成对话系统的回复。\n","id":"/research/方向介绍/sheng-cheng/","title":"生成"},"/research/方向介绍/tui-li/":{"body":"LARG（LA-Reasoning-Group）小组\n推理大模型是指通过多步逻辑推理以解决复杂推理问题，它能够帮助人们完成大量的需要复杂逻辑推断的任务，如解决复杂数学问题、编写竞赛级代码等。近年来，推理大模型的研究主要分为三个阶段：（1）分析推理机理，以理解推理能力的涌现来源、行为规律与运行机制，从而指导推理的优化；（2）增强推理技术，以优化推理能力的逻辑深度，思维广度与适度反思，从而促进推理的应用；（3）拓展推理应用，以满足推理能力的广泛、快速与可靠应用，从而促进推理应用的最终落地。\n推理机理\n推理一直是认知科学、哲学和人工智能领域的核心议题。随着模型和工具的发展，研究者不断从逻辑学、神经科学和概率推理等角度探索推理的复杂性。尽管大语言模型在处理复杂任务时表现出了显著的推理能力，其推理机制仍未完全揭示，特别是在与人类推理路径的异同方面，仍有诸多未知。目前，大多数提示优化策略仍依赖经验，缺乏系统性的理论支撑。因此，构建一种统一且可量化的推理建模方法，既能深化我们对人类推理过程的理解，也能推动人工智能推理能力的发展，成为当前研究的关键挑战之一。\n本组研究专注于\n\n以系统性视角探索推理大模型的理论框架\n以优美的数学公式揭示推理大模型的运行规律\n以动态的视角分析推理大模型能力的起源\n\n推理技术\n近年来，传统监督微调的在推理大模型中面临瓶颈：监督微调依赖大量标注数据且其泛化能力有限。在此背景下，强化学习凭借其自主探索和奖励驱动的特性成为突破的方向。例如，DeepSeek-R1-Zero仅仅通过强化学习训练，使数学推理的准确率从15.6%跃升至71%。强化学习不仅能够规避数据标注的高成本，还通过试错机制激发模型的“顿悟时刻”，如自发调整推理路径以提高答案的准确性。这一特点为推理能力的自我进化提供了新的范式。因此，如何建立一种更高效、准确的推理训练方法，成为当前研究的关键挑战之一。\n本组研究专注于\n\n如何更高效地激发推理大模型的性能\n如何增强推理大模型的推理能力\n如何平衡推理训练的效率与性能提升\n\n推理应用\n随着推理大模型在任务规划与推理能力方面的不断提升，如何在现有模型基础上实现高效且合理的推理，已成为研究领域的热点问题。部分研究已开始将推理大模型应用于多语言、多模态等多种场景。然而，在应用过程中，推理速度慢和可靠性差的问题依然存在。因此，如何扩展推理应用，以实现推理能力的广泛、快速和可靠应用，进而推动其实际落地，已成为当前研究的关键挑战之一。\n本组研究专注于\n\n提升推理大模型的推理速度；\n扩展推理大模型的应用场景；\n增强推理大模型应用的可靠性。\n\n","id":"/research/方向介绍/tui-li/","title":"推理"},"/resources/":{"body":"","id":"/resources/","title":"技术资源"},"/resources/开源项目/":{"body":"开源项目\n","id":"/resources/开源项目/","title":"开源项目"},"/resources/开源项目/abacus/":{"body":"","id":"/resources/开源项目/abacus/","title":"Abacus"},"/resources/开源项目/chinese-mixtral-8x7b/":{"body":"","id":"/resources/开源项目/chinese-mixtral-8x7b/","title":"Chinese-Mixtral-8x7B"},"/resources/开源项目/ltp/":{"body":"LTP 4\nLTP（Language Technology Platform） 提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。\n引用\n如果您在工作中使用了 LTP，您可以引用这篇论文\n\n参考书：\n由哈工大社会计算与信息检索研究中心（HIT-SCIR）的多位学者共同编著的《自然语言处理：基于预训练模型的方法\n》（作者：车万翔、郭江、崔一鸣；主审：刘挺）一书现已正式出版，该书重点介绍了新的基于预训练模型的自然语言处理技术，包括基础知识、预训练词向量和预训练模型三大部分，可供广大LTP用户学习参考。\n更新说明\n\n4.2.0\n\n[结构性变化] 将 LTP 拆分成 2 个部分，维护和训练更方便，结构更清晰\n\n[Legacy 模型] 针对广大用户对于推理速度的需求，使用 Rust 重写了基于感知机的算法，准确率与 LTP3 版本相当，速度则是 LTP v3 的 3.55 倍，开启多线程更可获得 17.17 倍的速度提升，但目前仅支持分词、词性、命名实体三大任务\n[深度学习模型] 即基于 PyTorch 实现的深度学习模型，支持全部的6大任务（分词/词性/命名实体/语义角色/依存句法/语义依存）\n\n\n[其他改进] 改进了模型训练方法\n\n[共同] 提供了训练脚本和训练样例，使得用户能够更方便地使用私有的数据，自行训练个性化的模型\n[深度学习模型] 采用 hydra 对训练过程进行配置，方便广大用户修改模型训练参数以及对 LTP 进行扩展（比如使用其他包中的 Module）\n\n\n[其他变化] 分词、依存句法分析 (Eisner) 和 语义依存分析 (Eisner) 任务的解码算法使用 Rust 实现，速度更快\n[新特性] 模型上传至 Huggingface Hub，支持自动下载，下载速度更快，并且支持用户自行上传自己训练的模型供LTP进行推理使用\n[破坏性变更] 改用 Pipeline API 进行推理，方便后续进行更深入的性能优化（如SDP和SDPG很大一部分是重叠的，重用可以加快推理速度），使用说明参见Github快速使用部分\n\n\n4.1.0\n\n提供了自定义分词等功能\n修复了一些bug\n\n\n4.0.0\n\n基于Pytorch 开发，原生 Python 接口\n可根据需要自由选择不同速度和指标的模型\n分词、词性、命名实体、依存句法、语义角色、语义依存6大任务\n\n\n\n快速使用\nPython\n\n注： 如果遇到任何错误，请尝试使用上述命令重新安装 ltp，如果依然报错，请在 Github issues 中反馈。\n\n详细说明\nRust\n\n模型性能以及下载地址\n深度学习模型分词词性命名实体语义角色依存句法语义依存速度(句/S)\nBase98.798.595.480.689.575.239.12\nBase199.2298.7396.3979.2889.5776.57--.--\nBase299.1898.6995.9779.4990.1976.62--.--\nSmall98.498.294.378.488.374.743.13\nTiny96.897.191.670.983.870.153.22\n\n感知机算法分词词性命名实体速度(句/s)备注\nLegacy97.9398.4194.2821581.48性能详情\n\n注：感知机算法速度为开启16线程速度\n构建 Wheel 包\n\n其他语言绑定\n感知机算法\n\nRust\nC/C++\n\n深度学习算法\n\nRust\nC++\nJava\n\n作者信息\n\n冯云龙 &lt;&lt;ylfeng@ir.hit.edu.cn&gt;&gt;\n\n开源协议\n\n语言技术平台面向国内外大学、中科院各研究所以及个人研究者免费开放源代码，但如上述机构和个人将该平台用于商业目的（如企业合作项目等）则需要付费。\n除上述机构以外的企事业单位，如申请使用该平台，需付费。\n凡涉及付费问题，请发邮件到 car@ir.hit.edu.cn 洽商。\n如果您在 LTP 基础上发表论文或取得科研成果，请您在发表论文和申报成果时声明“使用了哈工大社会计算与信息检索研究中心研制的语言技术平台（LTP）”.\n同时，发信给car@ir.hit.edu.cn，说明发表论文或申报成果的题目、出处等。\n\n","id":"/resources/开源项目/ltp/","title":"LTP"},"/resources/开源项目/pyltp/":{"body":"pyltp\n\n\n\n\n\npyltp 是 语言技术平台（Language Technology Platform, LTP）的 Python 封装。\n在使用 pyltp 之前，您需要简要了解 语言技术平台（LTP） 能否帮助您解决问题。\n目前基于Pytorch的LTP4 已经发布，而PyLTP将会只有非常有限的维护，请大家移步使用[LTP 4](LTP 4)\n","id":"/resources/开源项目/pyltp/","title":"pyltp"},"/resources/社区资源/":{"body":"社区资源\n","id":"/resources/社区资源/","title":"社区资源"},"/resources/社区资源/da/":{"body":"","id":"/resources/社区资源/da/","title":"DA"},"/resources/社区资源/elmoformanylangs/":{"body":"","id":"/resources/社区资源/elmoformanylangs/","title":"ELMoForManyLangs"},"/resources/社区资源/la-beginner/":{"body":"","id":"/resources/社区资源/la-beginner/","title":"LA-beginner"},"/resources/社区资源/slu/":{"body":"","id":"/resources/社区资源/slu/","title":"A Survey on Spoken Language Understanding: Recent Advances and New Frontiers"},"/resources/社区资源/task-oriented-dialog-research-progress/":{"body":"","id":"/resources/社区资源/task-oriented-dialog-research-progress/","title":"Task-Oriented Dialog Research Progress"},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"body":"下载HIT-SCIR的模型和数据集\n\n\nHugging Face - HIT-SCIR\n查看 Hugging Face 上 HIT-SCIR 发布的模型和相关资源。\n\n\nModelScope - HIT-SCIR\n查看 ModelScope 上 HIT-SCIR 发布的模型和相关资源。\n\n\nWiseModel - HIT-SCIR\n查看 WiseModel 上 HIT-SCIR 发布的模型和相关资源。\n\n\n","id":"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/","title":"下载HIT-SCIR的模型和数据集"}},"docInfo":{"/":{"body":0,"title":0},"/about/":{"body":4,"title":0},"/demo/":{"body":0,"title":0},"/demo/演示系统/":{"body":0,"title":0},"/demo/演示系统/codegen/":{"body":0,"title":0},"/demo/演示系统/csc/":{"body":0,"title":0},"/demo/演示系统/huozi/":{"body":0,"title":0},"/demo/演示系统/ltp/":{"body":64,"title":1},"/demo/演示系统/zhu-suan-sql/":{"body":4,"title":1},"/demo/演示系统/zhu-suan-vscodecha-jian/":{"body":1,"title":1},"/joinus/":{"body":0,"title":0},"/news/":{"body":0,"title":0},"/news/acl-2010-2020yan-jiu-qu-shi-zong-jie/":{"body":0,"title":3},"/news/acl-2022-fan-xiang-yu-ce-geng-hao-ji-yu-fan-xiang-ti-shi-de-xiao-yang-ben-cao-wei-biao-zhu-fang-fa/":{"body":0,"title":1},"/news/ccl2021xue-sheng-yan-tao-hui-ru-he-dan-sheng-idea-ru-he-gen-shen-gao-ren-rebuttal-ru-he-xie-zi-ji-de-di-yi-pian-ding-hui-wen-zhang-deng-qiang-xian-kan/":{"body":0,"title":1},"/news/cong-jing-tai-dao-dong-tai-ci-biao-zheng-jin-ji-shi-nian-fa-zhan-hui-gu/":{"body":0,"title":0},"/news/di-er-jie-thunlp-hit-scirxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"body":0,"title":3},"/news/di-er-shi-jie-zhong-guo-ji-suan-yu-yan-xue-da-hui-ccl-2021-zheng-gao-qi-shi/":{"body":0,"title":2},"/news/ha-gong-da-fa-bu-zhu-suan-dai-ma-da-mo-xing/":{"body":0,"title":0},"/news/ha-gong-da-jiu-da-aimo-xing-deng-chang-jie-suo-qian-xing-bai-ye-zhi-neng-xin-fan-shi/":{"body":0,"title":1},"/news/ha-gong-da-ju-ban-deepseekji-shu-qian-yan-yu-ying-yong-zhu-ti-jiang-zuo/":{"body":0,"title":1},"/news/ha-gong-da-kai-yuan-huo-zi-3-5-dui-hua-da-mo-xing/":{"body":0,"title":1},"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing-3-0ban-ben/":{"body":0,"title":1},"/news/ha-gong-da-kai-yuan-huo-zi-dui-hua-da-mo-xing/":{"body":0,"title":0},"/news/ha-gong-da-ltpyu-yan-ji-shu-ping-tai-zheng-shi-shang-xian-guo-jia-zhi-hui-jiao-yu-gong-gong-fu-wu-ping-tai/":{"body":0,"title":1},"/news/ha-gong-da-pptgong-kai-deepseekji-shu-qian-yan-yu-ying-yong-zhuan-ti-jiang-zuo-1xiao-shi-kuai-su-zhang-wo-deepseekji-ben-yuan-li/":{"body":0,"title":1},"/news/ha-gong-da-scir-13pian-chang-wen-bei-acl-2023zhu-hui-findingslu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scir-14pian-chang-wen-bei-acl-2021zhu-hui-findingshe-ijcai-2021lu-yong/":{"body":0,"title":4},"/news/ha-gong-da-scir-14pian-chang-wen-bei-emnlp-2024zhu-hui-findingslu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scir-2023jie-29ming-tong-xue-shun-li-tong-guo-shuo-shi-da-bian/":{"body":0,"title":2},"/news/ha-gong-da-scir-2025yuan-dan-wan-hui-cheng-gong-ju-ban/":{"body":0,"title":2},"/news/ha-gong-da-scir-20pian-chang-wen-bei-acl-2024zhu-hui-findingslu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scir-22pian-chang-wen-bei-emnlp-2025zhu-hui-findingslu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scir-29pian-chang-wen-bei-acl-2025zhu-hui-findingslu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scir-6pian-zhu-hui-2pian-findings-1pian-demo-gong-9pian-chang-wen-bei-emnlp-2021lu-yong/":{"body":0,"title":4},"/news/ha-gong-da-scir-8pian-chang-wen-bei-coling-2025lu-yong/":{"body":0,"title":4},"/news/ha-gong-da-scir-9pian-lun-wen-bei-neurips-2024lu-yong/":{"body":0,"title":4},"/news/ha-gong-da-scir-fa-bu-zhu-suan-sql/":{"body":0,"title":2},"/news/ha-gong-da-scir-zhu-suan-sql-bei-acl-2025-demolu-yong/":{"body":0,"title":5},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-da-yu-yan-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"body":0,"title":1},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-chu-ban/":{"body":0,"title":1},"/news/ha-gong-da-scir-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-yi-shu-ru-xuan-zhong-guo-zhi-wang-gao-bei-yin-tu-shu-top-1-2019-2023/":{"body":0,"title":3},"/news/ha-gong-da-scirba-pian-chang-wen-bei-acl-2020lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirba-pian-lun-wen-bei-aaai-20lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirba-pian-lun-wen-bei-emnlp-ijcnlp-2019lu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-huo-de-di-liu-jie-bai-du-jiang-xue-jin/":{"body":0,"title":1},"/news/ha-gong-da-scirbo-shi-sheng-liu-yi-jia-yin-qing-yu-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":0,"title":1},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-2021nian-wei-ruan-xue-zhe-cheng-hao/":{"body":0,"title":1},"/news/ha-gong-da-scirbo-shi-sheng-tan-li-bo-huo-de-di-ba-jie-bai-du-jiang-xue-jin/":{"body":0,"title":1},"/news/ha-gong-da-scirbo-shi-sheng-xu-jun-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":0,"title":1},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-dang-xuan-acl-2025cheng-xu-wei-yuan-hui-zhu-xi/":{"body":0,"title":2},"/news/ha-gong-da-scirche-mo-xiang-jiao-shou-ru-xuan-2019nian-du-long-jiang-xue-zhe-qing-nian-xue-zhe/":{"body":0,"title":1},"/news/ha-gong-da-scirche-mo-xiang-liu-ting-zi-ran-yu-yan-chu-li-xin-fan-shi-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"body":0,"title":1},"/news/ha-gong-da-scirduo-ming-jiao-shi-shou-yao-can-jia-yssnlp-2019/":{"body":0,"title":2},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-acl-2018/":{"body":0,"title":2},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-coling-2018/":{"body":0,"title":2},"/news/ha-gong-da-scirduo-ming-shi-sheng-can-jia-ijcai-2018/":{"body":0,"title":2},"/news/ha-gong-da-scirduo-wei-shi-sheng-shou-yao-can-jia-di-yi-jie-zhong-guo-zi-ran-yu-yan-chu-li-xue-sheng-yan-tao-hui-cssnlp-2020/":{"body":0,"title":2},"/news/ha-gong-da-scirjiu-pian-chang-wen-bei-emnlp-2020ji-zi-kan-lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-jing-dong-wang-yu-xuan-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":0,"title":1},"/news/ha-gong-da-scirliang-wei-bo-shi-sheng-shi-xiao-ming-hou-yu-tai-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":0,"title":1},"/news/ha-gong-da-scirliu-pian-chang-wen-bei-coling-2018lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirliu-pian-wen-zhang-bei-coling-2020lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirqu-de-ccir-cup-2022hun-he-biao-ge-yu-wen-ben-shu-ju-wen-da-sai-dao-guan-jun/":{"body":0,"title":3},"/news/ha-gong-da-scirqu-de-guo-jia-dian-wang-diao-kong-aichuang-xin-da-sai-sai-dao-2-text2sql-guan-jun/":{"body":0,"title":1},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-aaai-2021lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirsan-pian-chang-wen-bei-ijcai-pricai-2020lu-yong/":{"body":0,"title":3},"/news/ha-gong-da-scirsan-pian-lun-wen-bei-acl-2019lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirsan-wei-bo-shi-sheng-li-jia-qi-yuan-jian-hua-liu-ze-ming-shun-li-tong-guo-bo-shi-xue-wei-da-bian/":{"body":0,"title":1},"/news/ha-gong-da-scirshi-pian-chang-wen-bei-emnlp-2022zhu-hui-ji-zi-kan-lu-yong/":{"body":0,"title":2},"/news/ha-gong-da-scirshi-sheng-can-jia-ccir-2019/":{"body":0,"title":2},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2018/":{"body":0,"title":2},"/news/ha-gong-da-scirshi-sheng-can-jia-ccl-2019/":{"body":0,"title":2},"/news/ha-gong-da-scirshi-sheng-can-jia-di-ba-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2019/":{"body":0,"title":2},"/news/ha-gong-da-scirshi-yan-shi-shi-sheng-can-jia-acl-2025-xue-shu-hui-yi/":{"body":0,"title":3},"/news/ha-gong-da-scirzai-conll-2017duo-yu-yan-tong-yong-yi-cun-ju-fa-fen-xi-ping-ce-zhong-qu-de-jia-ji/":{"body":0,"title":2},"/news/ha-gong-da-scirzai-conll-2019guo-ji-kua-kuang-jia-yu-yi-fen-xi-ping-ce-zhong-qu-de-di-yi-ming/":{"body":0,"title":2},"/news/ha-gong-da-scirzai-mmnlu-22duo-yu-yan-ren-wu-xing-dui-hua-zi-ran-yu-yan-li-jie-ping-ce-qu-de-full-datasetsai-dao-di-yi-ming/":{"body":0,"title":3},"/news/ha-gong-da-xun-fei-lian-he-shi-yan-shi-yan-zhi-de-wen-ben-shun-hua-ji-shu-cheng-gong-jie-ru-xun-fei-ting-jian-xi-tong/":{"body":0,"title":0},"/news/ha-gong-da-xun-fei-rong-huo-2024nian-du-wu-wen-jun-ren-gong-zhi-neng-ke-xue-ji-shu-jiang-ke-ji-jin-bu-jiang-yi-deng-jiang/":{"body":0,"title":1},"/news/ha-gong-da-zhu-suan-da-mo-xing-tui-chu-vscodecha-jian/":{"body":0,"title":1},"/news/ha-gong-da-zi-ran-yu-yan-chu-li-yan-jiu-suo-gong-kai-chatgptdiao-yan-bao-gao-nei-ce-ha-gong-da-huo-zi-dui-hua-da-mo-xing/":{"body":0,"title":1},"/news/hit-scirfa-bu-shou-ge-zhong-wen-kuo-ci-biao-zeng-liang-yu-xun-lian-hun-he-zhuan-jia-mo-xing-chinese-mixtral-8x7b/":{"body":0,"title":4},"/news/jin-ri-arxivzui-re-nlpda-mo-xing-lun-wen-zuo-dao-tou-liao-qing-hua-he-ha-gong-da-ba-da-mo-xing-liang-hua-zuo-dao-liao-1bi-te/":{"body":0,"title":1},"/news/jin-tian-ha-gong-da-zai-huo-guo-jia-biao-zhang/":{"body":0,"title":0},"/news/ltp-4-0-dan-mo-xing-wan-cheng-6xiang-zi-ran-yu-yan-chu-li-ren-wu/":{"body":0,"title":2},"/news/mai-xiang-tui-li-shi-dai-900-pian-can-kao-wen-xian-jie-shi-chang-lian-si-wei-de-qian-shi-jin-sheng-ha-gong-da-scir-tui-chu-quan-mian-zong-shu/":{"body":0,"title":2},"/news/qing-chun-de-xuan-ze-gun-ha-gong-da-zhe-ge-tuan-dui-li-yu-sheng-cheng-shi-aichao-tou/":{"body":0,"title":1},"/news/sai-er-bi-ji-da-mo-xing-shang-xia-wen-chang-du-kuo-zhan-zhong-de-jian-suo-zeng-qiang-ji-shu-jian-shu/":{"body":0,"title":0},"/news/sai-er-bi-ji-mian-xiang-biao-ge-shu-ju-de-da-mo-xing-tui-li-zong-shu/":{"body":0,"title":0},"/news/sai-er-bi-ji-tou-ji-jie-ma-jie-suo-zi-hui-gui-tui-li-su-du-shang-xian/":{"body":0,"title":0},"/news/sai-er-bi-ji-xin-fen-lei-quan-zong-jie-zui-xin-awesome-slu-surveyzi-yuan-ku-kai-yuan/":{"body":0,"title":3},"/news/sai-er-yuan-chuang-aaai-2021-jiu-jie-yu-lian-he-xue-xi-zhong-de-jian-mo-fang-fa-kuai-lai-kan-kan-tu-wang-luo-xian-shi-jian-mo/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-aaai-2021-shu-ju-zeng-qiang-mei-xiao-guo-shi-shi-yong-cluster-to-clustersheng-cheng-geng-duo-yang-hua-de-xin-shu-ju-ba/":{"body":0,"title":4},"/news/sai-er-yuan-chuang-aaai-2022-ji-yu-profilexin-xi-de-kou-yu-yu-yan-li-jie-ji-zhun/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-aaai-2023-bridgetower-zai-shi-jue-yu-yan-biao-shi-xue-xi-zhong-jian-li-bian-ma-qi-jian-de-qiao-liang/":{"body":0,"title":3},"/news/sai-er-yuan-chuang-aaai-2024-yu-yi-yin-dao-de-sheng-cheng-shi-tu-xiang-zeng-yan-fang-fa/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-aaai20-ji-yu-goal-hua-ti-de-kai-fang-yu-duo-lun-dui-hua-gui-hua/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-aaai20-yong-yu-lian-he-jian-mo-dui-hua-xing-wei-shi-bie-he-qing-gan-fen-lei-de-shen-du-jiao-hu-guan-xi-wang-luo/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-aaai2021-xiao-yang-ben-xue-xi-xia-de-duo-biao-qian-fen-lei-wen-ti-chu-tan/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-acl-2021-ji-yu-yi-zhi-xing-zheng-ze-de-kua-yu-yan-wei-diao-fang-fa/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-acl-2021-kai-fang-yu-dui-hua-jie-gou-fa-xian/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-acl-findings-ji-yu-gao-zhi-liang-dui-kang-yang-ben-de-yi-cun-fen-xi-qi-lu-bang-xing-tan-jiu/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-acl-findings-ren-wu-gong-wu-xiao-yang-ben-chang-jing-xia-de-duo-ren-wu-lian-he-xue-xi-fang-fa-chu-tan/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-acl20-ji-yu-dui-hua-tu-pu-de-kai-fang-yu-duo-lun-dui-hua-ce-lue-xue-xi/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-acl20-ji-yu-tu-zhu-yi-li-wang-luo-de-duo-li-du-ji-qi-yue-du-li-jie-wen-dang-jian-mo/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-acl20-rang-mo-xing-shi-ban-gong-bei-tan-jiu-shao-yang-ben-xu-lie-biao-zhu-fang-fa/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-acl20-yong-yu-duo-ling-yu-duan-dao-duan-ren-wu-xing-dui-hua-xi-tong-de-dong-tai-rong-he-wang-luo/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-che-mo-xiang-jiao-shou-tuan-dui-zui-xin-zong-shu-dui-hua-xi-tong-zhong-kou-yu-yu-yan-li-jie-yan-jiu-de-xin-jin-zhan-yu-xin-ling-yu/":{"body":0,"title":0},"/news/sai-er-yuan-chuang-coling-2022-cctc-mian-xiang-zhong-wen-mu-yu-shi-yong-zhe-de-kua-ju-zi-wen-ben-jiu-cuo-shu-ju-ji/":{"body":0,"title":3},"/news/sai-er-yuan-chuang-coling-2022-metaprompting-ji-yu-yuan-xue-xi-de-soft-promptchu-shi-hua-fang-fa/":{"body":0,"title":4},"/news/sai-er-yuan-chuang-coling-2022-rong-he-zi-gua-ying-ji-zhi-yu-zi-xun-lian-kuang-jia-de-wu-jian-du-wen-ben-shun-hua-fang-fa/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-coling2024-lm-combiner-tong-guo-mo-xing-gai-xie-shi-xian-geng-jing-zhun-de-yu-fa-jiu-cuo/":{"body":0,"title":3},"/news/sai-er-yuan-chuang-coling2024-mian-xiang-bian-cheng-de-zi-ran-yu-yan-chu-li-zong-shu/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-coling2024-ren-gong-zhi-neng-zhu-shou-apidiao-yong-neng-li-de-dong-tai-ping-gu-fang-fa/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-coling24-ji-cha-ji-yong-zi-dong-ti-qu-ling-yu-xiang-guan-te-zheng-ti-sheng-fan-hua-neng-li/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-coling24-wu-xu-biao-zhu-ji-ke-zeng-qiang-mo-xing-cot-neng-li/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-emnlp-2020-qie-hui-yi-qie-xue-xi-zai-geng-shao-de-yi-wang-xia-jing-diao-shen-ceng-yu-xun-lian-yu-yan-mo-xing/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-emnlp-2020-rong-he-zi-xun-lian-he-zi-jian-du-fang-fa-de-wu-jian-du-wen-ben-shun-hua-yan-jiu/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-emnlp-2021-duo-yu-yan-he-kua-yu-yan-dui-hua-tui-jian/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-emnlp-2021-yu-xun-lian-kua-yu-yan-mo-xing-zhong-de-da-ci-biao-gou-jian-ji-shi-yong/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-emnlp-2023-tong-guo-kua-yu-yan-ti-shi-gai-jin-ling-yang-ben-cot-tui-li-neng-li/":{"body":0,"title":3},"/news/sai-er-yuan-chuang-findings-ji-yu-dong-tai-tu-jiao-hu-wang-luo-de-duo-yi-tu-kou-yu-yu-yan-li-jie-kuang-jia/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-findings-zhong-wen-yu-xun-lian-yu-yan-mo-xing-hui-gu/":{"body":0,"title":1},"/news/sai-er-yuan-chuang-icassp-2021-shou-ci-tan-suo-zhong-wen-ci-xin-xi-zeng-qiang-zhong-wen-kou-yu-yu-yan-li-jie/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-n-ltp-ji-yu-yu-xun-lian-mo-xing-de-zhong-wen-zi-ran-yu-yan-chu-li-ping-tai/":{"body":0,"title":2},"/news/sai-er-yuan-chuang-shou-ge-ren-wu-xing-dui-hua-xi-tong-zhong-sheng-cheng-mo-kuai-zi-yuan-ku-awesome-tod-nlg-surveykai-yuan/":{"body":0,"title":4},"/news/semeval-2016-task-9zhong-wen-yu-yi-yi-cun-tu-shu-ju-dui-wai-fa-bu/":{"body":0,"title":4},"/news/wo-zhong-xin-3pian-chang-wen-bei-aaai-2018lu-yong/":{"body":0,"title":2},"/news/wo-zhong-xin-3pian-chang-wen-bei-acl-2018lu-yong/":{"body":0,"title":2},"/news/wo-zhong-xin-6pian-chang-wen-bei-ijcai-ecai-2018lu-yong/":{"body":0,"title":3},"/news/wo-zhong-xin-bo-shi-sheng-guo-jiang-shun-li-tong-guo-bo-shi-da-bian/":{"body":0,"title":0},"/news/wo-zhong-xin-che-mo-xiang-jiao-shou-shou-yao-can-jia-di-er-jie-teng-xun-ai-labxue-shu-lun-tan/":{"body":0,"title":2},"/news/wo-zhong-xin-fan-yi-de-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li-yi-shu-zheng-shi-chu-ban/":{"body":0,"title":0},"/news/wo-zhong-xin-qing-nian-jiao-shi-shou-yao-can-jia-di-shi-si-jie-zhong-guo-zi-ran-yu-yan-chu-li-qing-nian-xue-zhe-yan-tao-hui/":{"body":0,"title":0},"/news/wo-zhong-xin-shi-sheng-can-jia-di-liu-jie-zi-ran-yu-yan-chu-li-he-zhong-wen-ji-suan-hui-yi-nlpcc-2017/":{"body":0,"title":2},"/news/wo-zhong-xin-shi-sheng-can-jia-di-shi-liu-jie-quan-guo-ji-suan-yu-yan-xue-hui-yi-ccl-2017/":{"body":0,"title":2},"/news/wo-zhong-xin-shi-sheng-can-jia-emnlp-2017/":{"body":0,"title":2},"/news/xin-wen-di-san-jie-hit-scir-thunlp-fudannlpxue-shu-lian-yi-hui-cheng-gong-ju-ban/":{"body":0,"title":2},"/news/yef2021yan-jiang-shi-lu-ha-er-bin-gong-ye-da-xue-che-mo-xiang-zi-ran-yu-yan-chu-li-xin-fan-shi/":{"body":0,"title":1},"/news/yu-yan-ji-shu-ping-tai-ltp-tui-chu-v4-2-ban-ben/":{"body":0,"title":2},"/news/yu-yan-ji-shu-ping-tai-ltp-you-xin-jia-la/":{"body":0,"title":1},"/news/zhu-he-wo-zhong-xin-che-mo-xiang-lao-shi-jin-sheng-jiao-shou/":{"body":0,"title":0},"/news/zui-xin-ha-gong-da-scirzai-guo-ji-duo-yu-yan-tong-yong-yi-cun-fen-xi-ping-ce-zhong-duo-de-guan-jun/":{"body":0,"title":1},"/people/":{"body":0,"title":0},"/people/在读博士生/":{"body":0,"title":0},"/people/在读博士生/chen-qi-guang/":{"body":0,"title":0},"/people/在读博士生/feng-yun-long/":{"body":0,"title":0},"/people/在读博士生/guan-jian-nan/":{"body":0,"title":0},"/people/在读博士生/he-ye/":{"body":0,"title":0},"/people/在读博士生/li-bo-han/":{"body":0,"title":0},"/people/在读博士生/li-shang-zhan/":{"body":0,"title":0},"/people/在读博士生/liu-yi-jun/":{"body":0,"title":0},"/people/在读博士生/luo-xian-zhen/":{"body":0,"title":0},"/people/在读博士生/mou-hong-lin/":{"body":0,"title":0},"/people/在读博士生/teng-de-chuan/":{"body":0,"title":0},"/people/在读博士生/wang-ding-zi-rui/":{"body":0,"title":0},"/people/在读博士生/wang-yi-xuan/":{"body":0,"title":0},"/people/在读博士生/xu-yang/":{"body":0,"title":0},"/people/在读博士生/xu-yu-zhuang/":{"body":0,"title":0},"/people/在读博士生/zhang-wen-bin/":{"body":0,"title":0},"/people/在读本科生/":{"body":0,"title":0},"/people/在读硕士生/":{"body":0,"title":0},"/people/在读硕士生/guo-chuan-zhe/":{"body":0,"title":0},"/people/在读硕士生/han-zi-yu/":{"body":0,"title":0},"/people/在读硕士生/ji-shi-yu/":{"body":0,"title":0},"/people/在读硕士生/niu-tian-hao/":{"body":0,"title":0},"/people/在读硕士生/peng-deng-yun/":{"body":0,"title":0},"/people/在读硕士生/xu-ke-yan/":{"body":0,"title":0},"/people/在读硕士生/yan-zheng/":{"body":0,"title":0},"/people/在读硕士生/zhang-xuan-jing/":{"body":0,"title":0},"/people/在读硕士生/zhang-zhi-ming/":{"body":0,"title":0},"/people/在读硕士生/zhou-shi-qi/":{"body":0,"title":0},"/people/教师/":{"body":0,"title":0},"/people/教师/che-mo-xiang/":{"body":0,"title":0},"/people/教师/shi-qi/":{"body":0,"title":0},"/people/教师/zhu-qing-fu/":{"body":0,"title":0},"/people/毕业博士/":{"body":0,"title":0},"/people/毕业博士/chen-san-yuan/":{"body":0,"title":0},"/people/毕业博士/dou-long-xu/":{"body":0,"title":0},"/people/毕业博士/guo-jiang/":{"body":0,"title":0},"/people/毕业博士/hou-yu-tai/":{"body":0,"title":0},"/people/毕业博士/li-zheng-hua/":{"body":0,"title":0},"/people/毕业博士/liu-yi-jia/":{"body":0,"title":0},"/people/毕业博士/liu-ze-ming/":{"body":0,"title":0},"/people/毕业博士/shi-xiao-ming/":{"body":0,"title":0},"/people/毕业博士/tan-li-bo/":{"body":0,"title":0},"/people/毕业博士/wang-bao-xin/":{"body":0,"title":0},"/people/毕业博士/wang-shao-lei/":{"body":0,"title":0},"/people/毕业博士/wang-yu-xuan/":{"body":0,"title":0},"/people/毕业博士/xu-jun/":{"body":0,"title":0},"/people/毕业博士/xu-xiao/":{"body":0,"title":0},"/people/毕业博士/zhang-mei-shan/":{"body":0,"title":0},"/people/毕业博士/zheng-bo/":{"body":0,"title":0},"/people/毕业学士/":{"body":0,"title":0},"/people/毕业学士/niu-guo-cheng/":{"body":0,"title":0},"/people/毕业学士/wang-zhe/":{"body":0,"title":0},"/people/毕业学士/wen-hao-yang/":{"body":0,"title":0},"/people/毕业学士/xie-tian-bao/":{"body":0,"title":0},"/people/毕业学士/xu-yi-heng/":{"body":0,"title":0},"/people/毕业硕士/":{"body":0,"title":0},"/people/毕业硕士/chen-cheng/":{"body":0,"title":0},"/people/毕业硕士/chen-xin/":{"body":0,"title":0},"/people/毕业硕士/deng-wen-chao/":{"body":0,"title":0},"/people/毕业硕士/dong-hong-yuan/":{"body":0,"title":0},"/people/毕业硕士/han-bing/":{"body":0,"title":0},"/people/毕业硕士/han-yu/":{"body":0,"title":0},"/people/毕业硕士/han-zhong-hua/":{"body":0,"title":0},"/people/毕业硕士/hu-xiao/":{"body":0,"title":0},"/people/毕业硕士/ji-yu-qiu/":{"body":0,"title":0},"/people/毕业硕士/lai-yong-kui/":{"body":0,"title":0},"/people/毕业硕士/lei-zhi-lin/":{"body":0,"title":0},"/people/毕业硕士/li-qi-xin/":{"body":0,"title":0},"/people/毕业硕士/li-yong-qiang/":{"body":0,"title":0},"/people/毕业硕士/li-zhou-yang/":{"body":0,"title":0},"/people/毕业硕士/liu-yang/":{"body":0,"title":0},"/people/毕业硕士/mao-jia-feng/":{"body":0,"title":0},"/people/毕业硕士/pan-ming-yang/":{"body":0,"title":0},"/people/毕业硕士/qiao-zhen-hao/":{"body":0,"title":0},"/people/毕业硕士/ren-bin/":{"body":0,"title":0},"/people/毕业硕士/sun-bo/":{"body":0,"title":0},"/people/毕业硕士/tang-guo-hua/":{"body":0,"title":0},"/people/毕业硕士/wang-li-jie/":{"body":0,"title":0},"/people/毕业硕士/wang-xing-hao/":{"body":0,"title":0},"/people/毕业硕士/wang-zhong-yuan/":{"body":0,"title":0},"/people/毕业硕士/wei-fu-xuan/":{"body":0,"title":0},"/people/毕业硕士/xia-wen-tian/":{"body":0,"title":0},"/people/毕业硕士/xu-wei/":{"body":0,"title":0},"/people/毕业硕士/xu-zi-xiang/":{"body":0,"title":0},"/people/毕业硕士/zhang-yi/":{"body":0,"title":0},"/people/毕业硕士/zhao-huai-peng/":{"body":0,"title":0},"/people/毕业硕士/zhao-jing/":{"body":0,"title":0},"/people/毕业硕士/zhu-jia-qi/":{"body":0,"title":0},"/projects/":{"body":0,"title":0},"/projects/da-mo-xing-zeng-liang-xue-xi-ji-shu-yan-jiu/":{"body":0,"title":0},"/projects/jian-suo-zeng-qiang-kai-fang-yu-dui-hua-sheng-cheng/":{"body":0,"title":0},"/projects/kai-fang-yu-duo-yuan-zhi-shi-huo-qu-yu-shen-ceng-jie-gou-hua-yu-yi-fen-xi/":{"body":0,"title":0},"/projects/kua-yu-yan-yu-yi-yi-cun-fen-xi-yan-jiu/":{"body":0,"title":0},"/projects/mian-xiang-san-yuan-kong-jian-de-hu-lian-wang-zhong-wen-xin-xi-chu-li-li-lun-yu-fang-fa/":{"body":0,"title":0},"/projects/shao-biao-zhu-zi-ran-yu-yan-chu-li-li-lun-yu-fang-fa/":{"body":0,"title":0},"/projects/sheng-cheng-shi-da-mo-xing-zhong-de-si-wei-lian-ji-shu-ji-li-ji-ying-yong-yan-jiu/":{"body":0,"title":0},"/projects/yi-cun-ju-fa-fen-xi-zi-jie-gou-ke-xin-du-ji-suan-yan-jiu/":{"body":0,"title":0},"/projects/yi-yu-yi-cun-ju-fa-fen-xi-ruo-gan-guan-jian-ji-shu-yan-jiu/":{"body":0,"title":0},"/projects/yu-yan-ji-shu-ping-tai/":{"body":63,"title":0},"/publications/":{"body":0,"title":0},"/publications/2001-fast-deletion-algorithm-for-large-scale-duplicated-web-pages/":{"body":0,"title":8},"/publications/2004-a-new-chinese-natural-language-understanding-architecture-based-on-multilayer-search-mechanism/":{"body":0,"title":10},"/publications/2004-similar-chinese-sentence-retrieval-based-on-improved-edit-distance/":{"body":0,"title":8},"/publications/2005-improved-edit-distance-kernel-for-chinese-relation-extraction/":{"body":0,"title":7},"/publications/2005-semantic-role-labeling-system-using-maximum-entropy-classifier/":{"body":0,"title":8},"/publications/2006-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"body":0,"title":7},"/publications/2007-a-fast-clustering-algorithm-for-abnormal-and-short-texts/":{"body":0,"title":6},"/publications/2007-a-grammar-driven-convolution-tree-kernel-for-semantic-role-classification/":{"body":0,"title":8},"/publications/2007-feature-engineering-for-chinese-semantic-role-labeling/":{"body":0,"title":6},"/publications/2007-hit-ir-wsd-a-wsd-system-for-english-lexical-sample-task/":{"body":0,"title":9},"/publications/2007-semantic-role-labeling-with-maximum-entropy-classifier/":{"body":0,"title":6},"/publications/2008-a-cascaded-syntactic-and-semantic-dependency-parsing-system/":{"body":0,"title":6},"/publications/2008-a-study-on-constituentto-dependency-conversion/":{"body":0,"title":4},"/publications/2008-fast-computing-grammar-driven-convolution-tree-kernel-for-semantic-role-labeling/":{"body":0,"title":10},"/publications/2008-introduction-to-information-retrieval-system/":{"body":0,"title":4},"/publications/2008-semantic-role-labeling-using-a-grammar-driven-convolution-tree-kernel/":{"body":0,"title":9},"/publications/2008-using-a-hybrid-convolution-tree-kernel-for-semantic-role-labeling/":{"body":0,"title":8},"/publications/2008-xin-xi-jian-suo-xi-tong-dao-lun/":{"body":0,"title":0},"/publications/2009-language-specific-issue-and-feature-exploration-in-chinese-event-extraction/":{"body":0,"title":8},"/publications/2009-multilingual-dependency-based-syntactic-and-semantic-parsing/":{"body":0,"title":6},"/publications/2010-appraisal-expression-recognition-with-syntactic-path-for-sentence-sentiment-classification/":{"body":0,"title":8},"/publications/2010-beam-search-based-high-order-dependency-parser-j/":{"body":0,"title":8},"/publications/2010-coherent-dialog-generation-with-query-graph/":{"body":0,"title":5},"/publications/2010-combining-self-supervised-learning-and-active-learning-for-disfluency-detection/":{"body":0,"title":8},"/publications/2010-hit-cir-an-unsupervised-wsd-system-based-on-domain-most-frequent-sense-estimation/":{"body":0,"title":10},"/publications/2010-improving-dependency-parsing-using-punctuation/":{"body":0,"title":5},"/publications/2010-improving-semantic-role-labeling-with-word-sense/":{"body":0,"title":6},"/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/":{"body":0,"title":6},"/publications/2010-introduction-to-information-retrieval/":{"body":0,"title":3},"/publications/2010-jointly-modeling-wsd-and-srl-with-markov-logic/":{"body":0,"title":6},"/publications/2010-ltp-a-chinese-language-technology-platform/":{"body":0,"title":5},"/publications/2010-semi-supervised-domain-adaptation-for-wsd-using-a-word-by-word-model-selection-approach/":{"body":0,"title":11},"/publications/2010-sou-suo-yin-qing-xin-xi-jian-suo-shi-jian/":{"body":0,"title":0},"/publications/2010-using-word-sense-disambiguation-for-semantic-role-labeling/":{"body":0,"title":7},"/publications/2011-a-graph-based-method-for-entity-linking/":{"body":0,"title":5},"/publications/2011-hit-approaches-to-entity-linking-at-tac-2011/":{"body":0,"title":6},"/publications/2011-improving-chinese-pos-tagging-with-dependency-parsing/":{"body":0,"title":6},"/publications/2011-joint-models-for-chinese-pos-tagging-and-dependency-parsing/":{"body":0,"title":7},"/publications/2011-the-data-paper-a-mechanism-to-incentivize-data-publishing-in-biodiversity-science/":{"body":0,"title":8},"/publications/2011-word-sense-disambiguation-corpora-acquisition-via-confirmation-code/":{"body":0,"title":8},"/publications/2012-a-comparison-of-chinese-parsers-for-stanford-dependencies/":{"body":0,"title":5},"/publications/2012-a-separately-passive-aggressive-training-algorithm-for-joint-pos-tagging-and-dependency-parsing/":{"body":0,"title":10},"/publications/2012-active-learning-for-chinese-dependency-parsing/":{"body":0,"title":5},"/publications/2012-combining-statistical-model-and-dictionary-for-domain-adaption-of-chinese-word-segmentation/":{"body":0,"title":9},"/publications/2012-exploiting-multiple-treebanks-for-parsing-with-quasi-synchronous-grammars/":{"body":0,"title":7},"/publications/2012-hit-dependency-parsing-bootstrap-aggregating-heterogeneous-parsers/":{"body":0,"title":7},"/publications/2012-improve-chinese-semantic-dependency-parsing-via-syntactic-dependency-parsing/":{"body":0,"title":9},"/publications/2012-micro-blogs-oriented-word-segmentation-system/":{"body":0,"title":6},"/publications/2012-multiple-treebanks-integration-for-chinese-phrase-structure-grammar-parsing-using-bagging/":{"body":0,"title":10},"/publications/2012-semeval-2012-task-5-chinese-semantic-dependency-parsing/":{"body":0,"title":8},"/publications/2012-stacking-heterogeneous-joint-models-of-chinese-pos-tagging-and-dependency-parsing/":{"body":0,"title":9},"/publications/2012-stanfords-system-for-parsing-the-english-web/":{"body":0,"title":5},"/publications/2013-a-comparison-sthdy-of-sequence-labeling-methods-for-chinesewords-segmentation-pos-tagging-models/":{"body":0,"title":10},"/publications/2013-chinese-parsing-exploiting-characters/":{"body":0,"title":4},"/publications/2013-convolution-neural-network-for-relation-extraction/":{"body":0,"title":5},"/publications/2013-effective-bilingual-constraints-for-semi-supervised-learning-of-namedentity-recognizers/":{"body":0,"title":8},"/publications/2013-enhancing-chinese-word-segmentation-with-character-clustering/":{"body":0,"title":6},"/publications/2013-joint-optimization-for-chinese-pos-tagging-and-dependency-parsing/":{"body":0,"title":7},"/publications/2013-joint-word-alignment-and-bilingual-named-entity-recognition-using-dual-decomposition/":{"body":0,"title":10},"/publications/2013-named-entity-recognition-with-bilingual-constraints/":{"body":0,"title":5},"/publications/2014-a-semantics-oriented-grammar-for-chinese-treebanking/":{"body":0,"title":5},"/publications/2014-character-level-chinese-dependency-parsing/":{"body":0,"title":5},"/publications/2014-dependency-graph-based-chinese-semantic-parsing/":{"body":0,"title":6},"/publications/2014-domain-adaptation-for-crf-based-chinese-word-segmentation-using-free-annotations/":{"body":0,"title":10},"/publications/2014-jointly-or-separately-which-is-better-for-parsing-heterogeneous-dependencies/":{"body":0,"title":6},"/publications/2014-learning-semantic-hierarchies-via-word-embeddings/":{"body":0,"title":6},"/publications/2014-learning-sense-specific-word-embeddings-by-exploiting-bilingual-resources/":{"body":0,"title":8},"/publications/2014-reliable-dependency-arc-recognition/":{"body":0,"title":4},"/publications/2014-revisiting-embedding-features-for-simple-semi-supervised-learning/":{"body":0,"title":7},"/publications/2014-sentence-compression-for-target-polarity-word-collocation-extraction/":{"body":0,"title":7},"/publications/2014-type-supervised-domain-adaptation-for-joint-segmentation-and-pos-tagging/":{"body":0,"title":8},"/publications/2015-cross-lingual-dependency-parsing-based-on-distributed-representations/":{"body":0,"title":7},"/publications/2015-sentence-compression-for-aspect-based-sentiment-analysis/":{"body":0,"title":6},"/publications/2015-transition-based-syntactic-linearization/":{"body":0,"title":4},"/publications/2016-a-distributed-representation-based-framework-for-cross-lingual-transfer-parsing/":{"body":0,"title":8},"/publications/2016-a-neural-attention-model-for-disfluency-detection/":{"body":0,"title":5},"/publications/2016-a-representation-learning-framework-for-multi-source-transfer-parsing/":{"body":0,"title":7},"/publications/2016-a-unified-architecture-for-semantic-role-labeling-and-relation-classification/":{"body":0,"title":7},"/publications/2016-a-universal-framework-for-inductive-transfer-parsing-across-multi-typed-treebanks/":{"body":0,"title":8},"/publications/2016-chinese-grammatical-error-diagnosis-with-long-short-term-memory-networks/":{"body":0,"title":9},"/publications/2016-enhancing-neural-disfluency-detection-with-hand-crafted-features/":{"body":0,"title":7},"/publications/2016-exploiting-multi-typed-treebanks-for-parsing-with-deep-multi-task-learning/":{"body":0,"title":9},"/publications/2016-exploring-segment-representations-for-neural-segmentation-models/":{"body":0,"title":6},"/publications/2016-hc-search-for-incremental-parsing/":{"body":0,"title":4},"/publications/2016-python-cheng-xu-she-ji/":{"body":0,"title":1},"/publications/2016-transition-based-chinese-semantic-dependency-graph-parsing/":{"body":0,"title":7},"/publications/2017-a-review-on-entity-relation-extraction/":{"body":0,"title":4},"/publications/2017-benben-a-chinese-intelligent-conversational-robot/":{"body":0,"title":5},"/publications/2017-deep-learning-in-lexical-analysis-and-parsing/":{"body":0,"title":5},"/publications/2017-enhancing-lstm-based-word-segmentation-using-unlabeled-data/":{"body":0,"title":8},"/publications/2017-the-first-evaluation-of-chinese-human-computer-dialogue-technology/":{"body":0,"title":7},"/publications/2017-the-hit-scir-system-for-end-to-end-parsing-of-universal-dependencies/":{"body":0,"title":8},"/publications/2017-transition-based-disfluency-detection-using-lstms/":{"body":0,"title":6},"/publications/2018-a-neural-transition-based-approach-for-semantic-dependency-graph-parsing/":{"body":0,"title":8},"/publications/2018-an-amr-aligner-tuned-by-transition-based-parser/":{"body":0,"title":6},"/publications/2018-chinese-grammatical-error-diagnosis-using-statistical-and-prior-knowledge-driven-features-with-probabilistic-ensemble-enhan/":{"body":0,"title":13},"/publications/2018-deep-learning-in-lexical-analysis-and-parsing/":{"body":0,"title":5},"/publications/2018-distilling-knowledge-for-search-based-structured-prediction/":{"body":0,"title":6},"/publications/2018-ji-yu-shen-du-xue-xi-de-zi-ran-yu-yan-chu-li/":{"body":0,"title":0},"/publications/2018-joint-extraction-of-entities-and-relations-based-on-a-novel-graphscheme/":{"body":0,"title":7},"/publications/2018-parsing-tweets-into-universal-dependencies/":{"body":0,"title":4},"/publications/2018-sequence-to-sequence-data-augmentation-for-dialogue-language-understanding/":{"body":0,"title":7},"/publications/2018-sequence-to-sequence-learning-for-task-oriented-dialogue-with-dialogue-state-representation/":{"body":0,"title":9},"/publications/2018-towards-better-ud-parsing-deep-contextualized-word-embeddings-ensemble-and-treebank-concatenation/":{"body":0,"title":11},"/publications/2019-a-corpus-free-state2seq-user-simulator-for-task-oriented-dialogue/":{"body":0,"title":8},"/publications/2019-a-key-phrase-aware-end2end-neural-response-generation-model/":{"body":0,"title":8},"/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/":{"body":0,"title":7},"/publications/2019-a-stack-propagation-framework-with-token-level-intent-detection-for-spoken-language-understanding/":{"body":0,"title":10},"/publications/2019-an-evaluation-of-chinese-human-computer-dialogue-technology/":{"body":0,"title":6},"/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/":{"body":0,"title":7},"/publications/2019-cross-lingual-bert-transformation-for-zero-shot-dependency-parsing/":{"body":0,"title":8},"/publications/2019-cross-lingual-machine-reading-comprehension/":{"body":0,"title":5},"/publications/2019-deep-contextualized-word-embeddings-for-universal-dependency-parsing/":{"body":0,"title":7},"/publications/2019-entity-consistent-end-to-end-task-oriented-dialogue-system-with-kb-retriever/":{"body":0,"title":10},"/publications/2019-generating-natural-language-adversarial-examples-through-probability-weighted-word-saliency/":{"body":0,"title":10},"/publications/2019-hit-scir-at-mrp-2019-a-unified-pipeline-for-meaning-representation-parsing-via-efficient-training-and-effective-encoding/":{"body":0,"title":14},"/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/":{"body":0,"title":7},"/publications/2019-learning-semantic-hierarchies-a-continuous-vector-space-approach/":{"body":0,"title":7},"/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/":{"body":0,"title":7},"/publications/2020-a-co-interactive-transformer-for-joint-slot-filling-and-intent-detection/":{"body":0,"title":8},"/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/":{"body":0,"title":7},"/publications/2020-agif-an-adaptive-graph-interactive-framework-for-joint-multiple-intent-detection-and-slot-filling/":{"body":0,"title":11},"/publications/2020-combining-self-training-and-self-supervised-learning-for-unsupervised-disfluency-detection/":{"body":0,"title":9},"/publications/2020-conversational-graph-grounded-policy-learning-for-open-domain-conversation-generation/":{"body":0,"title":9},"/publications/2020-cosda-ml-multi-lingual-code-switching-data-augmentation-for-zero-shotcross-lingual-nlp/":{"body":0,"title":12},"/publications/2020-dcr-net-a-deep-co-interactive-relation-network-for-joint-dialog-act-recognition-and-sentiment-classification/":{"body":0,"title":13},"/publications/2020-discovering-dialog-structure-graph-for-open-domain-dialog-generation/":{"body":0,"title":8},"/publications/2020-discriminative-sentence-modeling-for-story-ending-prediction/":{"body":0,"title":6},"/publications/2020-document-modeling-with-graph-attention-networks-for-multi-grained-machine-reading-comprehension/":{"body":0,"title":10},"/publications/2020-dynamic-fusion-network-for-multi-domain-end-to-end-task-oriented-dialog/":{"body":0,"title":10},"/publications/2020-enhancing-dialog-coherence-with-event-graph-grounded-content-planning/":{"body":0,"title":8},"/publications/2020-exploring-segment-representations-for-neural-semi-markov-conditional-random-fields/":{"body":0,"title":9},"/publications/2020-few-shot-slot-tagging-with-collapsed-dependency-transfer-and-label-enhanced-task-adaptive-projection-network/":{"body":0,"title":13},"/publications/2020-fewjoint-a-few-shot-learning-benchmark-for-joint-language-understanding/":{"body":0,"title":8},"/publications/2020-from-static-to-dynamic-word-representations-a-survey/":{"body":0,"title":5},"/publications/2020-hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/":{"body":0,"title":10},"/publications/2020-injecting-word-information-with-multi-level-word-adapter-for-chinese-spoken-language-understanding/":{"body":0,"title":11},"/publications/2020-keywords-generation-improves-e-commerce-session-based-recommendation/":{"body":0,"title":8},"/publications/2020-knowledge-graph-grounded-goal-planning-for-open-domain-conversation-generation/":{"body":0,"title":9},"/publications/2020-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/":{"body":0,"title":10},"/publications/2020-multi-task-self-supervised-learning-for-disfluency-detection/":{"body":0,"title":7},"/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/":{"body":0,"title":10},"/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/":{"body":0,"title":8},"/publications/2020-slot-consistent-nlg-for-task-oriented-dialogue-systems-with-iterative-rectification-network/":{"body":0,"title":10},"/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/":{"body":0,"title":9},"/publications/2020-towards-conversational-recommendation-over-multi-type-dialogs/":{"body":0,"title":7},"/publications/2020-understanding-medical-conversations-with-scattered-keyword-attention-and-weak-supervision-from-responses/":{"body":0,"title":9},"/publications/2021-a-closer-look-into-the-robustness-of-neural-dependency-parsers-using-better-adversarial-examples/":{"body":0,"title":10},"/publications/2021-a-survey-on-spoken-language-understanding-recent-advances-and-new-frontiers/":{"body":0,"title":8},"/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/":{"body":0,"title":7},"/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/":{"body":0,"title":10},"/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/":{"body":0,"title":9},"/publications/2021-c2c-genda-cluster-to-cluster-generation-for-data-augmentation-of-slot-filling/":{"body":0,"title":9},"/publications/2021-character-level-syntax-infusion-in-pre-trained-models-for-chinese-semantic-role-labeling/":{"body":0,"title":11},"/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/":{"body":0,"title":6},"/publications/2021-discovering-dialog-structure-graph-for-coherent-dialog-generation/":{"body":0,"title":7},"/publications/2021-discovering-drug-target-interaction-knowledge-from-biomedical-literature/":{"body":0,"title":7},"/publications/2021-dont-be-contradicted-with-anything-ci-tod-towards-benchmarking-consistency-for-task-oriented-dialogue-system/":{"body":0,"title":12},"/publications/2021-durecdial-20-a-bilingual-parallel-corpus-for-conversational-recommendation/":{"body":0,"title":7},"/publications/2021-dynamic-connected-networks-for-chinese-spelling-check/":{"body":0,"title":6},"/publications/2021-few-shot-learning-for-multi-label-intent-detection/":{"body":0,"title":7},"/publications/2021-gl-gin-fast-and-accurate-non-autoregressive-model-for-joint-multiple-intent-detection-and-slot-filling/":{"body":0,"title":13},"/publications/2021-knowing-where-to-leverage-context-aware-graph-convolutional-network-with-an-adaptive-fusion-layer-for-contextual-spoken-lan/":{"body":0,"title":14},"/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/":{"body":0,"title":9},"/publications/2021-learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling/":{"body":0,"title":12},"/publications/2021-n-ltp-an-open-source-neural-language-technology-platform-for-chinese/":{"body":0,"title":9},"/publications/2021-nl-augmenter-a-framework-for-task-sensitive-natural-language-augmentation/":{"body":0,"title":8},"/publications/2021-understanding-attention-in-machine-reading-comprehension/":{"body":0,"title":5},"/publications/2021-understanding-patient-query-with-weak-supervision-from-doctor-response/":{"body":0,"title":7},"/publications/2021-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/":{"body":0,"title":0},"/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/":{"body":0,"title":6},"/publications/2022-cctc-a-cross-sentence-chinese-text-correction-dataset-for-native-speakers/":{"body":0,"title":9},"/publications/2022-cgim-a-cycle-guided-interactive-learning-model-for-consistency-identification-in-task-oriented-dialogue/":{"body":0,"title":11},"/publications/2022-data-augmentation-approaches-in-natural-language-processing-a-survey/":{"body":0,"title":7},"/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/":{"body":0,"title":6},"/publications/2022-fewjoint-few-shot-learning-for-joint-dialogue-understanding/":{"body":0,"title":7},"/publications/2022-gl-clef-a-global-local-contrastive-learning-framework-for-cross-lingual-spoken-language-understanding/":{"body":0,"title":12},"/publications/2022-graph-grounded-goal-planning-for-conversational-recommendation/":{"body":0,"title":6},"/publications/2022-improving-pre-trained-language-models-with-syntactic-dependency-prediction-task-for-chinese-semantic-error-recognition/":{"body":0,"title":13},"/publications/2022-interht-knowledge-graph-embeddings-by-interaction-between-head-and-tail-entities/":{"body":0,"title":9},"/publications/2022-inverse-is-better-fast-and-accurate-prompt-for-few-shot-slot-tagging/":{"body":0,"title":9},"/publications/2022-learning-based-hybrid-local-search-for-the-hard-label-textual-attack/":{"body":0,"title":9},"/publications/2022-metaprompting-learning-to-learn-better-prompts/":{"body":0,"title":5},"/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/":{"body":0,"title":9},"/publications/2022-overview-of-ctc-2021-chinese-text-correction-for-native-speakers/":{"body":0,"title":8},"/publications/2022-simple-and-effective-graph-to-graph-annotation-conversion/":{"body":0,"title":6},"/publications/2022-teaching-machines-to-read-answer-and-explain/":{"body":0,"title":5},"/publications/2022-text-is-no-more-enough-a-benchmark-for-profile-based-spoken-language-understanding/":{"body":0,"title":9},"/publications/2022-towards-knowledge-intensive-text-to-sql-semantic-parsing-with-formulaic-knowledge/":{"body":0,"title":9},"/publications/2022-unisar-a-unified-structure-aware-autoregressive-language-model-for-text-to-sql/":{"body":0,"title":9},"/publications/2023-a-preliminary-evaluation-of-chatgpt-for-zero-shot-dialogue-understanding/":{"body":0,"title":7},"/publications/2023-bridgetower-building-bridges-between-encoders-in-vision-language-representation-learning/":{"body":0,"title":9},"/publications/2023-combating-with-extremely-noisy-samples-in-weakly-supervised-slot-filling-for-automatic-diagnosis/":{"body":0,"title":10},"/publications/2023-controllable-data-augmentation-for-context-dependent-text-to-sql/":{"body":0,"title":7},"/publications/2023-cross-lingual-prompting-improving-zero-shot-chain-of-thought-reasoning-across-languages/":{"body":0,"title":10},"/publications/2023-csed-a-chinese-semantic-error-diagnosis-corpus/":{"body":0,"title":6},"/publications/2023-improving-cross-lingual-language-understanding-with-consistency-regularization-based-fine-tuning/":{"body":0,"title":10},"/publications/2023-language-anisotropic-cross-lingual-model-editing/":{"body":0,"title":6},"/publications/2023-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning/":{"body":0,"title":10},"/publications/2023-metricprompt-prompting-model-as-a-relevance-metric-for-few-shot-text-classification/":{"body":0,"title":9},"/publications/2023-mixpro-simple-yet-effective-data-augmentation-for-prompt-based-learning/":{"body":0,"title":8},"/publications/2023-modularized-pre-training-for-end-to-end-task-oriented-dialogue/":{"body":0,"title":8},"/publications/2023-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding/":{"body":0,"title":8},"/publications/2023-semantic-guided-image-augmentation-with-pre-trained-models/":{"body":0,"title":7},"/publications/2024-a-survey-on-natural-language-processing-for-programming/":{"body":0,"title":5},"/publications/2024-a-two-stage-framework-with-self-supervised-distillation-for-cross-domain-text-classification/":{"body":0,"title":10},"/publications/2024-beyond-static-evaluation-a-dynamic-approach-to-assessing-ai-assistants-api-invocation-capabilities/":{"body":0,"title":11},"/publications/2024-concise-and-precise-context-compression-for-tool-using-language-models/":{"body":0,"title":8},"/publications/2024-decoupling-breaks-data-barriers-a-decoupled-pre-training-framework-for-multi-intent-spoken-language-understanding/":{"body":0,"title":13},"/publications/2024-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes/":{"body":0,"title":7},"/publications/2024-exploring-equation-as-a-better-intermediate-meaning-representation-for-numerical-reasoning-of-large-language-models/":{"body":0,"title":11},"/publications/2024-exploring-hybrid-question-answering-via-program-based-prompting/":{"body":0,"title":8},"/publications/2024-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql/":{"body":0,"title":8},"/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/":{"body":0,"title":8},"/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/":{"body":0,"title":9},"/publications/2024-m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought/":{"body":0,"title":11},"/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/":{"body":0,"title":11},"/publications/2024-onebit-towards-extremely-low-bit-large-language-models/":{"body":0,"title":8},"/publications/2024-self-constructed-context-decompilation-with-fined-grained-alignment-enhancement/":{"body":0,"title":8},"/publications/2024-semantic-guided-generative-image-augmentation-method-with-diffusion-models-for-image-classification/":{"body":0,"title":10},"/publications/2024-unlocking-the-capabilities-of-thought-a-reasoning-boundary-framework-to-quantify-and-optimize-chain-of-thought/":{"body":0,"title":10},"/publications/2024-what-factors-affect-multi-modal-in-context-learning-an-in-depth-exploration/":{"body":0,"title":8},"/publications/2025-a-survey-of-multilingual-large-language-models/":{"body":0,"title":5},"/publications/2025-can-large-language-models-understand-you-better-an-mbti-personality-detection-dataset-aligned-with-population-traits/":{"body":0,"title":12},"/publications/2025-murre-multi-hop-table-retrieval-with-removal-for-open-domain-text-to-sql/":{"body":0,"title":10},"/publications/2025-towards-reasoning-era-a-survey-of-long-chain-of-thought-for-reasoning-large-language-models/":{"body":0,"title":11},"/publications/2025-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa-copy/":{"body":0,"title":0},"/research/":{"body":0,"title":0},"/research/方向介绍/":{"body":0,"title":0},"/research/方向介绍/an-quan/":{"body":0,"title":0},"/research/方向介绍/bu-shu/":{"body":1,"title":0},"/research/方向介绍/ju-shen/":{"body":3,"title":0},"/research/方向介绍/sheng-cheng/":{"body":6,"title":0},"/research/方向介绍/tui-li/":{"body":7,"title":0},"/resources/":{"body":0,"title":0},"/resources/开源项目/":{"body":0,"title":0},"/resources/开源项目/abacus/":{"body":0,"title":1},"/resources/开源项目/chinese-mixtral-8x7b/":{"body":0,"title":3},"/resources/开源项目/ltp/":{"body":63,"title":1},"/resources/开源项目/pyltp/":{"body":13,"title":1},"/resources/社区资源/":{"body":0,"title":0},"/resources/社区资源/da/":{"body":0,"title":1},"/resources/社区资源/elmoformanylangs/":{"body":0,"title":1},"/resources/社区资源/la-beginner/":{"body":0,"title":2},"/resources/社区资源/slu/":{"body":0,"title":8},"/resources/社区资源/task-oriented-dialog-research-progress/":{"body":0,"title":5},"/resources/社区资源/xia-zai-hit-scirde-mo-xing-he-shu-ju-ji/":{"body":22,"title":2}},"length":492},"lang":"English"};