<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title> - Cui Yiming</title>
	<link href="/authors/cui-yiming/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="/"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2099-12-31T00:00:00+00:00</updated>
	<id>/authors/cui-yiming/atom.xml</id>
	<entry xml:lang="en">
		<title>自然语言处理：基于预训练模型的方法</title>
		<published>2099-12-31T00:00:00+00:00</published>
		<updated>2099-12-31T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2021-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/" type="text/html"/>
		<id>/publications/2021-zi-ran-yu-yan-chu-li-ji-yu-yu-xun-lian-mo-xing-de-fang-fa/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>ExpMRC explainability evaluation for machine reading comprehension</title>
		<published>2022-01-01T00:00:00+00:00</published>
		<updated>2022-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2022-expmrc-explainability-evaluation-for-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Multilingual multi-aspect explainability analyses on machine reading comprehension models</title>
		<published>2022-01-01T00:00:00+00:00</published>
		<updated>2022-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/" type="text/html"/>
		<id>/publications/2022-multilingual-multi-aspect-explainability-analyses-on-machine-reading-comprehension-models/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Teaching machines to read answer and explain</title>
		<published>2022-01-01T00:00:00+00:00</published>
		<updated>2022-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2022-teaching-machines-to-read-answer-and-explain/" type="text/html"/>
		<id>/publications/2022-teaching-machines-to-read-answer-and-explain/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Adversarial Training for Machine Reading Comprehension with Virtual Embeddings</title>
		<published>2021-01-01T00:00:00+00:00</published>
		<updated>2021-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/" type="text/html"/>
		<id>/publications/2021-adversarial-training-for-machine-reading-comprehension-with-virtual-embeddings/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</title>
		<published>2021-01-01T00:00:00+00:00</published>
		<updated>2021-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/" type="text/html"/>
		<id>/publications/2021-bilingual-alignment-pre-training-for-zero-shot-cross-lingual-transfer/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Understanding attention in machine reading comprehension</title>
		<published>2021-01-01T00:00:00+00:00</published>
		<updated>2021-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2021-understanding-attention-in-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2021-understanding-attention-in-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>A Sentence Cloze Dataset for Chinese Machine Reading Comprehension</title>
		<published>2020-01-01T00:00:00+00:00</published>
		<updated>2020-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2020-a-sentence-cloze-dataset-for-chinese-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Recall and Learn Fine-tuning Deep Pretrained Language Models with Less Forgetting</title>
		<published>2020-01-01T00:00:00+00:00</published>
		<updated>2020-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/" type="text/html"/>
		<id>/publications/2020-recall-and-learn-fine-tuning-deep-pretrained-language-models-with-less-forgetting/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Revisiting Pre-Trained Models for Chinese Natural Language Processing</title>
		<published>2020-01-01T00:00:00+00:00</published>
		<updated>2020-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/" type="text/html"/>
		<id>/publications/2020-revisiting-pre-trained-models-for-chinese-natural-language-processing/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>TextBrewer An Open-Source Knowledge Distillation Toolkit for Natural Language Processing</title>
		<published>2020-01-01T00:00:00+00:00</published>
		<updated>2020-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/" type="text/html"/>
		<id>/publications/2020-textbrewer-an-open-source-knowledge-distillation-toolkit-for-natural-language-processing/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>A Span-Extraction Dataset for Chinese Machine Reading Comprehension</title>
		<published>2019-01-01T00:00:00+00:00</published>
		<updated>2019-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2019-a-span-extraction-dataset-for-chinese-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Contextual recurrent units for cloze-style reading comprehension</title>
		<published>2019-01-01T00:00:00+00:00</published>
		<updated>2019-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/" type="text/html"/>
		<id>/publications/2019-contextual-recurrent-units-for-cloze-style-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Cross-Lingual Machine Reading Comprehension</title>
		<published>2019-01-01T00:00:00+00:00</published>
		<updated>2019-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2019-cross-lingual-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2019-cross-lingual-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Improving machine reading comprehension via adversarial training</title>
		<published>2019-01-01T00:00:00+00:00</published>
		<updated>2019-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/" type="text/html"/>
		<id>/publications/2019-improving-machine-reading-comprehension-via-adversarial-training/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Pre-training with whole word masking for chinese bert</title>
		<published>2019-01-01T00:00:00+00:00</published>
		<updated>2019-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/" type="text/html"/>
		<id>/publications/2019-pre-training-with-whole-word-masking-for-chinese-bert/</id>
		<content type="html"></content>
	</entry>
	<entry xml:lang="en">
		<title>Interactive Gated Decoder for Machine Reading Comprehension</title>
		<published>2010-01-01T00:00:00+00:00</published>
		<updated>2010-01-01T00:00:00+00:00</updated>
		<link rel="alternate" href="/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/" type="text/html"/>
		<id>/publications/2010-interactive-gated-decoder-for-machine-reading-comprehension/</id>
		<content type="html"></content>
	</entry>
</feed>
