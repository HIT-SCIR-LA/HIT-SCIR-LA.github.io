<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title> - Wang Yixuan</title>
    <link rel="self" type="application/atom+xml" href="/authors/wang-yixuan/atom.xml"/>
    <link rel="alternate" type="text/html" href="/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-01-01T00:00:00+00:00</updated>
    <id>/authors/wang-yixuan/atom.xml</id>
    <entry xml:lang="en">
        <title>CAMERA Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title>
        <published>2025-01-01T00:00:00+00:00</published>
        <updated>2025-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2025-camera-multi-matrix-joint-compression-for-moe-models-via-micro-expert-redundancy-analysis/"/>
        <id>/publications/2025-camera-multi-matrix-joint-compression-for-moe-models-via-micro-expert-redundancy-analysis/</id>
        
        <content type="html" xml:base="/publications/2025-camera-multi-matrix-joint-compression-for-moe-models-via-micro-expert-redundancy-analysis/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Judge Q Trainable Queries for Optimized Information Retention in KV Cache Eviction</title>
        <published>2025-01-01T00:00:00+00:00</published>
        <updated>2025-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2025-judge-q-trainable-queries-for-optimized-information-retention-in-kv-cache-eviction/"/>
        <id>/publications/2025-judge-q-trainable-queries-for-optimized-information-retention-in-kv-cache-eviction/</id>
        
        <content type="html" xml:base="/publications/2025-judge-q-trainable-queries-for-optimized-information-retention-in-kv-cache-eviction/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Lookahead Q-Cache Achieving More Consistent KV Cache Eviction via Pseudo Query</title>
        <published>2025-01-01T00:00:00+00:00</published>
        <updated>2025-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2025-lookahead-q-cache-achieving-more-consistent-kv-cache-eviction-via-pseudo-query/"/>
        <id>/publications/2025-lookahead-q-cache-achieving-more-consistent-kv-cache-eviction-via-pseudo-query/</id>
        
        <content type="html" xml:base="/publications/2025-lookahead-q-cache-achieving-more-consistent-kv-cache-eviction-via-pseudo-query/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Tag-Evol Achieving Efficient Instruction Evolving via Tag Injection</title>
        <published>2025-01-01T00:00:00+00:00</published>
        <updated>2025-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2025-tag-evol-achieving-efficient-instruction-evolving-via-tag-injection/"/>
        <id>/publications/2025-tag-evol-achieving-efficient-instruction-evolving-via-tag-injection/</id>
        
        <content type="html" xml:base="/publications/2025-tag-evol-achieving-efficient-instruction-evolving-via-tag-injection/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Turning Trash into Treasure Accelerating Inference of Large Language Models with Token Recycling</title>
        <published>2025-01-01T00:00:00+00:00</published>
        <updated>2025-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2025-turning-trash-into-treasure-accelerating-inference-of-large-language-models-with-token-recycling/"/>
        <id>/publications/2025-turning-trash-into-treasure-accelerating-inference-of-large-language-models-with-token-recycling/</id>
        
        <content type="html" xml:base="/publications/2025-turning-trash-into-treasure-accelerating-inference-of-large-language-models-with-token-recycling/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Improving Grammatical Error Correction via Contextual Data Augmentation</title>
        <published>2024-01-01T00:00:00+00:00</published>
        <updated>2024-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/"/>
        <id>/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/</id>
        
        <content type="html" xml:base="/publications/2024-improving-grammatical-error-correction-via-contextual-data-augmentation/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>LM-Combiner A Contextual Rewriting Model for Chinese Grammatical Error Correction</title>
        <published>2024-01-01T00:00:00+00:00</published>
        <updated>2024-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/"/>
        <id>/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/</id>
        
        <content type="html" xml:base="/publications/2024-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Make Some Noise Unlocking Language Model Parallel Inference Capability through Noisy Training</title>
        <published>2024-01-01T00:00:00+00:00</published>
        <updated>2024-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/"/>
        <id>/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/</id>
        
        <content type="html" xml:base="/publications/2024-make-some-noise-unlocking-language-model-parallel-inference-capability-through-noisy-training/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Adaptive Unsupervised Self-training for Disfluency Detection</title>
        <published>2022-01-01T00:00:00+00:00</published>
        <updated>2022-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/"/>
        <id>/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/</id>
        
        <content type="html" xml:base="/publications/2022-adaptive-unsupervised-self-training-for-disfluency-detection/"></content>
        
    </entry>
</feed>
