<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title> - Wei Furu</title>
    <link rel="self" type="application/atom+xml" href="/authors/wei-furu/atom.xml"/>
    <link rel="alternate" type="text/html" href="/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2021-01-01T00:00:00+00:00</updated>
    <id>/authors/wei-furu/atom.xml</id>
    <entry xml:lang="en">
        <title>Allocating large vocabulary capacity for cross-lingual language model pre-training</title>
        <published>2021-01-01T00:00:00+00:00</published>
        <updated>2021-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/"/>
        <id>/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/</id>
        
        <content type="html" xml:base="/publications/2021-allocating-large-vocabulary-capacity-for-cross-lingual-language-model-pre-training/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>Consistency Regularization for Cross-Lingual Fine-Tuning</title>
        <published>2021-01-01T00:00:00+00:00</published>
        <updated>2021-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/"/>
        <id>/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/</id>
        
        <content type="html" xml:base="/publications/2021-consistency-regularization-for-cross-lingual-fine-tuning/"></content>
        
    </entry>
    <entry xml:lang="en">
        <title>LayoutLMv2 Multi-modal Pre-training for Visually-rich Document Understanding</title>
        <published>2021-01-01T00:00:00+00:00</published>
        <updated>2021-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/"/>
        <id>/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/</id>
        
        <content type="html" xml:base="/publications/2021-layoutlmv2-multi-modal-pre-training-for-visually-rich-document-understanding/"></content>
        
    </entry>
</feed>
